[
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "[List main concepts from lecture]\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "[List main concepts from lecture]\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniques",
    "href": "weekly-notes/week-04-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches] #Predicate Definition Policy Use Case st_intersects() #Any overlap at all ‚ÄúCounties affected by flooding‚Äù #st_touches() Share boundary, no interior overlap ‚ÄúNeighboring counties‚Äù #st_within() Completely inside ‚ÄúSchools within district boundaries‚Äù #st_contains() Completely contains ‚ÄúDistricts containing hospitals‚Äù #st_overlaps() Partial overlap ‚ÄúOverlapping service areas‚Äù #st_disjoint() No spatial relationship ‚ÄúCounties separate from urban areas‚Äù\n[Quarto features learned]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn‚Äôt fully understand]\n[Areas needing more practice]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week‚Äôs content applies to real policy work]"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I‚Äôll apply this knowledge]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "[List main concepts from lecture]\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "[List main concepts from lecture]\n[Technical skills covered]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches]\n[Quarto features learned]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn‚Äôt fully understand]\n[Areas needing more practice]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week‚Äôs content applies to real policy work]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I‚Äôll apply this knowledge]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jinyang Xu - MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Course Practice\nAssignment: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\n[MUSA student trying hard on data]\n\n\n\n\nEmail: [jyxu48@upenn.edu]\nGitHub: [@jyxu48]"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "Jinyang Xu - MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "Jinyang Xu - MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Course Practice\nAssignment: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Jinyang Xu - MUSA 5080 Portfolio",
    "section": "",
    "text": "[MUSA student trying hard on data]"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Jinyang Xu - MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: [jyxu48@upenn.edu]\nGitHub: [@jyxu48]"
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Load necessary libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE, tigris_class = \"sf\")\nlibrary(MASS)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(nngeo)\nlibrary(car)\nlibrary(knitr)\nlibrary(readr)\nlibrary(patchwork)\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  \n\n\n1.1 Load and clean Philadelphia sales data:\n\n1.1.1 Load data\n\n\n\nCode\nlibrary(tidyverse)\nopa &lt;- read_csv(\"opa_properties_public1.csv\")\n\n\n\n1.1.2 Filter to residential properties, 2023-2024 sales\n\n\n\nCode\n# data in 2022 will be used as predictor, so keep them as well.\nopa_clean &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# record the amount of data we will focus on\nopa_clean2 &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# Select relevant variables \nopa_var &lt;- opa_clean %&gt;%\n  dplyr::select(\n    sale_date, sale_price, market_value, building_code_description,\n    total_livable_area, number_of_bedrooms, number_of_bathrooms,\n    number_stories, garage_spaces, central_air, quality_grade,\n    interior_condition, exterior_condition, year_built, \n    off_street_open, zip_code, census_tract, zoning, owner_1,\n    category_code_description, shape, fireplaces\n  )\n\n\n\n1.1.3 Remove obvious errors & Handle missing values\n\n\n\nCode\n# ! check before remove NA value\n\ncat(\"&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\")\n\n\n&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\n\n\nCode\n#remove errors and drop rows with small NA counts in specific columns\nopa_var &lt;- opa_var %&gt;% \n  distinct() %&gt;%  #Remove duplicate lines\n  filter(\n    !is.na(total_livable_area) & total_livable_area &gt; 0,\n    !is.na(year_built) & year_built &gt; 0 & year_built &lt; 2025,\n    !is.na(number_of_bathrooms),\n    !is.na(fireplaces),\n    !is.na(interior_condition),\n    garage_spaces&lt;30\n  )   \n\n\n\n1.1.4 Other cleaning decisions\n\n\n\nCode\n#numeric quality_grade\nvalid_grades &lt;- c(\"A+\", \"A\", \"A-\", \n                  \"B+\", \"B\", \"B-\", \n                  \"C+\", \"C\", \"C-\", \n                  \"D+\", \"D\", \"D-\", \n                  \"E+\", \"E\", \"E-\")\n\nopa_var &lt;- opa_var %&gt;%\n  filter(quality_grade %in% valid_grades) %&gt;%\n  mutate(\n    quality_grade = factor(quality_grade, levels = valid_grades, ordered = TRUE),\n    quality_grade_num = rev(seq_along(valid_grades))[as.numeric(quality_grade)]\n  )\n\n#central_air (keep and transform the large NA values)\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    central_air_dummy = case_when(\n      central_air %in% c(1, \"Y\", \"y\") ~ 1,\n      central_air %in% c(0, \"N\", \"n\") ~ 0,\n      TRUE ~ NA_real_\n    ),\n    central_air_missing = if_else(is.na(central_air), 1, 0),\n    central_air_dummy = if_else(is.na(central_air_dummy), 0, central_air_dummy)\n  )\n\n#house age\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    house_age = 2025 - year_built\n  )\n\n\n\n1.1.5 Explore structural data biases and identify non-market transactions\n\n\n\nCode\n# check the relation of Sale Price ~ Market Value\noptions(scipen = 999)\nplot(opa_var$sale_price, opa_var$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value (Predicted)  vs  Sale Price (Actual)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# create a new column to record the identified non-market transactions\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    non_market = \n      (((sale_price &lt; 0.10*market_value) | sale_price &lt; 2000) | (sale_price&gt; 5*market_value))\n  )\n\n\nInterpretation: - The goal of this model is to predict typical, market-driven sale prices. The provided scatter plot of Market Value (Predicted) vs.¬†Sale Price (Actual) is an essential diagnostic tool for assessing data quality. - The red line on the plot represents a perfect 1-to-1 relationship (Y=X), where the property‚Äôs actual sale price is exactly equal to its predicted market value. While most data points cluster tightly around this line‚Äîindicating the ‚ÄúMarket Value‚Äù is a strong predictor‚Äîthe plot clearly reveals two distinct types of extreme outliers that do not represent typical market transactions. 1. Removing Non-Market Sales (The Sale Price &lt; 0.05 * Market Value Rule) - There is a dense cluster of points stacked vertically along the y-axis, where Sale Price (X) is near zero, but Market Value (Y) is high (e.g., $1M, $2M, even $4M). These points represent transactions where a property was ‚Äúsold‚Äù for a price that is a tiny fraction of its assessed worth (e.g., a $2,000,000 house sold for $50,000). - These are non-arm‚Äôs-length transactions, not true market sales. Examples include sales between family members, inheritance transfers, or other legal transactions where the price does not reflect the market. 2. Removing Anomalous High Sales (The Sale Price &gt; 5 * Market Value Rule) - There are several data points scattered far to the right and bottom of the plot, far below the red Y=X line. These points represent properties where the Sale Price (X) was massively higher than its Market Value (Y). For example, a property with an assessed value of $500,000 (Y) selling for $4,000,000 (X). - These are also not typical sales. They could represent (a) data entry errors, (b) sales where the ‚ÄúMarket Value‚Äù figure is obsolete (e.g., a run-down property sold for land value/development potential), or (c) properties that underwent major renovations not yet captured in the assessed value. - Retaining these two groups of outliers would severely skew the model‚Äôs coefficients and reduce its predictive accuracy for the vast majority of normal, market-based transactions. This filtering rule is a necessary step to clean the data, ensure the model learns from valid transactions, and improve its overall reliability.\n\n1.1.6 enhance the sales data\n\nWe have 8000+ non market transactions, that is 1/4 of the total sales data! That‚Äôs too big to let go, The model that we want to generate will become much more stable if we can make use of them.\n\n\nCode\n# filter the REAL MARKET data in the time period we need\nopa_selected &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n  )   \n\n#filter the NON MARKET data in the time period we need\nopa_non_market &lt;- opa_var %&gt;%\n  filter(\n    non_market ==1,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n    )\n\nopa_selected2 &lt;- opa_var\nopa_bonus &lt;- opa_var \n\n\n\n\nCode\n#try to find the relationship between market_value and sale_price\noptions(scipen = 999)\nplot(opa_selected$sale_price, opa_selected$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value vs  Sale Price (cleaned)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# That's quite linear, let's try to build a simple OLS model\nopa_mdata &lt;- opa_bonus %&gt;%\n  filter(\n    non_market == 0\n    )\n\nmodel_non &lt;- lm(sale_price ~ market_value, data = opa_mdata)\nsummary(model_non)\n\n\n\nCall:\nlm(formula = sale_price ~ market_value, data = opa_mdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1432832   -35207    -1669    30014  1975714 \n\nCoefficients:\n                 Estimate   Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  -9189.286424   663.771782  -13.84 &lt;0.0000000000000002 ***\nmarket_value     1.027924     0.001773  579.62 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 86310 on 40927 degrees of freedom\nMultiple R-squared:  0.8914,    Adjusted R-squared:  0.8914 \nF-statistic: 3.36e+05 on 1 and 40927 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\npred &lt;- predict(model_non, newdata = opa_mdata)\nresid &lt;- opa_mdata$sale_price - pred\n\n# RMSE\nrmse_value &lt;- sqrt(mean(resid^2, na.rm = TRUE))\nrmse_value\n\n\n[1] 86311.29\n\n\nThe results demonstrate a very strong linear relationship between sale_price and market_value. The market_value variable in the dataset effectively predicts the sale_price. Therefore, we can leverage this relationship to estimate the normal market prices for non-market transactions. We record these estimated values as sale_price_predicted. By doing this, we enhance our data!\n\n\nCode\n#bring data back\nopa_non_market$sale_price_predicted &lt;- predict(model_non, newdata = opa_non_market)\n\n#join back to the main data\nopa_selected &lt;- opa_selected %&gt;%\n  mutate(sale_price_predicted= sale_price)\n\nset.seed(123)\nopa_bind &lt;- bind_rows(opa_selected, opa_non_market) %&gt;%\n  slice_sample(prop = 1)\n\n\n1.2 Load and clean secondary dataset:\n\n1.2.1 Census data (tidycensus):\n\n\n\nCode\n# Transform to sf object \nopa_bind &lt;- st_as_sf(opa_bind, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(4326)\nopa_sf&lt;- opa_bind\n\n\n\n\nCode\n# Load Census data for Philadelphia tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    \n    ba_degree = \"B15003_022\",\n    total_edu = \"B15003_001\",\n    \n    median_income = \"B19013_001\",\n   \n    labor_force = \"B23025_003\",\n    unemployed = \"B23025_005\",\n    \n    total_housing = \"B25002_001\",\n    vacant_housing = \"B25002_003\"\n  ),\n  year = 2023,\n  state = \"PA\",\n  county = \"Philadelphia\",\n  geometry = TRUE\n) %&gt;%\n  dplyr::select(GEOID, variable, estimate, geometry) %&gt;%   \n  tidyr::pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  dplyr::mutate(\n    ba_rate = 100 * ba_degree / total_edu,\n    unemployment_rate = 100 * unemployed / labor_force,\n    vacancy_rate = 100 * vacant_housing / total_housing\n  ) %&gt;%\n  st_transform(st_crs(opa_sf))\n\n# Spatial join of OPA data with Census data\nopa_census &lt;- st_join(opa_sf, philly_census, join = st_within) %&gt;%\n  filter(!is.na(median_income))\n\n\n\n1.2.2 Spatial amenities (OpenDataPhilly)\n\n\n\nCode\n#load crime,poi,transit,hospital\nopa_census &lt;- st_transform(opa_census, 3857)\n\ncrime &lt;- read_csv(\"crime_sel.csv\") %&gt;% \n  filter(!is.na(lat) & !is.na(lng)) \ncrime_sf &lt;- st_as_sf(crime, coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census)) \n\npoi_sf &lt;- st_read(\"data/gis_osm_pois_a_free_1.shp\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census)) \n\nTransit &lt;- read_csv(\"Transit.csv\")\ntransit_sf &lt;- st_as_sf(Transit, coords = c(\"Lon\", \"Lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census))\n\nhospital_sf &lt;- st_read(\"hospitals.geojson\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census))\n\n\n1.3 Summary tables showing before/after dimensions\n\n\nCode\n# Original data dimensions\nopa_dims &lt;- tibble(\n  dataset = \"raw CSV\",\n  rows = nrow(opa),\n  columns = ncol(opa)\n)\n\n# cleaned data dimensions\nopa_filter_dims &lt;- tibble(\n  dataset = \"after fixed criteria\",\n  rows = nrow(opa_clean2),\n  columns = ncol(opa_clean2)\n)\n\nopa_selected_dims &lt;- tibble(\n  dataset = \"after cleaned\",\n  rows = nrow(opa_bind),\n  columns = ncol(opa_bind)\n)\n# data dimensions (within census tracts)\nopa_census_dims &lt;- tibble(\n  dataset = \"after census joined\",\n  rows = nrow(opa_census),\n  columns = ncol(opa_census)\n)\n\n# create summary table\nsummary_table &lt;- bind_rows(opa_dims, opa_filter_dims,opa_selected_dims, opa_census_dims)\nlibrary(knitr)\nsummary_table %&gt;%\n  kable(caption = \"Summary of OPA data before and after cleaning\")\n\n\n\nSummary of OPA data before and after cleaning\n\n\ndataset\nrows\ncolumns\n\n\n\n\nraw CSV\n153267\n79\n\n\nafter fixed criteria\n34559\n79\n\n\nafter cleaned\n31968\n28\n\n\nafter census joined\n31613\n40\n\n\n\n\n\nPrinciples of Data Processing\n\nopa Data\n\nFilter sale_date between 2023-01-01 and 2024-12-31.\n\nKeep only residential properties (category_code == 1).\n\nRemove records with missing values in total_livable_area, sale_price, or number_of_bathrooms.\n\nFilter records year_built &gt; 0 .\n\nCensus data\n\nLoad data including total_pop, ba_degree, total_edu, median_income, labor_force, unemployed, total_housing, vacant_housing.\nTransform to spatial format and remove records with missing values.\n\nSpatial amenities\n\nLoad datasets Transit, crime, POIs, Hospitals.\nTransform to spatial format and remove records with missing values.\n\n\nInterpretation: The selected variables can be grouped into several meaningful categories that are theoretically and empirically linked to housing prices:\n- Neighborhood Safety: - Crime data: Areas with lower crime rates are generally more desirable, leading to higher property values. Including crime incidents helps capture the effect of public safety on housing prices. - Accessibility and Transportation: - Transit points: Proximity to public transportation (e.g., bus stops, subway stations) increases accessibility and convenience, which is often capitalized into higher home values. - Points of Interest (POIs): Nearby amenities such as shops, restaurants, and parks improve quality of life and can positively influence housing prices. - Healthcare Access: - Hospitals: Easy access to healthcare facilities is a valued neighborhood characteristic, especially for families and older residents, and can contribute to higher property values. - Socioeconomic and Demographic Context (from Census data): - Total population: Indicates population density, which can affect demand for housing. - Educational attainment (e.g., % with BA degree): Higher education levels are often correlated with higher income and neighborhood desirability. - Median income: Directly influences purchasing power and demand for housing in an area. - Employment status (labor force and unemployment): Reflects economic stability and local job market health, which affect housing demand. - Housing market conditions (total and vacant housing): Vacancy rates can signal neighborhood decline or oversupply, both of which impact prices. - Together, these variables provide a multidimensional view of a neighborhood‚Äôs appeal, safety, convenience, and economic health‚Äîall key determinants of housing prices."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-1-data-preparation",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-1-data-preparation",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Load necessary libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE, tigris_class = \"sf\")\nlibrary(MASS)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(nngeo)\nlibrary(car)\nlibrary(knitr)\nlibrary(readr)\nlibrary(patchwork)\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  \n\n\n1.1 Load and clean Philadelphia sales data:\n\n1.1.1 Load data\n\n\n\nCode\nlibrary(tidyverse)\nopa &lt;- read_csv(\"opa_properties_public1.csv\")\n\n\n\n1.1.2 Filter to residential properties, 2023-2024 sales\n\n\n\nCode\n# data in 2022 will be used as predictor, so keep them as well.\nopa_clean &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# record the amount of data we will focus on\nopa_clean2 &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# Select relevant variables \nopa_var &lt;- opa_clean %&gt;%\n  dplyr::select(\n    sale_date, sale_price, market_value, building_code_description,\n    total_livable_area, number_of_bedrooms, number_of_bathrooms,\n    number_stories, garage_spaces, central_air, quality_grade,\n    interior_condition, exterior_condition, year_built, \n    off_street_open, zip_code, census_tract, zoning, owner_1,\n    category_code_description, shape, fireplaces\n  )\n\n\n\n1.1.3 Remove obvious errors & Handle missing values\n\n\n\nCode\n# ! check before remove NA value\n\ncat(\"&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\")\n\n\n&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\n\n\nCode\n#remove errors and drop rows with small NA counts in specific columns\nopa_var &lt;- opa_var %&gt;% \n  distinct() %&gt;%  #Remove duplicate lines\n  filter(\n    !is.na(total_livable_area) & total_livable_area &gt; 0,\n    !is.na(year_built) & year_built &gt; 0 & year_built &lt; 2025,\n    !is.na(number_of_bathrooms),\n    !is.na(fireplaces),\n    !is.na(interior_condition),\n    garage_spaces&lt;30\n  )   \n\n\n\n1.1.4 Other cleaning decisions\n\n\n\nCode\n#numeric quality_grade\nvalid_grades &lt;- c(\"A+\", \"A\", \"A-\", \n                  \"B+\", \"B\", \"B-\", \n                  \"C+\", \"C\", \"C-\", \n                  \"D+\", \"D\", \"D-\", \n                  \"E+\", \"E\", \"E-\")\n\nopa_var &lt;- opa_var %&gt;%\n  filter(quality_grade %in% valid_grades) %&gt;%\n  mutate(\n    quality_grade = factor(quality_grade, levels = valid_grades, ordered = TRUE),\n    quality_grade_num = rev(seq_along(valid_grades))[as.numeric(quality_grade)]\n  )\n\n#central_air (keep and transform the large NA values)\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    central_air_dummy = case_when(\n      central_air %in% c(1, \"Y\", \"y\") ~ 1,\n      central_air %in% c(0, \"N\", \"n\") ~ 0,\n      TRUE ~ NA_real_\n    ),\n    central_air_missing = if_else(is.na(central_air), 1, 0),\n    central_air_dummy = if_else(is.na(central_air_dummy), 0, central_air_dummy)\n  )\n\n#house age\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    house_age = 2025 - year_built\n  )\n\n\n\n1.1.5 Explore structural data biases and identify non-market transactions\n\n\n\nCode\n# check the relation of Sale Price ~ Market Value\noptions(scipen = 999)\nplot(opa_var$sale_price, opa_var$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value (Predicted)  vs  Sale Price (Actual)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# create a new column to record the identified non-market transactions\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    non_market = \n      (((sale_price &lt; 0.10*market_value) | sale_price &lt; 2000) | (sale_price&gt; 5*market_value))\n  )\n\n\nInterpretation: - The goal of this model is to predict typical, market-driven sale prices. The provided scatter plot of Market Value (Predicted) vs.¬†Sale Price (Actual) is an essential diagnostic tool for assessing data quality. - The red line on the plot represents a perfect 1-to-1 relationship (Y=X), where the property‚Äôs actual sale price is exactly equal to its predicted market value. While most data points cluster tightly around this line‚Äîindicating the ‚ÄúMarket Value‚Äù is a strong predictor‚Äîthe plot clearly reveals two distinct types of extreme outliers that do not represent typical market transactions. 1. Removing Non-Market Sales (The Sale Price &lt; 0.05 * Market Value Rule) - There is a dense cluster of points stacked vertically along the y-axis, where Sale Price (X) is near zero, but Market Value (Y) is high (e.g., $1M, $2M, even $4M). These points represent transactions where a property was ‚Äúsold‚Äù for a price that is a tiny fraction of its assessed worth (e.g., a $2,000,000 house sold for $50,000). - These are non-arm‚Äôs-length transactions, not true market sales. Examples include sales between family members, inheritance transfers, or other legal transactions where the price does not reflect the market. 2. Removing Anomalous High Sales (The Sale Price &gt; 5 * Market Value Rule) - There are several data points scattered far to the right and bottom of the plot, far below the red Y=X line. These points represent properties where the Sale Price (X) was massively higher than its Market Value (Y). For example, a property with an assessed value of $500,000 (Y) selling for $4,000,000 (X). - These are also not typical sales. They could represent (a) data entry errors, (b) sales where the ‚ÄúMarket Value‚Äù figure is obsolete (e.g., a run-down property sold for land value/development potential), or (c) properties that underwent major renovations not yet captured in the assessed value. - Retaining these two groups of outliers would severely skew the model‚Äôs coefficients and reduce its predictive accuracy for the vast majority of normal, market-based transactions. This filtering rule is a necessary step to clean the data, ensure the model learns from valid transactions, and improve its overall reliability.\n\n1.1.6 enhance the sales data\n\nWe have 8000+ non market transactions, that is 1/4 of the total sales data! That‚Äôs too big to let go, The model that we want to generate will become much more stable if we can make use of them.\n\n\nCode\n# filter the REAL MARKET data in the time period we need\nopa_selected &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n  )   \n\n#filter the NON MARKET data in the time period we need\nopa_non_market &lt;- opa_var %&gt;%\n  filter(\n    non_market ==1,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n    )\n\nopa_selected2 &lt;- opa_var\nopa_bonus &lt;- opa_var \n\n\n\n\nCode\n#try to find the relationship between market_value and sale_price\noptions(scipen = 999)\nplot(opa_selected$sale_price, opa_selected$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value vs  Sale Price (cleaned)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# That's quite linear, let's try to build a simple OLS model\nopa_mdata &lt;- opa_bonus %&gt;%\n  filter(\n    non_market == 0\n    )\n\nmodel_non &lt;- lm(sale_price ~ market_value, data = opa_mdata)\nsummary(model_non)\n\n\n\nCall:\nlm(formula = sale_price ~ market_value, data = opa_mdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1432832   -35207    -1669    30014  1975714 \n\nCoefficients:\n                 Estimate   Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  -9189.286424   663.771782  -13.84 &lt;0.0000000000000002 ***\nmarket_value     1.027924     0.001773  579.62 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 86310 on 40927 degrees of freedom\nMultiple R-squared:  0.8914,    Adjusted R-squared:  0.8914 \nF-statistic: 3.36e+05 on 1 and 40927 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\npred &lt;- predict(model_non, newdata = opa_mdata)\nresid &lt;- opa_mdata$sale_price - pred\n\n# RMSE\nrmse_value &lt;- sqrt(mean(resid^2, na.rm = TRUE))\nrmse_value\n\n\n[1] 86311.29\n\n\nThe results demonstrate a very strong linear relationship between sale_price and market_value. The market_value variable in the dataset effectively predicts the sale_price. Therefore, we can leverage this relationship to estimate the normal market prices for non-market transactions. We record these estimated values as sale_price_predicted. By doing this, we enhance our data!\n\n\nCode\n#bring data back\nopa_non_market$sale_price_predicted &lt;- predict(model_non, newdata = opa_non_market)\n\n#join back to the main data\nopa_selected &lt;- opa_selected %&gt;%\n  mutate(sale_price_predicted= sale_price)\n\nset.seed(123)\nopa_bind &lt;- bind_rows(opa_selected, opa_non_market) %&gt;%\n  slice_sample(prop = 1)\n\n\n1.2 Load and clean secondary dataset:\n\n1.2.1 Census data (tidycensus):\n\n\n\nCode\n# Transform to sf object \nopa_bind &lt;- st_as_sf(opa_bind, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(4326)\nopa_sf&lt;- opa_bind\n\n\n\n\nCode\n# Load Census data for Philadelphia tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    \n    ba_degree = \"B15003_022\",\n    total_edu = \"B15003_001\",\n    \n    median_income = \"B19013_001\",\n   \n    labor_force = \"B23025_003\",\n    unemployed = \"B23025_005\",\n    \n    total_housing = \"B25002_001\",\n    vacant_housing = \"B25002_003\"\n  ),\n  year = 2023,\n  state = \"PA\",\n  county = \"Philadelphia\",\n  geometry = TRUE\n) %&gt;%\n  dplyr::select(GEOID, variable, estimate, geometry) %&gt;%   \n  tidyr::pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  dplyr::mutate(\n    ba_rate = 100 * ba_degree / total_edu,\n    unemployment_rate = 100 * unemployed / labor_force,\n    vacancy_rate = 100 * vacant_housing / total_housing\n  ) %&gt;%\n  st_transform(st_crs(opa_sf))\n\n# Spatial join of OPA data with Census data\nopa_census &lt;- st_join(opa_sf, philly_census, join = st_within) %&gt;%\n  filter(!is.na(median_income))\n\n\n\n1.2.2 Spatial amenities (OpenDataPhilly)\n\n\n\nCode\n#load crime,poi,transit,hospital\nopa_census &lt;- st_transform(opa_census, 3857)\n\ncrime &lt;- read_csv(\"crime_sel.csv\") %&gt;% \n  filter(!is.na(lat) & !is.na(lng)) \ncrime_sf &lt;- st_as_sf(crime, coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census)) \n\npoi_sf &lt;- st_read(\"data/gis_osm_pois_a_free_1.shp\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census)) \n\nTransit &lt;- read_csv(\"Transit.csv\")\ntransit_sf &lt;- st_as_sf(Transit, coords = c(\"Lon\", \"Lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census))\n\nhospital_sf &lt;- st_read(\"hospitals.geojson\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census))\n\n\n1.3 Summary tables showing before/after dimensions\n\n\nCode\n# Original data dimensions\nopa_dims &lt;- tibble(\n  dataset = \"raw CSV\",\n  rows = nrow(opa),\n  columns = ncol(opa)\n)\n\n# cleaned data dimensions\nopa_filter_dims &lt;- tibble(\n  dataset = \"after fixed criteria\",\n  rows = nrow(opa_clean2),\n  columns = ncol(opa_clean2)\n)\n\nopa_selected_dims &lt;- tibble(\n  dataset = \"after cleaned\",\n  rows = nrow(opa_bind),\n  columns = ncol(opa_bind)\n)\n# data dimensions (within census tracts)\nopa_census_dims &lt;- tibble(\n  dataset = \"after census joined\",\n  rows = nrow(opa_census),\n  columns = ncol(opa_census)\n)\n\n# create summary table\nsummary_table &lt;- bind_rows(opa_dims, opa_filter_dims,opa_selected_dims, opa_census_dims)\nlibrary(knitr)\nsummary_table %&gt;%\n  kable(caption = \"Summary of OPA data before and after cleaning\")\n\n\n\nSummary of OPA data before and after cleaning\n\n\ndataset\nrows\ncolumns\n\n\n\n\nraw CSV\n153267\n79\n\n\nafter fixed criteria\n34559\n79\n\n\nafter cleaned\n31968\n28\n\n\nafter census joined\n31613\n40\n\n\n\n\n\nPrinciples of Data Processing\n\nopa Data\n\nFilter sale_date between 2023-01-01 and 2024-12-31.\n\nKeep only residential properties (category_code == 1).\n\nRemove records with missing values in total_livable_area, sale_price, or number_of_bathrooms.\n\nFilter records year_built &gt; 0 .\n\nCensus data\n\nLoad data including total_pop, ba_degree, total_edu, median_income, labor_force, unemployed, total_housing, vacant_housing.\nTransform to spatial format and remove records with missing values.\n\nSpatial amenities\n\nLoad datasets Transit, crime, POIs, Hospitals.\nTransform to spatial format and remove records with missing values.\n\n\nInterpretation: The selected variables can be grouped into several meaningful categories that are theoretically and empirically linked to housing prices:\n- Neighborhood Safety: - Crime data: Areas with lower crime rates are generally more desirable, leading to higher property values. Including crime incidents helps capture the effect of public safety on housing prices. - Accessibility and Transportation: - Transit points: Proximity to public transportation (e.g., bus stops, subway stations) increases accessibility and convenience, which is often capitalized into higher home values. - Points of Interest (POIs): Nearby amenities such as shops, restaurants, and parks improve quality of life and can positively influence housing prices. - Healthcare Access: - Hospitals: Easy access to healthcare facilities is a valued neighborhood characteristic, especially for families and older residents, and can contribute to higher property values. - Socioeconomic and Demographic Context (from Census data): - Total population: Indicates population density, which can affect demand for housing. - Educational attainment (e.g., % with BA degree): Higher education levels are often correlated with higher income and neighborhood desirability. - Median income: Directly influences purchasing power and demand for housing in an area. - Employment status (labor force and unemployment): Reflects economic stability and local job market health, which affect housing demand. - Housing market conditions (total and vacant housing): Vacancy rates can signal neighborhood decline or oversupply, both of which impact prices. - Together, these variables provide a multidimensional view of a neighborhood‚Äôs appeal, safety, convenience, and economic health‚Äîall key determinants of housing prices."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-2-feature-engineering",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-2-feature-engineering",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 2: Feature Engineering",
    "text": "Phase 2: Feature Engineering\n2.1 Buffer-based features:\n\n2.1.1 neighborhood avg sale price in the past year\n\n\n\nCode\n#filter the past sales data\n\nopa_census &lt;- opa_census %&gt;%\n  mutate(sale_id = row_number())\n\n\n\nopa_past &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2022-12-31\"\n  ) \n\nopa_past &lt;- opa_past %&gt;%\n  mutate(sale_id2 = row_number())\n\nopa_past &lt;- st_as_sf(opa_past, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(3857)\n\nopa_census &lt;- st_transform(opa_census, 3857)\nopa_past   &lt;- st_transform(opa_past, 3857)\n\nopa_census_buffer &lt;- st_buffer(opa_census, 300)\ndrop_cols &lt;- names(opa_census_buffer) %in% c(\"sale_price\", \"total_livable_area\",\n                                             \"sale_price.y\", \"total_livable_area.y\")\nopa_census_buffer &lt;- opa_census_buffer[ , !drop_cols, drop = FALSE]\n\njoin_result &lt;- st_join(\n  opa_census_buffer,\n  opa_past,\n  join = st_intersects,\n  left = TRUE\n)\n\n\njoin_result &lt;- st_join(\n  opa_census_buffer,\n  opa_past,\n  join = st_intersects,\n  left = TRUE\n)\njoin_dedup &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  distinct(sale_id, sale_id2, .keep_all = TRUE)\n\nopa_summary &lt;- join_dedup %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(\n    past_count = sum(!is.na(sale_id2)),\n    avg_past_price_density = ifelse(\n      sum(!is.na(total_livable_area)) == 0, NA_real_,\n      sum(sale_price, na.rm = TRUE) / sum(total_livable_area, na.rm = TRUE)\n    ),\n    .groups = \"drop\"\n  )\nopa_census &lt;- opa_census %&gt;%\n  left_join(opa_summary, by = \"sale_id\")\n\n\n\n2.1.2 crime numbers\n\n\n\nCode\nradius_cri &lt;- 250 \n\nopa_census$crime_count &lt;- lengths(st_is_within_distance(opa_census, crime_sf, dist = radius_cri)) \n\n\n\n2.1.3 POI numbers\n\n\n\nCode\nopa_census_m &lt;- st_transform(opa_census, 3857)\npoi_sf_m     &lt;- st_transform(poi_sf, 3857)\n\n\nradius_poi &lt;- 400\nopa_census_buffer &lt;- st_buffer(opa_census_m, radius_poi)\n\njoin_result &lt;- st_join(opa_census_buffer, poi_sf_m, join = st_intersects, left = TRUE)\n\npoi_summary &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(poi_count = sum(!is.na(osm_id)))  \n\nopa_census &lt;- opa_census_m %&gt;%\n  left_join(poi_summary, by = \"sale_id\")\n\n\n\n2.1.4 Transit numbers\n\n\n\nCode\nopa_census_m &lt;- st_transform(opa_census, 3857)\ntransit_sf_m &lt;- st_transform(transit_sf, 3857)\n\nradius_ts &lt;- 400\nopa_census_buffer &lt;- st_buffer(opa_census_m, radius_ts)\n\njoin_result &lt;- st_join(opa_census_buffer, transit_sf_m, join = st_intersects, left = TRUE)\n\ntransit_summary &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(transit_count = sum(!is.na(FID)))  \n\nopa_census &lt;- opa_census_m %&gt;%\n  left_join(transit_summary, by = \"sale_id\")\n\n\n2.2 k-Nearest Neighbor features:\n\n2.2.1 Hospitals (KNN-3)\n\n\n\nCode\nnearest_hospital_index &lt;- st_nn(\n  opa_census,\n  hospital_sf,\n  k = 3,            \n  returnDist = TRUE \n)\n\nopa_census$nearest_hospital_knn3 &lt;- sapply(nearest_hospital_index$dist, mean)\n\n\n2.3 Weights:\n\n\nCode\n# add different weights to actual and non market transactions\n\nopa_census_all &lt;- opa_census\n\nnon_market_share&lt;- mean(opa_census_all$non_market == 1, na.rm = TRUE)\nnon_market_share\n\n\n[1] 0.261791\n\n\nCode\nopa_census_all &lt;- opa_census %&gt;%\n  mutate(weight_mix = ifelse(non_market == 1, non_market_share, 1))\n\n\n2.4 Transformation:\n\n\nCode\n#standardize houseage and median income\nmean_age_all &lt;- mean(opa_census_all$house_age, na.rm = TRUE)\nmean_income_log &lt;- mean(log(opa_census_all$median_income), na.rm = TRUE)\n\n# centralize\nopa_census_all &lt;- opa_census_all %&gt;%\n  mutate(\n    house_age_c  = house_age - mean_age_all,\n    house_age_c2 = house_age_c^2,\n    income_log   = log(median_income),\n    income_scaled = income_log - mean_income_log\n  )\n\nopa_census_2&lt;- opa_census_all %&gt;%\n  filter(\n    non_market==0\n  )"
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-3-exploratory-data-analysis",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-3-exploratory-data-analysis",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 3: Exploratory Data Analysis",
    "text": "Phase 3: Exploratory Data Analysis\n\n3.1 Distribution of sale prices (histogram)\n\n\nCode\nggplot(opa_census_all, aes(x = sale_price_predicted)) +\n  geom_histogram(\n    bins = 50,                 \n    fill = \"#6A1B9A\",          \n    color = \"white\",           \n    alpha = 0.8                \n  ) +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Distribution of  Sale Prices\",\n    x = \"Predicted Sale Price (USD)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(opa_census_all, aes(x = log(sale_price_predicted))) +\n  geom_histogram(\n    bins = 50,                 \n    fill = \"#6A1B9A\",          \n    color = \"white\",           \n    alpha = 0.8               \n  ) +\n  scale_x_continuous() +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Distribution of  Log(Sale Prices)\",\n    x = \"Log(Predicted Sale Price) \",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\nKey Findings: - Highly Right-Skewed: The bulk of the distribution is concentrated on the left side (low price range), while the right tail is very long, extending towards higher prices. This means that the majority of houses have lower prices, and very expensive houses are very few in number.\n- Concentration and Outliers: The count of houses in each bin drops rapidly as the price increases. The number of houses above $2,500,000 is very small, indicating the presence of extreme high-price outliers.\n- Preprocessing Requirement: This distribution strongly suggests that before building a house price prediction model, the Sale Price variable will need a transformation, most commonly a log transformation, to make the distribution more closely resemble a normal distribution.\n\n\n3.2 Spatial distribution of sale prices (map)\n\n\nCode\nopa_census_all &lt;- opa_census_all %&gt;%\n  mutate(price_quartile = ntile(sale_price_predicted, 4))\n\nggplot() +\n  geom_sf(data = philly_census, fill = \"lightgrey\", color = \"white\") +\n  geom_sf(data = opa_census_all, aes(color = factor(price_quartile)), size = 0.5, alpha = 0.7) +\n  scale_color_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    labels = c(\"0%-25%\", \"25%-50%\", \"50%-75%\", \"75%-100%\")\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Housing Sales Price in Philadelphia (2023‚Äì2024)\",\n    color = \"Sale Price Quartile\"\n  )  +\n  guides(color = guide_legend(override.aes = list(size = 3)))\n\n\n\n\n\n\n\n\n\nKey Findings:\n- Spatial Concentration of Housing Prices: Highest Prices are clearly concentrated in the Center City/Downtown area of Philadelphia and its immediate surroundings, which indicates that the most expensive property transactions occur in the high-value areas of and around the city center.\n- Discontinuous Price Transitions: Housing prices do not exhibit smooth gradients across the city; instead, they show abrupt changes between different price quartiles, forming distinct spatial clusters. This suggests that price variations are influenced by fixed boundaries such as neighborhoods, infrastructure, or socio-economic factors, rather than continuous spatial diffusion. - Peripheral Price Patterns: Lower-price quartiles (0%-25% and 25%-50%) are predominantly located in the outer regions of the city, particularly in the northern and southern peripheries, indicating a clear core-periphery divide in housing values.\n\n\n3.3 Price vs.¬†structural features (scatter plots)\n\n\nCode\nopa_census_plot &lt;- opa_census_all %&gt;%\n  mutate(log_sale_price = log(sale_price_predicted))\n\n# 1Ô∏è‚É£ total_livable_area\np1 &lt;- ggplot(opa_census_plot, aes(x = log(total_livable_area), y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Log(Livable Area)\",\n    x = \"Log(Total Livable Area)\",\n    y = \"Log(Sale Price)\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 2Ô∏è‚É£ number_of_bathrooms\np2 &lt;- ggplot(opa_census_plot, aes(x = number_of_bathrooms, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Bathrooms\",\n    x = \"Number of Bathrooms\",  \n    y = \"\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 3Ô∏è‚É£ interior_condition\np3 &lt;- ggplot(opa_census_plot, aes(x = interior_condition, y = log_sale_price)) +\n  geom_jitter(alpha = 0.5, color = \"#6A1B9A\", width = 0.2, height = 0) +  \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Interior Condition\",\n    x = \"Interior Condition\",\n    y = \"Log(Sale Price)\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 4Ô∏è‚É£ house_age\np4 &lt;- ggplot(opa_census_plot, aes(x = house_age^2, y = log_sale_price)) + \n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. House Age¬≤\",\n    x = \"House Age¬≤ (2025 - Year Built)¬≤\",\n    y = \"\"\n  ) +\n  theme_minimal(base_size = 10)\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\nquarto check Key Findings:\n- Strong Positive Correlation with Size and Bathrooms: There is a clear positive linear relationship between the log of sale price and both the log of total livable area and the number of bathrooms. This indicates that larger properties and those with more bathrooms command significantly higher market prices. - Non-Linear Relationship with Interior Condition: While a general positive trend exists, the relationship between log sale price and interior condition rating is not perfectly linear. The data dispersion suggests that the effect of interior condition on price may be subject to diminishing marginal returns or other non-linear dynamics. - Negative Correlation with House Age Squared: A significant negative relationship is observed between log sale price and the squared term of house age. This indicates that property values depreciate as homes get older, and this depreciation effect may accelerate over time, reflecting a non-linear aging effect on housing value.\n\n\n3.4 Price vs.¬†spatial & Social features (scatter plots)\n\n\nCode\nopa_census_plot &lt;- opa_census_all %&gt;%\n  mutate(\n    log_sale_price = log(sale_price_predicted),\n    sqrt_crime_count = sqrt(crime_count)\n  )\n\np1 &lt;- ggplot(opa_census_plot, aes(x = ba_rate, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  labs(title = \"Log(Sale Price) vs. BA Rate\",\n       x = \"Bachelor's Degree Rate\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np2 &lt;- ggplot(opa_census_plot, aes(x = unemployment_rate, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  labs(title = \"Log(Sale Price) vs. Unemployment Rate\",\n       x = \"Unemployment Rate\", y = \"\") +\n  theme_minimal(base_size = 10)\n\np3 &lt;- ggplot(opa_census_plot, aes(x = median_income, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = dollar_format()) +\n  labs(title = \"Log(Sale Price) vs. Median Income\",\n       x = \"Median Household Income (USD)\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np4 &lt;- ggplot(opa_census_plot, aes(x = avg_past_price_density, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. Past Price Density\",\n       x = \"Average Past Price Density\", y = \"\") +\n  theme_minimal(base_size = 10)\n\np5 &lt;- ggplot(opa_census_plot, aes(x = sqrt_crime_count, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. ‚àö(Crime Count)\",\n       x = \"Square Root of Crime Count\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np6 &lt;- ggplot(opa_census_plot, aes(x = transit_count, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. Transit Count\",\n       x = \"Number of Transit Stops Nearby\", y = \"\") +\n  theme_minimal(base_size = 10)\n\n(p1 | p2 | p3) / (p4 | p5 | p6)\n\n\n\n\n\n\n\n\n\nKey Findings:\n- Strong Socioeconomic Influence: Housing prices show clear positive correlations with key socioeconomic indicators. Both bachelor‚Äôs degree rate and median household income exhibit strong positive relationships with sale prices, indicating that neighborhoods with higher educational attainment and income levels command substantially higher property values. - Negative Impact of Crime and Unemployment: There are evident negative relationships between housing prices and both crime levels (measured by square root of crime count) and unemployment rates. This demonstrates that public safety and local economic vitality are significant determinants of property values in Philadelphia. - Positive Effects of Historical Prices and Transit Access: Sale prices maintain a positive relationship with both historical price density and accessibility to public transportation. This suggests that areas with established high-value characteristics and better transit infrastructure maintain their premium in the housing market, reflecting path dependence in neighborhood valuation and the value of transportation accessibility.\n\n\nOther visualization\n\n\nCode\ntract_price &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(avg_past_price_density = mean(avg_past_price_density, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_price, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = avg_past_price_density), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"white\", high = \"#6A1B9A\",\n    name = \"Mean Sale Price Per sqft\",\n    labels = scales::dollar_format(),\n    na.value = \"grey60\"\n  ) +\n  scale_alpha(range = c(0.2, 0.8), name = \"Interior Condition\") +\n  \n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Buffered Area Mean Sold Price of 2022\",\n    subtitle = \"clustered in census tracts\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ntract_condition &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_interior_condition = mean(interior_condition, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_condition, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_interior_condition), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"#6A1B9A\", high = \"white\",      \n    name = \"Avg Interior Condition\",\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Average Interior Condition by Census Tract\",\n    subtitle = \"Darker color = better average condition\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ntract_area &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_total_livable_area = mean(total_livable_area, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_area, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_total_livable_area), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"white\", high = \"#6A1B9A\",   \n    name = \"Avg Livable Area (sqft)\",\n    labels = scales::comma_format(),\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Average Livable Area by Census Tract\",\n    subtitle = \"Darker color indicates larger average livable area\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\nKey Findings:\n- Spatial Variation in Property Values: The average sale price per square foot shows significant geographic clustering across census tracts, with distinct high-value areas concentrated in specific neighborhoods. This indicates strong spatial autocorrelation in housing prices, where adjacent tracts tend to have similar price levels. - Correlation Between Property Condition and Location: Better average interior conditions are systematically concentrated in particular geographic areas, suggesting that housing maintenance and quality are not randomly distributed but follow spatial patterns that may correlate with neighborhood characteristics and property values. - Heterogeneous Distribution of Housing Size: The average livable area varies substantially across census tracts, with larger properties clustered in specific regions. This spatial patterning of housing size complements the price distribution, indicating that both property characteristics and location factors contribute to the overall housing market structure in Philadelphia."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-4-model-building",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-4-model-building",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 4: Model Building",
    "text": "Phase 4: Model Building\n\nBuild models progressively\n4.1 Structural features only:\n\n\nCode\nmodel_1 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +  #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing, \n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_1)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing, data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-3.2930 -0.2144  0.0510  0.2911  2.4585 \n\nCoefficients:\n                            Estimate   Std. Error t value            Pr(&gt;|t|)\n(Intercept)              6.435857858  0.077629460  82.905 &lt;0.0000000000000002\nlog(total_livable_area)  0.744348571  0.010689344  69.635 &lt;0.0000000000000002\nnumber_of_bathrooms      0.051667122  0.005580674   9.258 &lt;0.0000000000000002\nhouse_age_c              0.000002273  0.000118374   0.019               0.985\nhouse_age_c2             0.000046457  0.000001746  26.607 &lt;0.0000000000000002\ninterior_condition      -0.112636239  0.004358818 -25.841 &lt;0.0000000000000002\nquality_grade_num        0.069613279  0.002849720  24.428 &lt;0.0000000000000002\nfireplaces               0.116405937  0.010884143  10.695 &lt;0.0000000000000002\ngarage_spaces            0.140429585  0.006549989  21.440 &lt;0.0000000000000002\ncentral_air_dummy        0.458743112  0.008036173  57.085 &lt;0.0000000000000002\ncentral_air_missing     -0.237637667  0.008809496 -26.975 &lt;0.0000000000000002\n                           \n(Intercept)             ***\nlog(total_livable_area) ***\nnumber_of_bathrooms     ***\nhouse_age_c                \nhouse_age_c2            ***\ninterior_condition      ***\nquality_grade_num       ***\nfireplaces              ***\ngarage_spaces           ***\ncentral_air_dummy       ***\ncentral_air_missing     ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4911 on 31602 degrees of freedom\nMultiple R-squared:  0.5212,    Adjusted R-squared:  0.521 \nF-statistic:  3439 on 10 and 31602 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation:\n- log(total_livable_area) (0.752): An elasticity coefficient. A 1% increase in livable area is associated with a 0.752% increase in price. This is a strong positive driver. - number_of_bathrooms (0.046): Each additional bathroom is associated with a 4.6% increase in price. - house_age_c (-0.00001): The linear term for house age is statistically insignificant (p=0.929). - house_age_c2 (0.000047): The squared term for age is positive and significant. Combined with the insignificant linear term, this suggests a slight U-shaped relationship, where new homes and very old homes (perhaps with historical value) command a premium over middle-aged homes. - interior_condition (-0.114): Assuming a higher value means worse condition, each one-unit worsening in condition is associated with an 11.4% decrease in price. - quality_grade_num (0.070): Each one-unit increase in the quality grade is associated with a 7.0% increase in price. - fireplaces (0.117): Each additional fireplace is associated with a 11.7% increase in price. - garage_spaces (0.143): Each additional garage space is associated with a 14.3% increase in price. - central_air_dummy (0.458): Homes with central air are estimated to be 45.8% more expensive than the baseline (e.g., no AC). This is a very significant amenity premium. - central_air_missing (-0.230): Homes where central air data is missing are 23.0% cheaper than the baseline.\n4.2 Census variables:\n\n\nCode\nmodel_2 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census\n                  ba_rate +\n                  unemployment_rate,\n  \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_2)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate, \n    data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-3.2514 -0.1415  0.0546  0.2237  2.0880 \n\nCoefficients:\n                            Estimate   Std. Error t value             Pr(&gt;|t|)\n(Intercept)              7.279501574  0.064443586 112.959 &lt; 0.0000000000000002\nlog(total_livable_area)  0.700431343  0.008755698  79.997 &lt; 0.0000000000000002\nnumber_of_bathrooms      0.057764788  0.004568488  12.644 &lt; 0.0000000000000002\nhouse_age_c             -0.000064246  0.000097377  -0.660                0.509\nhouse_age_c2             0.000009814  0.000001468   6.686    0.000000000023255\ninterior_condition      -0.126482820  0.003572164 -35.408 &lt; 0.0000000000000002\nquality_grade_num        0.001702051  0.002417186   0.704                0.481\nfireplaces               0.064233267  0.008917462   7.203    0.000000000000602\ngarage_spaces            0.168324520  0.005472052  30.761 &lt; 0.0000000000000002\ncentral_air_dummy        0.219136581  0.006851612  31.983 &lt; 0.0000000000000002\ncentral_air_missing     -0.155840316  0.007252663 -21.487 &lt; 0.0000000000000002\nincome_scaled            0.453407655  0.009052596  50.086 &lt; 0.0000000000000002\nba_rate                  0.012813063  0.000358216  35.769 &lt; 0.0000000000000002\nunemployment_rate       -0.006619614  0.000527486 -12.549 &lt; 0.0000000000000002\n                           \n(Intercept)             ***\nlog(total_livable_area) ***\nnumber_of_bathrooms     ***\nhouse_age_c                \nhouse_age_c2            ***\ninterior_condition      ***\nquality_grade_num          \nfireplaces              ***\ngarage_spaces           ***\ncentral_air_dummy       ***\ncentral_air_missing     ***\nincome_scaled           ***\nba_rate                 ***\nunemployment_rate       ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4017 on 31599 degrees of freedom\nMultiple R-squared:  0.6796,    Adjusted R-squared:  0.6794 \nF-statistic:  5155 on 13 and 31599 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation:\n- Coefficient Evolution (vs.¬†Model 1): - log(total_livable_area) (0.710 vs 0.752): The elasticity of area decreased. This suggests Model 1 overestimated the impact of area. Why? Because larger homes are often located in wealthier neighborhoods. Model 1 incorrectly attributed some of the ‚Äúwealthy neighborhood‚Äù premium to ‚Äúlarge area.‚Äù - quality_grade_num (0.0015 vs 0.070): The coefficient for quality grade became statistically insignificant (p=0.520). This is a key finding: home quality is highly correlated with neighborhood income. Once we directly control for income (income_scaled), the independent effect of quality grade disappears. - central_air_dummy (0.219 vs 0.458): The premium for central air was halved. This also indicates that central air is more common in affluent areas, and Model 1 suffered from significant Omitted Variable Bias (OVB). - New Variable (Census) Interpretation: - income_scaled (0.455): A one-unit increase in standardized census tract income is associated with a 45.5% increase in price. A very strong positive effect. - ba_rate (0.0129): A 1 percentage point increase in the neighborhood‚Äôs bachelor‚Äôs degree attainment rate is associated with a 1.3% price increase. - unemployment_rate (-0.0066): A 1 percentage point increase in the unemployment rate is associated with a 0.66% decrease in price.\n4.3 Spatial features:\n\n\nCode\nmodel_3 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census \n                  ba_rate +\n                  unemployment_rate +\n                \n                  transit_count+\n                  avg_past_price_density+      #Spatial \n                  sqrt(crime_count) +\n                  log(nearest_hospital_knn3),\n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_3)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate + \n    transit_count + avg_past_price_density + sqrt(crime_count) + \n    log(nearest_hospital_knn3), data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-3.03217 -0.09905  0.05666  0.18464  2.17114 \n\nCoefficients:\n                               Estimate   Std. Error t value\n(Intercept)                 6.562309985  0.084718417  77.460\nlog(total_livable_area)     0.742766329  0.007908779  93.917\nnumber_of_bathrooms         0.057510788  0.004057815  14.173\nhouse_age_c                -0.000093444  0.000087876  -1.063\nhouse_age_c2               -0.000005113  0.000001322  -3.866\ninterior_condition         -0.161139310  0.003179231 -50.685\nquality_grade_num          -0.029091664  0.002254432 -12.904\nfireplaces                  0.031025554  0.008023515   3.867\ngarage_spaces               0.096993528  0.005071631  19.125\ncentral_air_dummy           0.099045830  0.006176862  16.035\ncentral_air_missing        -0.164351875  0.006393736 -25.705\nincome_scaled               0.166039882  0.008635180  19.228\nba_rate                     0.003250996  0.000349765   9.295\nunemployment_rate          -0.002568034  0.000466955  -5.500\ntransit_count               0.000311944  0.000171205   1.822\navg_past_price_density      0.003018698  0.000039395  76.626\nsqrt(crime_count)          -0.049042531  0.001408152 -34.828\nlog(nearest_hospital_knn3)  0.081937118  0.006616703  12.383\n                                       Pr(&gt;|t|)    \n(Intercept)                &lt; 0.0000000000000002 ***\nlog(total_livable_area)    &lt; 0.0000000000000002 ***\nnumber_of_bathrooms        &lt; 0.0000000000000002 ***\nhouse_age_c                            0.287626    \nhouse_age_c2                           0.000111 ***\ninterior_condition         &lt; 0.0000000000000002 ***\nquality_grade_num          &lt; 0.0000000000000002 ***\nfireplaces                             0.000110 ***\ngarage_spaces              &lt; 0.0000000000000002 ***\ncentral_air_dummy          &lt; 0.0000000000000002 ***\ncentral_air_missing        &lt; 0.0000000000000002 ***\nincome_scaled              &lt; 0.0000000000000002 ***\nba_rate                    &lt; 0.0000000000000002 ***\nunemployment_rate                  0.0000000384 ***\ntransit_count                          0.068458 .  \navg_past_price_density     &lt; 0.0000000000000002 ***\nsqrt(crime_count)          &lt; 0.0000000000000002 ***\nlog(nearest_hospital_knn3) &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3536 on 31481 degrees of freedom\n  (114 observations deleted due to missingness)\nMultiple R-squared:  0.7505,    Adjusted R-squared:  0.7504 \nF-statistic:  5571 on 17 and 31481 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation:\n- Coefficient Evolution (vs.¬†Model 2): - income_scaled (0.146 vs 0.455): The effect of income dropped sharply (by ~2/3). This again reveals OVB in Model 2. The large ‚Äúincome‚Äù effect in Model 2 was confounded with ‚Äúspatial amenities‚Äù‚Äîhigh-income individuals tend to live in low-crime, accessible areas. - ba_rate (0.0027 vs 0.0129): The education premium also dropped significantly for the same reason. - garage_spaces (0.095 vs 0.170): The garage premium decreased, likely because spatial variables (like density or transit access) have captured related information. - New Variable (Spatial) Interpretation: - transit_count (0.00029): Each additional nearby public transit stop is associated with a 0.029% increase in price. - avg_past_price_density (0.0032): As a proxy for local market heat or locational value, each unit increase is associated with a 0.32% price increase. - sqrt(crime_count) (-0.040): A one-unit increase in the square root of the crime count is associated with a 4.0% decrease in price. - log(nearest_hospital_knn3) (0.087): A 1% increase in the distance from the nearest hospital is associated with a 0.087% increase in price. This suggests people prefer to live further away from hospitals (perhaps to avoid noise, traffic, or sirens), not closer.\n4.4 Interactions and fixed effects:\n\n\nCode\nmodel_4 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census \n                  ba_rate +\n                  unemployment_rate +\n                \n                  transit_count+\n                  avg_past_price_density+      #Spatial \n                  sqrt(crime_count) +\n                  log(nearest_hospital_knn3)+\n                \n                  (interior_condition * income_scaled)+  #FE & Interaction\n                  factor(zip_code),\n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_4)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate + \n    transit_count + avg_past_price_density + sqrt(crime_count) + \n    log(nearest_hospital_knn3) + (interior_condition * income_scaled) + \n    factor(zip_code), data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-2.86007 -0.09067  0.05500  0.16974  2.17553 \n\nCoefficients:\n                                     Estimate   Std. Error t value\n(Intercept)                       7.147174262  0.125631514  56.890\nlog(total_livable_area)           0.762702187  0.007951214  95.923\nnumber_of_bathrooms               0.062545809  0.003951966  15.827\nhouse_age_c                       0.000081673  0.000091303   0.895\nhouse_age_c2                      0.000002396  0.000001333   1.797\ninterior_condition               -0.167693153  0.003180473 -52.726\nquality_grade_num                -0.020614687  0.002369293  -8.701\nfireplaces                        0.040919276  0.008065426   5.073\ngarage_spaces                     0.062755291  0.005224896  12.011\ncentral_air_dummy                 0.088693723  0.006224102  14.250\ncentral_air_missing              -0.145545244  0.006529396 -22.291\nincome_scaled                    -0.308308801  0.022099101 -13.951\nba_rate                           0.004727434  0.000434125  10.890\nunemployment_rate                -0.002009575  0.000512327  -3.922\ntransit_count                     0.000210309  0.000188166   1.118\navg_past_price_density            0.002676947  0.000053851  49.710\nsqrt(crime_count)                -0.040746561  0.001633811 -24.940\nlog(nearest_hospital_knn3)       -0.007368586  0.012879355  -0.572\nfactor(zip_code)19103            -0.191888874  0.038623356  -4.968\nfactor(zip_code)19104             0.043168643  0.044596804   0.968\nfactor(zip_code)19106            -0.072911591  0.039431744  -1.849\nfactor(zip_code)19107             0.002870411  0.041113568   0.070\nfactor(zip_code)19111             0.092033842  0.042730139   2.154\nfactor(zip_code)19114             0.046991631  0.045784241   1.026\nfactor(zip_code)19115             0.036786258  0.045577032   0.807\nfactor(zip_code)19116             0.074693711  0.047046924   1.588\nfactor(zip_code)19118             0.042277408  0.050964443   0.830\nfactor(zip_code)19119            -0.005571634  0.045415173  -0.123\nfactor(zip_code)19120            -0.001979921  0.042451411  -0.047\nfactor(zip_code)19121            -0.079268931  0.042967693  -1.845\nfactor(zip_code)19122            -0.046067555  0.043677075  -1.055\nfactor(zip_code)19123            -0.124474312  0.042021722  -2.962\nfactor(zip_code)19124            -0.042922550  0.042779720  -1.003\nfactor(zip_code)19125            -0.053924641  0.040136398  -1.344\nfactor(zip_code)19126            -0.096134211  0.050099576  -1.919\nfactor(zip_code)19127            -0.054620383  0.046526938  -1.174\nfactor(zip_code)19128            -0.063379705  0.041111172  -1.542\nfactor(zip_code)19129            -0.081840032  0.044983229  -1.819\nfactor(zip_code)19130             0.004465219  0.038447508   0.116\nfactor(zip_code)19131            -0.124963371  0.044224170  -2.826\nfactor(zip_code)19132            -0.353731640  0.042632020  -8.297\nfactor(zip_code)19133            -0.360578685  0.045760207  -7.880\nfactor(zip_code)19134            -0.167019067  0.042150583  -3.962\nfactor(zip_code)19135             0.066237684  0.045406809   1.459\nfactor(zip_code)19136             0.093091097  0.045399194   2.051\nfactor(zip_code)19137             0.003922355  0.050119037   0.078\nfactor(zip_code)19138            -0.061547285  0.045851962  -1.342\nfactor(zip_code)19139            -0.125514020  0.043923079  -2.858\nfactor(zip_code)19140            -0.243638620  0.042070333  -5.791\nfactor(zip_code)19141            -0.104140516  0.045656220  -2.281\nfactor(zip_code)19142            -0.117894811  0.045192100  -2.609\nfactor(zip_code)19143            -0.121037876  0.042647300  -2.838\nfactor(zip_code)19144            -0.119218035  0.044443147  -2.682\nfactor(zip_code)19145             0.000416732  0.040490607   0.010\nfactor(zip_code)19146            -0.065296494  0.038364344  -1.702\nfactor(zip_code)19147            -0.022134932  0.038396424  -0.576\nfactor(zip_code)19148             0.034927326  0.039935049   0.875\nfactor(zip_code)19149             0.163264326  0.043056311   3.792\nfactor(zip_code)19150            -0.044481749  0.048200789  -0.923\nfactor(zip_code)19151            -0.087776350  0.045219855  -1.941\nfactor(zip_code)19152             0.091653850  0.044520396   2.059\nfactor(zip_code)19153            -0.033554531  0.052491824  -0.639\nfactor(zip_code)19154             0.056967215  0.044374083   1.284\ninterior_condition:income_scaled  0.119746969  0.005588581  21.427\n                                             Pr(&gt;|t|)    \n(Intercept)                      &lt; 0.0000000000000002 ***\nlog(total_livable_area)          &lt; 0.0000000000000002 ***\nnumber_of_bathrooms              &lt; 0.0000000000000002 ***\nhouse_age_c                                   0.37105    \nhouse_age_c2                                  0.07241 .  \ninterior_condition               &lt; 0.0000000000000002 ***\nquality_grade_num                &lt; 0.0000000000000002 ***\nfireplaces                        0.00000039295484010 ***\ngarage_spaces                    &lt; 0.0000000000000002 ***\ncentral_air_dummy                &lt; 0.0000000000000002 ***\ncentral_air_missing              &lt; 0.0000000000000002 ***\nincome_scaled                    &lt; 0.0000000000000002 ***\nba_rate                          &lt; 0.0000000000000002 ***\nunemployment_rate                 0.00008784000945812 ***\ntransit_count                                 0.26371    \navg_past_price_density           &lt; 0.0000000000000002 ***\nsqrt(crime_count)                &lt; 0.0000000000000002 ***\nlog(nearest_hospital_knn3)                    0.56724    \nfactor(zip_code)19103             0.00000067928689067 ***\nfactor(zip_code)19104                         0.33306    \nfactor(zip_code)19106                         0.06446 .  \nfactor(zip_code)19107                         0.94434    \nfactor(zip_code)19111                         0.03126 *  \nfactor(zip_code)19114                         0.30472    \nfactor(zip_code)19115                         0.41960    \nfactor(zip_code)19116                         0.11238    \nfactor(zip_code)19118                         0.40680    \nfactor(zip_code)19119                         0.90236    \nfactor(zip_code)19120                         0.96280    \nfactor(zip_code)19121                         0.06507 .  \nfactor(zip_code)19122                         0.29156    \nfactor(zip_code)19123                         0.00306 ** \nfactor(zip_code)19124                         0.31571    \nfactor(zip_code)19125                         0.17911    \nfactor(zip_code)19126                         0.05501 .  \nfactor(zip_code)19127                         0.24042    \nfactor(zip_code)19128                         0.12316    \nfactor(zip_code)19129                         0.06887 .  \nfactor(zip_code)19130                         0.90754    \nfactor(zip_code)19131                         0.00472 ** \nfactor(zip_code)19132            &lt; 0.0000000000000002 ***\nfactor(zip_code)19133             0.00000000000000339 ***\nfactor(zip_code)19134             0.00007435204084637 ***\nfactor(zip_code)19135                         0.14464    \nfactor(zip_code)19136                         0.04032 *  \nfactor(zip_code)19137                         0.93762    \nfactor(zip_code)19138                         0.17951    \nfactor(zip_code)19139                         0.00427 ** \nfactor(zip_code)19140             0.00000000705409283 ***\nfactor(zip_code)19141                         0.02256 *  \nfactor(zip_code)19142                         0.00909 ** \nfactor(zip_code)19143                         0.00454 ** \nfactor(zip_code)19144                         0.00731 ** \nfactor(zip_code)19145                         0.99179    \nfactor(zip_code)19146                         0.08876 .  \nfactor(zip_code)19147                         0.56429    \nfactor(zip_code)19148                         0.38180    \nfactor(zip_code)19149                         0.00015 ***\nfactor(zip_code)19150                         0.35610    \nfactor(zip_code)19151                         0.05225 .  \nfactor(zip_code)19152                         0.03953 *  \nfactor(zip_code)19153                         0.52268    \nfactor(zip_code)19154                         0.19922    \ninterior_condition:income_scaled &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3423 on 31435 degrees of freedom\n  (114 observations deleted due to missingness)\nMultiple R-squared:  0.7665,    Adjusted R-squared:  0.7661 \nF-statistic:  1638 on 63 and 31435 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\nvif(model_4)\n\n\n                                       GVIF Df GVIF^(1/(2*Df))\nlog(total_livable_area)            1.523908  1        1.234467\nnumber_of_bathrooms                1.611921  1        1.269614\nhouse_age_c                        1.580710  1        1.257263\nhouse_age_c2                       1.470007  1        1.212439\ninterior_condition                 1.533402  1        1.238306\nquality_grade_num                  1.779716  1        1.334060\nfireplaces                         1.295084  1        1.138018\ngarage_spaces                      1.586272  1        1.259473\ncentral_air_dummy                  2.084180  1        1.443669\ncentral_air_missing                1.453277  1        1.205519\nincome_scaled                     24.491928  1        4.948932\nba_rate                            6.178507  1        2.485660\nunemployment_rate                  2.016927  1        1.420186\ntransit_count                      1.715169  1        1.309645\navg_past_price_density             7.837830  1        2.799612\nsqrt(crime_count)                  2.815547  1        1.677959\nlog(nearest_hospital_knn3)         8.102261  1        2.846447\nfactor(zip_code)                 565.126609 45        1.072950\ninterior_condition:income_scaled  20.090358  1        4.482227\n\n\nCoefficient Interpretation:\n- Fixed Effects Interpretation: - These coefficients represent the price difference for each zip code relative to the ‚Äúreference zip code‚Äù (which is omitted from the list, e.g., 19102). - Example: factor(zip_code)19106 (-0.107): A home in zip code 19106 is, on average, 10.7% less expensive than a home in the reference zip code, holding all other variables constant. - Example: factor(zip_code)19149 (0.183): A home in zip code 19149 is, on average, 18.3% more expensive. - interior_condition:income_scaled (0.117) (Interaction Term): - This is one of the most interesting findings. It shows that the impact of interior_condition depends on income_scaled. - The total marginal effect of interior_condition is:\\[= -0.1695 + 0.1165 \\times \\text{income\\_scaled}\\] - At the baseline income level (income_scaled = 0), each one-unit worsening in condition is associated with a 17.0% price decrease (-0.1695). However, this penalty is mitigated (lessened) in higher-income areas. For each one-unit increase in income_scaled, the negative penalty of poor condition is reduced by 11.7 percentage points. This may imply that in high-income neighborhoods, ‚Äúfixer-uppers‚Äù (homes in poor condition) are seen as investment opportunities with high renovation potential. Therefore, the market penalty for ‚Äúpoor condition‚Äù is smaller."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-5-model-validation",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-5-model-validation",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 5: Model Validation",
    "text": "Phase 5: Model Validation\n\n10-fold cross-validation\n5.1 Compare all 4 models:"
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-6-model-diagnostics",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-6-model-diagnostics",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 6: Model Diagnostics",
    "text": "Phase 6: Model Diagnostics\n\nCheck assumptions for best model:\n6.1 Residual plot:\n\n\nCode\nmodel_data &lt;- data.frame(\n  Fitted = fitted(model_4),\n  Residuals = resid(model_4)\n)\n\np_resid_fitted &lt;- ggplot(model_data, aes(x = Fitted, y = Residuals)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\", size = 2) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"loess\", color = \"black\", se = FALSE, linewidth = 0.8) +\n  labs(\n    title = \"Residuals vs Fitted Values\",\n    subtitle = \"Checking linearity and homoscedasticity for Model 4\",\n    x = \"Fitted Values (Log(Sale Price))\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_resid_fitted\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\nresid_full &lt;- rep(NA, nrow(opa_census_all))\nresid_full[-as.numeric(model_4$na.action)] &lt;- resid(model_4)\n\nopa_census_all$residuals &lt;- resid_full\n\ntract_resid &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_residual = mean(residuals, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_resid, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_residual), color = \"white\", size = 0.2) +\n  scale_fill_gradient2(\n    low = \"#6A1B9A\", mid = \"white\", high = \"#FFB300\",\n    midpoint = 0,\n    limits = c(-0.5, 0.5),\n    name = \"Mean Log Residual\",\n    breaks = c(-0.3, 0, 0.3),\n    labels = c(\"Overestimated\", \"Accurate\", \"Underestimated\"),\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Hardest to Predict Neighborhoods in Philadelphia\",\n    subtitle = \"Yellow = underestimation | Purple = overestimation\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\nInterpretation:\n- Central Tendency of Residuals: The residuals are generally centered around the zero line, showing no systematic deviation. This indicates that the model captures the overall linear relationship between predictors and the response variable effectively.\n- Homoscedasticity: The residuals show greater variability and dispersion in the lower fitted value range (approximately 10‚Äì12), while they appear more stable at higher fitted values. This suggests that the model performs less effectively for observations with lower predicted values.\n- Model Assumption Assessment: Overall, the assumptions of linearity and homoscedasticity are largely satisfied, indicating a sound model fit, with only slight deviations to monitor at higher fitted values.\n6.2 Q-Q plot:\n\n\nCode\np_qq &lt;- ggplot(model_data, aes(sample = Residuals)) +\n  stat_qq(color = \"#6A1B9A\", size = 2, alpha = 0.6) +\n  stat_qq_line(color = \"red\",linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Normal Q-Q Plot\",\n    subtitle = \"Checking normality of residuals for Model 4\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_qq\n\n\n\n\n\n\n\n\n\nInterpretation:\n- Overall Shape of the Plot: The residual points generally follow the diagonal line, indicating that the overall distribution of residuals is roughly consistent with a normal distribution.\n- Good Fit in the Central Range: In the central range (around -1 to 1 quantiles), the sample quantiles align closely with the theoretical quantiles, suggesting that most residuals conform well to the assumption of normality.\n- Deviation in the Tails: At both tails‚Äîespecially the upper quantiles‚Äîthe points deviate noticeably from the red dashed line, indicating that the residuals have heavier tails than a normal distribution, suggesting slight non-normality.\n- Assessment of Normality Assumption: Although deviations appear in the tails, the overall alignment with the reference line is strong, indicating that the normality assumption largely holds, with only minor deviations for extreme residuals.\n6.3 Cook‚Äôs distance:\n\n\nCode\ncooks_d &lt;- cooks.distance(model_4)\nmodel_data &lt;- data.frame(\n  Index = 1:length(cooks_d),\n  CooksD = cooks_d\n)\nthreshold &lt;- 4 / nrow(model_4$model)\n\np_cook &lt;- ggplot(model_data, aes(x = Index, y = CooksD)) +\n  geom_segment(aes(xend = Index, yend = 0), color = \"#6A1B9A\", alpha = 0.7) +  # vertical lines\n  geom_point(color = \"#6A1B9A\", size = 0.15) +\n  geom_hline(yintercept = threshold, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Cook's Distance\",\n    subtitle = \"Identifying influential observations for Model 4\",\n    x = \"Observation Index\",\n    y = \"Cook's Distance\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_cook\n\n\n\n\n\n\n\n\n\nInterpretation:\n- Overall Distribution Pattern: Most observations have Cook‚Äôs Distance values close to zero, indicating that the dataset‚Äôs overall influence on the model is balanced, with no widespread undue impact.\n- Presence of Influential Points: A few vertical spikes rise noticeably above the rest, indicating the presence of some influential observations that may affect the estimated model coefficients. - Model Robustness Conclusion: Overall, the model appears robust, with no single observation exerting excessive influence."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-7-conclusions-recommendations",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-7-conclusions-recommendations",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 7: Conclusions & Recommendations",
    "text": "Phase 7: Conclusions & Recommendations\n\nConclusion:\n\nOur final model‚Äôs R¬≤ is 0.84, indicating that the model explains 84% of the variance in log sale prices. The RMSE is , showing that the model‚Äôs predictions are reasonably close to the observed values.\n\nLivable Area of the House matters most for Philadelphia prices.\n\n\n\nRecommendations:\n\nEquity concerns\n\nWhich neighborhoods are hardest to predict?\n\nThe largest prediction errors occur primarily in central Philadelphia, where both overestimation and underestimation coexist within close proximity. This pattern indicates that the model struggles most in areas with high housing heterogeneity- neighborhoods that contain a mix of old row houses, newly renovated apartments, and varying property types within short distances.\nIn contrast, outer neighborhoods such as those in the northwest and northeast tend to have more consistent housing characteristics, leading to smaller residuals. The central tracts‚Äô larger residuals suggest that cultural or historical features have introduced variability that the model‚Äôs current features (mainly physical characteristics) cannot fully explain.\n\nAny data bias?\n\nThe observed spatial pattern of prediction errors reflects inherent data bias. The dataset likely overrepresents mid-range housing conditions and underrepresents both luxury and low-income housing. As shown in the livable area and interior condition maps, data coverage in wealthier areas (especially the northwest) is limited, and these tracts often contain more unique, high-value properties that the model cannot generalize well.\n\nWealthier tracts are more likely to be well-documented, while poorer areas lack records, creating spatial imbalance in model performance.\n\nHigh-priced homes typically have larger negotiation margins, meaning their final sale prices are often lower than the listed prices. In contrast, low-priced homes sell closer to their listing prices. As a result, model tends to overestimate expensive homes and underestimate affordable ones, introducing systematic bias.\n\n\nRecommendations to government\n\n\nImmediate System Calibration: We recommend utilizing our model‚Äôs findings to immediately adjust property assessments in systematically overvalued low-income communities. This action will ensure a fair distribution of the tax burden and address current inequities.\nIntegrate Advanced Spatial Features: We advise the Office of Property Assessment (OPA) to permanently integrate the effective spatial characteristics identified by our study‚Äîspecifically Comparable Sales Proxies (surrounding transaction prices) and Neighborhood Fixed Effects‚Äîinto the next-generation AVM. This will significantly enhance the model‚Äôs responsiveness to rapidly changing market dynamics.\nExtreme high values almost exclusively stem from corporate transactions and require manual review for outliers.\n\n\nLimitations and nest steps\n\nLimitations: Inherent Data Biases\n\nOur predictive accuracy is constrained by inherent data biases which affect equity. We find a dual challenge in data coverage: in affluent areas, high-value, unique properties suffer from data sparsity, making generalization difficult. Conversely, lower-income areas often show data incompleteness, leading to less reliable predictions and higher residual errors. Critically, we observed a price-tier bias: high-priced homes tend to sell below their list price, while low-priced homes transact closer to it. This systemic pattern means the model is prone to over-assessing expensive properties and under-assessing affordable ones, creating a structural risk for vertical inequity in the tax system.\n\nNext Steps: Enhancing Data Quality and Fairness\n\nTo address these limitations, our next steps focus on data enrichment and equitable optimization. The City should partner with us to integrate non-public data, such as detailed appraisal records and permit data, which can account for unobserved renovation quality and close the data gap. Furthermore, to combat the systematic price-tier bias, we recommend integrating fairness metrics directly into the AVM‚Äôs optimization process. This will shift the model‚Äôs objective beyond simple average accuracy (RMSE) to ensure that prediction errors are uniformly low across all price tiers and all Philadelphia neighborhoods, securing both accuracy and equity."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html",
    "href": "Assignments/assignment_5/Assignment_5_coop.html",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "Philadelphia‚Äôs Indego bike share system faces the same operational challenge as every bike share system: rebalancing bikes to meet anticipated demand.\nIn this lab. we build predictive models that forecast bike share demand across space (different stations) and time (different hours) to help solve this operational problem."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#the-rebalancing-challenge-in-philadelphia",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#the-rebalancing-challenge-in-philadelphia",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "Philadelphia‚Äôs Indego bike share system faces the same operational challenge as every bike share system: rebalancing bikes to meet anticipated demand.\nIn this lab. we build predictive models that forecast bike share demand across space (different stations) and time (different hours) to help solve this operational problem."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#load-libraries",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#load-libraries",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n\nCode\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#define-themes",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#define-themes",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Define Themes",
    "text": "Define Themes\n\n\nCode\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 &lt;- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#data-import-preparation",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#data-import-preparation",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Data Import & Preparation",
    "text": "Data Import & Preparation\nIn this lab, we will use the Indego data (Q3 2024), because we want to explore the pattern‚Äôs diffenrence between summer and winter.\n\nDownload data for Q3 2024\n\n\nCode\n# Read Q3 2024 data\nindego_q3 &lt;- read_csv(\"data/indego-trips-2024-q3.csv\")\n\n\n\n\nExamine the Data Structure\n\n\nCode\n# How many trips?\ncat(\"Total trips in Q3 2024:\", nrow(indego_q3), \"\\n\")\n\n\nTotal trips in Q3 2024: 408408 \n\n\nCode\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego_q3$start_time)), \"to\", \n    max(mdy_hm(indego_q3$start_time)), \"\\n\")\n\n\nDate range: 1719792120 to 1727740740 \n\n\nCode\nas.POSIXct(1719792120, origin = \"1970-01-01\", tz = \"America/New_York\")\n\n\n[1] \"2024-06-30 20:02:00 EDT\"\n\n\nCode\nas.POSIXct(1727740740, origin = \"1970-01-01\", tz = \"America/New_York\")\n\n\n[1] \"2024-09-30 19:59:00 EDT\"\n\n\nCode\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego_q3$start_station)), \"\\n\")\n\n\nUnique start stations: 261 \n\n\nCode\n# Trip types\ntable(indego_q3$trip_route_category)\n\n\n\n   One Way Round Trip \n    380939      27469 \n\n\nCode\n# Passholder types\ntable(indego_q3$passholder_type)\n\n\n\n Day Pass  Indego30 Indego365   Walk-up \n    22885    239857    132358     13308 \n\n\nCode\n# Bike types\ntable(indego_q3$bike_type)\n\n\n\nelectric standard \n  236839   171569 \n\n\n\n\nCreate Time Bins\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\nCode\nindego_q3 &lt;- indego_q3 %&gt;%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego_q3 %&gt;% select(start_datetime, interval60, week, dotw, hour, weekend))\n\n\n# A tibble: 6 √ó 6\n  start_datetime      interval60           week dotw   hour weekend\n  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;ord&gt; &lt;int&gt;   &lt;dbl&gt;\n1 2024-07-01 00:02:00 2024-07-01 00:00:00    27 Mon       0       0\n2 2024-07-01 00:03:00 2024-07-01 00:00:00    27 Mon       0       0\n3 2024-07-01 00:04:00 2024-07-01 00:00:00    27 Mon       0       0\n4 2024-07-01 00:05:00 2024-07-01 00:00:00    27 Mon       0       0\n5 2024-07-01 00:06:00 2024-07-01 00:00:00    27 Mon       0       0\n6 2024-07-01 00:06:00 2024-07-01 00:00:00    27 Mon       0       0"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#exploratory-analysis",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#exploratory-analysis",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nWe aggregated individual trip records by date to obtain total daily ridership, then used ggplot2 to plot a time-series line chart with a smoothed trend line to visualize how Indego bike-share demand changed over the third quarter of 2024.\n\nTrips Over Time\n\n\nCode\n# Daily trip counts\ndaily_trips &lt;- indego_q3 %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q3 2024\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nInterpretation:\nThe first pattern is the weekly heartbeat. Those sharp peaks and valleys repeat with metronomic regularity. Indego always dips on the same days each week, typically weekends, because commuting demand relaxes\nThen there‚Äôs the gentle seasonal slope underneath that weekly pulse. Early July starts relatively high, and then the baseline slowly drifts downward into early August (Maybe because of the summer vocation). Mid-August shows a little recovery, followed by a September lift since the fall semester began. Before the season‚Äôs first chilly whisper pushes ridership down again heading into October.\n\n\nHourly Patterns\n\n\nCode\n# Average trips by hour and day type\nhourly_patterns &lt;- indego_q3 %&gt;%\n  group_by(hour, weekend) %&gt;%\n  summarize(avg_trips = n() / n_distinct(date)) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nInterpretation:\nOn weekdays, the peaks are tied to commuting: A morning spike around 8‚Äì9 AM, when the city jolts awake and people pedal to work. A bigger afternoon/evening spike around 5‚Äì6 PM, the classic homebound surge. The slope up to these points is steep, like a heartbeat aligned with office life. On weekends, the rhythm relaxes. There‚Äôs no sharp, early-morning people are not rushing anywhere. Ridership climbs more slowly through the morning toward a broad mid-day plateau, roughly 11 AM to 3 PM, and then drifts downward without the evening spike seen on weekdays.\nSo the pattern suggests that weekday trips are purposeful and synchronized with the workday, while weekend trips stretch out across the middle of the day in a more leisurely wave.\n\n\nTop Stations\n\n\nCode\n# Most popular origin stations\ntop_stations &lt;- indego_q3 %&gt;%\n  count(start_station, start_lat, start_lon, name = \"trips\") %&gt;%\n  arrange(desc(trips)) %&gt;%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 20 Indego Stations by Trip Origins\n\n\nstart_station\nstart_lat\nstart_lon\ntrips\n\n\n\n\n3,010\n39.94711\n-75.16618\n6,654\n\n\n3,032\n39.94527\n-75.17971\n5,436\n\n\n3,244\n39.93865\n-75.16674\n4,421\n\n\n3,054\n39.96250\n-75.17420\n4,305\n\n\n3,359\n39.94888\n-75.16978\n4,305\n\n\n3,296\n39.95134\n-75.16758\n4,252\n\n\n3,101\n39.94295\n-75.15955\n4,226\n\n\n3,020\n39.94855\n-75.19007\n4,154\n\n\n3,295\n39.95028\n-75.16027\n4,144\n\n\n3,066\n39.94561\n-75.17348\n4,114\n\n\n3,059\n39.96244\n-75.16121\n4,022\n\n\n3,022\n39.95472\n-75.18323\n3,954\n\n\n3,362\n39.94816\n-75.16226\n3,948\n\n\n3,163\n39.94974\n-75.18097\n3,942\n\n\n3,028\n39.94061\n-75.14958\n3,921\n\n\n3,208\n39.95048\n-75.19324\n3,919\n\n\n3,061\n39.95425\n-75.17761\n3,796\n\n\n3,046\n39.95012\n-75.14472\n3,734\n\n\n3,063\n39.94633\n-75.16980\n3,726\n\n\n3,185\n39.95169\n-75.15888\n3,678"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#get-philadelphia-spatial-context",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#get-philadelphia-spatial-context",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Get Philadelphia Spatial Context",
    "text": "Get Philadelphia Spatial Context\n\nLoad Philadelphia Census Data\nWe‚Äôll get census tract data to add demographic context to our stations.\n\n\nCode\n# Get Philadelphia census tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\",\n  progress_bar = FALSE\n) %&gt;%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %&gt;%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %&gt;%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n\n# Check the data\nglimpse(philly_census)\n\n\nRows: 408\nColumns: 17\n$ GEOID                  &lt;chr&gt; \"42101001500\", \"42101001800\", \"42101002802\", \"4‚Ä¶\n$ NAME                   &lt;chr&gt; \"Census Tract 15; Philadelphia County; Pennsylv‚Ä¶\n$ Total_Pop              &lt;dbl&gt; 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,‚Ä¶\n$ B01003_001M            &lt;dbl&gt; 677, 369, 796, 437, 853, 210, 480, 734, 763, 11‚Ä¶\n$ Med_Inc                &lt;dbl&gt; 110859, 114063, 78871, 61583, 32347, 48581, 597‚Ä¶\n$ B19013_001M            &lt;dbl&gt; 24975, 30714, 20396, 22293, 4840, 13812, 6278, ‚Ä¶\n$ Total_Commuters        &lt;dbl&gt; 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2‚Ä¶\n$ B08301_001M            &lt;dbl&gt; 387, 308, 478, 383, 456, 189, 380, 281, 456, 68‚Ä¶\n$ Transit_Commuters      &lt;dbl&gt; 429, 123, 685, 506, 534, 192, 658, 218, 438, 51‚Ä¶\n$ B08301_010M            &lt;dbl&gt; 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,‚Ä¶\n$ White_Pop              &lt;dbl&gt; 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35‚Ä¶\n$ B02001_002M            &lt;dbl&gt; 268, 381, 592, 380, 88, 190, 463, 112, 238, 778‚Ä¶\n$ Med_Home_Value         &lt;dbl&gt; 568300, 605000, 350600, 296400, 76600, 289700, ‚Ä¶\n$ B25077_001M            &lt;dbl&gt; 58894, 34876, 12572, 22333, 10843, 118720, 1506‚Ä¶\n$ geometry               &lt;MULTIPOLYGON [¬∞]&gt; MULTIPOLYGON (((-75.16558 3..., MU‚Ä¶\n$ Percent_Taking_Transit &lt;dbl&gt; 20.694645, 5.454545, 22.592348, 21.754084, 26.9‚Ä¶\n$ Percent_White          &lt;dbl&gt; 67.2100892, 75.5757576, 64.5279720, 79.9950360,‚Ä¶\n\n\n\n\nMap Philadelphia Context\nOur code builds a contextual map by shading each Philadelphia census tract according to its median household income. Then it overlays red points representing Indego trip start locations. The figure shows that most bike-share activity clusters in central and lower-income neighborhoods, while higher-income outer areas have fewer stations and fewer trip origins.\n\n\nCode\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego_q3,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n\n\n\n\n\n\n\n\n\n\n\nJoin Census Data to Stations\nWe‚Äôll spatially join census characteristics to each bike station.\n\n\nCode\n# Create sf object for stations\nstations_sf &lt;- indego_q3 %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census &lt;- st_join(stations_sf, philly_census, left = TRUE) %&gt;%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map &lt;- indego_q3 %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_q3_census &lt;- indego_q3 %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map &lt;- indego_q3 %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %&gt;% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %&gt;% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n\n\n\n\n\n\n\n\n\n\n\nDealing with missing data\nIn this step, we are going to remove the non-residential bike share stations.\n\n\nCode\n# Identify which stations to keep\nvalid_stations &lt;- stations_census %&gt;%\n  filter(!is.na(Med_Inc)) %&gt;%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_q3_census &lt;- indego_q3 %&gt;%\n  filter(start_station %in% valid_stations) %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#get-weather-data",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#get-weather-data",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Get Weather Data",
    "text": "Get Weather Data\nWeather significantly affects bike share demand. We get hourly weather for Philadelphia in this part.\n\n\nCode\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q3 2024: July 1 - September 31\nweather_data &lt;- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-07-01\",\n  date_end = \"2024-09-30\"\n)\n\n# Process weather data\nweather_processed &lt;- weather_data %&gt;%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %&gt;%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %&gt;%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete &lt;- weather_processed %&gt;%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %&gt;%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %&gt;% select(Temperature, Precipitation, Wind_Speed))\n\n\n  Temperature    Precipitation        Wind_Speed    \n Min.   :55.00   Min.   :0.000000   Min.   : 0.000  \n 1st Qu.:70.00   1st Qu.:0.000000   1st Qu.: 4.000  \n Median :76.00   Median :0.000000   Median : 7.000  \n Mean   :75.59   Mean   :0.007896   Mean   : 6.893  \n 3rd Qu.:81.00   3rd Qu.:0.000000   3rd Qu.: 9.000  \n Max.   :98.00   Max.   :1.250000   Max.   :44.000  \n\n\n\nVisualize Weather Patterns\n\n\nCode\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q3 2024\",\n    subtitle = \"Summer to early autumn transition\",\n    x = \"Date\",\n    y = \"Temperature (¬∞F)\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nInterpretation:\nThe pattern is a classic seasonal transition: hot, volatile summer days gradually giving way to cooler, steadier early-fall conditions."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#create-space-time-panel",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#create-space-time-panel",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Create Space-Time Panel",
    "text": "Create Space-Time Panel\n\nAggregate Trips to Station-Hour Level\n\n\nCode\n# Count trips by station-hour\ntrips_panel &lt;- indego_q3_census %&gt;%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %&gt;%\n  summarize(Trip_Count = n()) %&gt;%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n\n\n[1] 193072\n\n\nCode\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n\n\n[1] 241\n\n\nCode\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n\n\n[1] 2202\n\n\n\n\nCreate Complete Panel Structure\nNot every station has trips every hour. We need a complete panel where every station-hour combination exists (even if Trip_Count = 0).\n\n\nCode\n# Calculate expected panel size\nn_stations &lt;- length(unique(trips_panel$start_station))\nn_hours &lt;- length(unique(trips_panel$interval60))\nexpected_rows &lt;- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n\n\nExpected panel rows: 530,682 \n\n\nCode\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nCurrent rows: 193,072 \n\n\nCode\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nMissing rows: 337,610 \n\n\nCode\n# Create complete panel\nstudy_panel &lt;- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %&gt;%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %&gt;%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes &lt;- trips_panel %&gt;%\n  group_by(start_station) %&gt;%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n\n\nComplete panel rows: 530,682 \n\n\n\n\nAdd Time Features\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Create day of week factor with treatment (dummy) coding\nstudy_panel &lt;- study_panel %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(study_panel$dotw_simple) &lt;- contr.treatment(7)\n\n\n\n\nJoin Weather Data\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %&gt;% select(Trip_Count, Temperature, Precipitation))\n\n\n   Trip_Count       Temperature    Precipitation   \n Min.   : 0.0000   Min.   :55.00   Min.   :0.0000  \n 1st Qu.: 0.0000   1st Qu.:70.00   1st Qu.:0.0000  \n Median : 0.0000   Median :76.00   Median :0.0000  \n Mean   : 0.7068   Mean   :75.59   Mean   :0.0079  \n 3rd Qu.: 1.0000   3rd Qu.:81.00   3rd Qu.:0.0000  \n Max.   :24.0000   Max.   :98.00   Max.   :1.2500  \n                   NA's   :5784    NA's   :5784"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#create-temporal-lag-variables",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#create-temporal-lag-variables",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Create Temporal Lag Variables",
    "text": "Create Temporal Lag Variables\nThe key innovation for space-time prediction: past demand predicts future demand.\nWhy Lags?\nIf there were 15 bike trips from Station A at 8:00 AM, there will probably be ~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.\n\n\nCode\n# Sort by station and time\nstudy_panel &lt;- study_panel %&gt;%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel &lt;- study_panel %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24),\n    lag1week = lag(Trip_Count, 24*7)\n  ) %&gt;%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete &lt;- study_panel %&gt;%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n\n\nRows after removing NA lags: 646,362 \n\n\n\nVisualize Lag Correlations\n\n\nCode\n# Sample one station to visualize\nexample_station &lt;- study_panel_complete %&gt;%\n  filter(start_station == first(start_station)) %&gt;%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#temporal-traintest-split",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#temporal-traintest-split",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Temporal Train/Test Split",
    "text": "Temporal Train/Test Split\nThe code performs a clean temporal split for forecasting. It first separates the dataset into early weeks and later weeks, identifies stations that appear in both periods, and keeps only those consistent stations. Then it creates a training set using weeks 27‚Äì36 and a testing set using weeks 37‚Äì40, ensuring the model is trained on past data and evaluated on future data.\n\n\nCode\n# Split by week\n# Q3 has weeks 27-40 (Jul-Sep)\n# Train on weeks 27-36 (Jan 1 - early September)\n# Test on weeks 37-40 (rest of September)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 37) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\nlate_stations &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 37) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations &lt;- intersect(early_stations, late_stations)\n\n# Filter panel to only common stations\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 37)\n\ntest &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 37)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n\n\nTraining observations: 470,940 \n\n\nCode\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n\n\nTesting observations: 159,330 \n\n\nCode\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n\n\nTraining date range: 19905 to 19974 \n\n\nCode\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n\n\nTesting date range: 19975 to 19996"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#build-predictive-models",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#build-predictive-models",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Build Predictive Models",
    "text": "Build Predictive Models\nWe‚Äôll build 5 models with increasing complexity to see what improves predictions.\nThis is a model evaluation function that can generate data\n\n\nCode\n# model evaluation function\nmodel_metrics_lm &lt;- function(model, test_data, response_var = \"Trip_Count\") {\n  model_name &lt;- deparse(substitute(model))\n  \n  if (inherits(model, \"glm\") && family(model)$family == \"poisson\") {\n    type       &lt;- \"Poisson\"\n    y_train    &lt;- model$y\n    yhat_train &lt;- predict(model, type = \"response\")\n    \n    sse &lt;- sum((y_train - yhat_train)^2)\n    sst &lt;- sum((y_train - mean(y_train))^2)\n    r2  &lt;- 1 - sse / sst\n    \n    n &lt;- length(y_train)\n    p &lt;- length(coef(model))\n    adjr2 &lt;- 1 - (1 - r2) * (n - 1) / (n - p - 1)\n    \n  } else if (inherits(model, \"lm\")) {\n    type       &lt;- \"OLS\"\n    s          &lt;- summary(model)\n    r2         &lt;- s$r.squared\n    adjr2      &lt;- s$adj.r.squared\n    y_train    &lt;- model$model[[response_var]]\n    yhat_train &lt;- fitted(model)\n    \n  } else {\n    stop(\"Only supports lm and glm(poisson).\")\n  }\n  \n  mae_train &lt;- mean(abs(y_train - yhat_train))\n  \n  if (inherits(model, \"glm\") && family(model)$family == \"poisson\") {\n    yhat_test &lt;- predict(model, newdata = test_data, type = \"response\")\n  } else {\n    yhat_test &lt;- predict(model, newdata = test_data)\n  }\n  y_test &lt;- test_data[[response_var]]\n  cc &lt;- complete.cases(y_test, yhat_test)\n  mae_test &lt;- mean(abs(y_test[cc] - yhat_test[cc]))\n  \n  cat(\"Model:\", model_name, \"\\n\")\n  cat(\"  Type:              \", type, \"\\n\")\n  cat(\"  R-squared (train): \", round(r2, 4), \"\\n\")\n  cat(\"  Adj R-squared:     \", round(adjr2, 4), \"\\n\")\n  cat(\"  MAE (train):       \", round(mae_train, 4), \"\\n\")\n  cat(\"  MAE (test):        \", round(mae_test, 4), \"\\n\\n\")\n}\n\n\n\nModel 1: Baseline (Time + Weather)\nThis model captures daily and weekly cycles, weather effects.\n\n\nCode\n# Now run the model\nmodel1 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model1, test_data = test)\n\n\nModel: model1 \n  Type:               OLS \n  R-squared (train):  0.1085 \n  Adj R-squared:      0.1085 \n  MAE (train):        0.7937 \n  MAE (test):         0.8293 \n\n\nInterpretation:\nThe model shows that trip counts vary strongly by hour of day. Compared to the baseline hour (midnight), coefficients rise sharply in the morning and peak around the late afternoon, matching the commuter pattern observed earlier. Day-of-week effects are smaller but still significant: mid-week days tend to have slightly higher ridership, while weekends show lower counts. Weather matters too‚Äîhigher temperatures slightly reduce trips, and precipitation has a larger negative effect. Although all predictors are statistically significant (given the huge sample size), the model explains only about 11% of the variation, meaning most hour-to-hour fluctuations are driven by other unmeasured factors such as station-specific conditions or random demand swings.\n\n\nModel 2: Add Temporal Lags\n\n\nCode\nmodel2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model2, test_data = test)\n\n\nModel: model2 \n  Type:               OLS \n  R-squared (train):  0.3318 \n  Adj R-squared:      0.3317 \n  MAE (train):        0.6655 \n  MAE (test):         0.6857 \n\n\nInterpretation:\nThe model still captures the usual hourly and day-of-week ridership patterns, but the major change comes from adding the lagged demand terms. The one-hour lag has the strongest effect‚Äîstations that were busy an hour ago are much more likely to be busy now. The three-hour lag and the previous-day same-hour lag also contribute meaningful predictive power. Weather remains significant but plays a smaller role. Overall model fit improves substantially: R¬≤ rises from about 0.11 to about 0.33, and residual error drops, showing that including recent past demand explains much more of the variation. This confirms that short-term station activity is strongly tied to its own immediate history.\n\n\nModel 3: Add Demographics\nThis model captures neighborhood effects on demand\n\n\nCode\nmodel3 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model3, test_data = test)\n\n\nModel: model3 \n  Type:               OLS \n  R-squared (train):  0.3373 \n  Adj R-squared:      0.3373 \n  MAE (train):        0.6673 \n  MAE (test):         0.6868 \n\n\nInterpretation:\nThe R square means the model explains about 34% of hourly variation in bike demand.Adding demographics raises R¬≤ slightly compared to Model 2, they help, but only modestly.Temporal lags dominate the model, while demographics show systematic spatial patterns. All three demographic variables are all significant, and it shows that demographics matter, but compared to temporal lags and hour-of-day, their effect sizes are small.\n\n\nModel 4: Add Station Fixed Effects\n\n\nCode\nmodel4 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model4, test_data = test)\n\n\nModel: model4 \n  Type:               OLS \n  R-squared (train):  0.3627 \n  Adj R-squared:      0.3624 \n  MAE (train):        0.6667 \n  MAE (test):         0.6831 \n\n\nInterpretation:\nModel 4 adds a dummy variable for every station, letting the model control for unobserved station-specific factors. It shows that station-specific factors are strong drivers of bike-share demand. Adding fixed effects significantly boosts the model‚Äôs ability to explain past variation, but the improvement does not reliably boost predictive accuracy. Even though R¬≤ increases, the model‚Äôs MAE does not improve much. Because station fixed effects are great for explaining historical data, but they don‚Äôt generalize as well when predicting new periods.\n\n\nModel 5: Add Rush Hour Interaction\n\n\nCode\nmodel5 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model5, test_data = test)\n\n\nModel: model5 \n  Type:               OLS \n  R-squared (train):  0.3669 \n  Adj R-squared:      0.3665 \n  MAE (train):        0.6653 \n  MAE (test):         0.6833 \n\n\nInterpretation:\nModel 5 introduces an interaction between rush hour and weekends, allowing the model to differentiate the strong weekday commuting peaks from the much softer weekend patterns. It also adds month effects to capture seasonal shifts across July, August, and September. Even though Model 5 has the highest R¬≤, it performs worse on the test set. This indicates that Model 5 is overfit: it explains historical variation better but loses forecasting accuracy. Model 4 remains the best balance of simplicity and predictive accuracy."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#model-evaluation",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#model-evaluation",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nCalculate Predictions and MAE\n\n\nCode\n# Get predictions on test set\ntest &lt;- test %&gt;%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results1 &lt;- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results1, \n      digits = 4,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nMAE (trips)\n\n\n\n\n1. Time + Weather\n0.8293\n\n\n2. + Temporal Lags\n0.6857\n\n\n3. + Demographics\n0.6868\n\n\n4. + Station FE\n0.6831\n\n\n5. + Rush Hour Interaction\n0.6833\n\n\n\n\n\nInterpretation:\n\nTemporal lags are the most important predictive feature.\n\nModel 2 gives the biggest drop in MAE, showing that short-term demand patterns drive accuracy.\n\nDemographics help explain variation but don‚Äôt improve prediction.\nStation fixed effects give a small but meaningful improvement.\n\nThey adjust for inherent differences between stations.\n\nExtra complexity (Model 5) does not yield better predictions.\n\nThe model is already near its predictive ceiling.\n\nBest predictive models are Model 2 (simplest) and Model 4 (slightly better).\n\n\n\nVisualize Model Comparison\n\n\nCode\nggplot(mae_results1, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\",alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 4)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#compare-results-to-q1-2025",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#compare-results-to-q1-2025",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Compare results to Q1 2025:",
    "text": "Compare results to Q1 2025:\n\n\n1. How do the MAE values compare across quarters?\nQ1 2025 MAE values are generally lower than those we reported for Q3 2024.\nSo predictions in Q1 are simply easier.\n\n\n2. Why might they differ?\nWinter(Q1) has smoother, more predictable demand: Riders may behave more consistently in colder months. There are fewer tourists. Weather is cold but stable‚Äîmore consistently low temperatures, fewer big swings.\nSummer(Q3) is noisy and harder to predict: Weather-driven volatility: heat waves, thunderstorms, humidity. More variation in leisure rides, weekend trips, and irregular patterns. Tourism and events add bursts of unpredictable activity.\nSo the model in Q3 faces more randomness, and MAE naturally rises.\n\n\n3. Are temporal patterns different (summer vs.¬†winter)?\nYes. In winter, ridership follows a tighter, more predictable rhythm. The daily totals rise gradually from January into March without the big, volatile spikes that appear in summer. Hourly patterns also become cleaner: weekday mornings and evenings show sharp commuter peaks, while weekends stay much flatter with only a modest mid-day bump. By contrast, summer brings stronger mid-day activity, higher weekend volume, and much more weather-driven noise. Winter demand is steadier and more commute-oriented, whereas summer mixes commuting with a large amount of leisure riding, creating far more variation.\n\n\n4. Which features are most important in your quarter?\nTemporal lags. The results show a large performance jump when lag variables (1 hour, 3 hours, 1 day) are added. Summer ridership changes quickly with events, weather shifts, and bursts of leisure activity."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#space-time-error-analysis",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#space-time-error-analysis",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Space-Time Error Analysis",
    "text": "Space-Time Error Analysis\nObserved vs.¬†Predicted\nLet‚Äôs use our best model (Model 4) for error analysis.\n\n\nCode\ntest &lt;- test %&gt;%\n  mutate(\n    error = Trip_Count - pred4,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"PM Rush\",\n      hour &gt; 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred4)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 4 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nInterpretation:\nWhere the model performs well: It does best in low-to-moderate demand conditions, especially overnight and during mid-day. Those periods have fewer big swings and smaller trip counts, so the model can track the pattern closely. On weekends‚Äîwhen demand is generally smoother‚Äîthe points cluster tightly and the green line stays close to the red line. The model doesn‚Äôt dramatically over- or under-shoot in these calmer periods.\nWhere the model struggles: It struggles during high-demand commuter windows, especially the AM and PM rush hours. In these panels, the cloud of points stretches upward far past the green line, showing that the model systematically under-predicts the busiest moments. Sharp surges in demand are hard to anticipate with simple temporal lags, so the model tends to flatten out the peaks. There is also more scatter on weekday rush hours than on weekends, reflecting the added variability of work-day travel.\nIn conclusion, the model handles routine, low-volume times well, but it misses the sudden spikes that define heavy commuting periods."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#spatial-error-patterns",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#spatial-error-patterns",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Spatial Error Patterns",
    "text": "Spatial Error Patterns\nIn this part, we are exploring whether prediction errors are clustered in certain parts of Philadelphia.\n\n\nCode\n# 1. Calculate MAE by stations\nstation_errors &lt;- test %&gt;%\n  filter(!is.na(pred4)) %&gt;%\n  group_by(start_station, start_lat.y, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.y), !is.na(start_lon.y))\n\n# 2. Map 1ÔºöPrediction Errors\np1 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.y, color = MAE),\n    size = 0.5, alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12, barheight = 1,\n    title.position = \"top\", title.hjust = 0.5\n  ))\n\n# 3. Map 2ÔºöAverage Demand\np2 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.y, color = avg_demand),\n    size = 0.5, alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12, barheight = 1,\n    title.position = \"top\", title.hjust = 0.5\n  ))\n\n# 4. Combine two maps\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\nInterpretation:\nErrors form a dense cluster in and around Center City, extending slightly into University City and parts of South Philly. The neighborhoods in center city have the highest errors.Because these stations often sit near job centers, transit hubs, parks, and retail corridors ‚Äî all places with unpredictable surges. In short, The model struggles where human behavior is least predictable and demand is most dynamic."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#temporal-error-patterns",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#temporal-error-patterns",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Temporal Error Patterns",
    "text": "Temporal Error Patterns\nWhen are we most wrong?\n\n\nCode\n# MAE by time of day and day type\ntemporal_errors &lt;- test %&gt;%\n  group_by(time_of_day, weekend) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nInterpretation:\nWhen are errors highest? Errors peak during the commute periods: PM Rush and AM Rush. These periods have the most unpredictable spikes in demand."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#errors-and-demographics",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#errors-and-demographics",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Errors and Demographics",
    "text": "Errors and Demographics\nAre prediction errors related to neighborhood characteristics?\n\n\nCode\n# Join demographic data to station errors\nstation_errors_demo &lt;- station_errors %&gt;%\n  left_join(\n    station_attributes %&gt;% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %&gt;%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 &lt;- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 &lt;- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 &lt;- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n\n\n\n\n\n\n\n\n\nInterpretation:\nPrediction errors slightly related to neighborhood characteristics?\n\nHigher-income neighborhoods show slightly higher errors. The trend line slopes upward: as median income increases, MAE tends to rise a bit. This likely reflects busier, more mixed-use areas where demand is less predictable.\nNeighborhoods with high transit use show lower errors. Stations located in areas with more transit riders tend to have lower MAE. Transit-heavy areas typically have more stable, commute-driven travel patterns that are easier for the model to predict.\nStations in whiter neighborhoods show slightly higher errors. The relationship is mild, but the upward slope suggests that stations in predominantly white, higher-income areas may see more inconsistent demand‚Äîoften driven by leisure or irregular trip purposes."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#feature-selection-and-engineering",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#feature-selection-and-engineering",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Feature Selection and Engineering",
    "text": "Feature Selection and Engineering\nFrom the error analysis above, we noticed that stations with large prediction errors are concentrated in Center City. From the data structure, we also know that the distribution of Trip_Count is highly skewed. There are many observations with 0 or 1 trip and a few observations with very large counts. The large number of 0 or 1 counts makes the model under-predict for high values. The extreme large counts make the model over-predict for low values. This creates a structural bias that we cannot fully remove.\nTo address this, we want to add variables that can detect and partially adjust for this pattern. Distance to Center City is a natural candidate. It can help the model sense the spatial structure behind these errors. However, the station dummy variables may already capture most of the distance differences. Therefore, the distance variable needs to be transformed or used in interaction terms to avoid collinearity with the station fixed effects.\nWhen we examine the trip-count-over-time plots, we also see spikes that appear irregularly. These spikes may reflect special events that disrupt the usual demand pattern. They do not happen often, but they can create extreme values and break the roughly linear relationship that the model tries to learn. This can have a strong impact on model performance. Federal holidays are one potential source of such events. The current model does not include enough variables to detect these effects.\nBased on these observations, we decide to add two new variables:\n1.Holiday indicator\n\n\nCode\nlibrary(lubridate)\n\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  mutate(\n    # Convert datetime column to Date\n    date = as.Date(interval60),\n\n    # Weekday with Monday = 1, Tuesday = 2, ..., Sunday = 7\n    wday_mon = wday(date, week_start = 1),\n    \n    # US Federal Holidays (rule-based, works for any year)\n    \n    # New Year's Day: January 1st\n    holiday_newyear = month(date) == 1  & mday(date) == 1,\n\n    # Martin Luther King Jr. Day: 3rd Monday in January\n    holiday_mlk = month(date) == 1 &\n                  wday_mon == 1 &        \n                  mday(date) &gt;= 15 & mday(date) &lt;= 21,\n\n    # Presidents‚Äô Day: 3rd Monday in February\n    holiday_pres = month(date) == 2 &\n                   wday_mon == 1 &       \n                   mday(date) &gt;= 15 & mday(date) &lt;= 21,\n\n    # Memorial Day: last Monday in May (Monday on or after the 25th)\n    holiday_memorial = month(date) == 5 &\n                       wday_mon == 1 &     \n                       mday(date) &gt;= 25,\n\n    # Independence Day: July 4th\n    holiday_july4 = month(date) == 7 & mday(date) == 4,\n\n    # Labor Day: 1st Monday in September (Monday between 1st and 7th)\n    holiday_labor = month(date) == 9 &\n                    wday_mon == 1 &        \n                    mday(date) &lt;= 7,\n\n    # Thanksgiving: 4th Thursday in November\n    holiday_thanks = month(date) == 11 &\n                     wday_mon == 4 &              # Thursday\n                     mday(date) &gt;= 22 & mday(date) &lt;= 28,\n\n    # Christmas Day: December 25th\n    holiday_xmas = month(date) == 12 & mday(date) == 25,\n\n    # Combined holiday indicator: 1 if any of the above holidays, 0 otherwise\n    holiday_any = as.integer(\n      holiday_newyear | holiday_mlk | holiday_pres |\n      holiday_memorial | holiday_july4 | holiday_labor |\n      holiday_thanks | holiday_xmas\n    )\n  )\n\n\n2.Distance to Center City\n\n\nCode\n# 1. City Hall point\nphilly_cbd_3857 &lt;- st_sfc(\n  st_point(c(-75.1636, 39.9526)),   # lon, lat\n  crs = 4326\n) %&gt;%\n  st_transform(3857)\n\n# project the dataframe to the same CRS 3857 (to calculate distance in meter)\nstations_dist &lt;- stations_sf %&gt;%\n  st_transform(3857) %&gt;%\n  mutate(\n    dist_center_m = as.numeric(st_distance(geometry, philly_cbd_3857))\n  ) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(start_station, dist_center_m) %&gt;%\n  distinct()\n\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  left_join(stations_dist, by = \"start_station\")\n\n\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  mutate(\n    Rain_any   = as.integer(Precipitation &gt; 0),\n    Heavy_rain = as.integer(Precipitation &gt;= 0.3) \n  )\n\n# let's make a little change in the intersection by using \"weekday*rush_hour\"\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  mutate(\n    weekday = ifelse(dotw %in% c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"), 1, 0)\n  )\n\n#split the train adn test data\ntrain &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 37)\n\ntest &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 37)"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#model-improvement",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#model-improvement",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model Improvement",
    "text": "Model Improvement\n\nModel 6: Adjust variables in model 4\nIn this model, we refine some existing variables and use the result as the second baseline for later improved models.\nFirst, based on the summary of Model 4, we noticed that the Precipitation variable did not perform well. However, rain conditions should be an important factor for bike demand. The original variable is very small in magnitude because the total amount of rain is spread across 24 hours. To better capture the effect of rain, we transform it into two dummy variables that indicate whether it is rainy and whether it is raining heavily.\nSecond, we believe the weekend indicator is important because the demand pattern on weekends is different from weekdays. In the previous model, it did not work well because the interaction term weekend:rush_hour is highly collinear with as.factor(hour). To keep the weekend/weekday effect in the model, we replace this term with the interaction as.factor(hour):weekend.\n\n\nCode\nmodel6 &lt;- lm(\n  Trip_Count ~ as.factor(hour)* weekend + dotw_simple + \n    Temperature + \n    Rain_any + Heavy_rain +\n    lag1Hour + lag3Hours + lag1day  + \n    as.factor(month) + \n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y + \n    as.factor(start_station),  # Rush hour effects different on weekends\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model6, test_data = test)\n\n\nModel: model6 \n  Type:               OLS \n  R-squared (train):  0.3702 \n  Adj R-squared:      0.3698 \n  MAE (train):        0.664 \n  MAE (test):         0.6783 \n\n\nInterpretation:\nModel 6 serves as our second baseline model after refining several key predictors.\nOn weekdays, the model shows strong morning and evening peaks that are consistent with commuting.\nOn weekends, the peaks flatten and shift toward the middle of the day.\nThe new rain dummies behave as expected: both rainy and heavy-rain hours are associated with lower trip counts compared with dry conditions.\nDemographics and station fixed effects mainly shift the overall level of demand at each station, while the refined weather and weekend terms improve the shape of the predicted time profile without a large change in overall MAE.\n\n\nModel 7: Add Holiday Indicator and Distance to center city Interation\nThis model is based on Model 6 and adds two new components: a holiday indicator and an interaction between distance to Center City and rush hour. As discussed above, the interaction term is not only used to capture the relationship between distance and peak-hour demand, but also to reduce collinearity with the station fixed effects. Adding these two variables may help the model better detect structural patterns in the data and improve overall performance.\n\n\nCode\nmodel7 &lt;- lm(\n  Trip_Count ~ as.factor(hour)* weekday + \n    dotw_simple + \n    Temperature + \n    Rain_any + Heavy_rain +\n    lag1Hour + lag3Hours + lag1day  + \n    as.factor(month) + \n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y + \n    dist_center_m*rush_hour  +                              # interaction\n    holiday_any +                                           # holiday indicator\n    as.factor(start_station),  \n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model7, test_data = test)\n\n\nModel: model7 \n  Type:               OLS \n  R-squared (train):  0.3751 \n  Adj R-squared:      0.3748 \n  MAE (train):        0.6584 \n  MAE (test):         0.6762 \n\n\nInterpretation:\nModel 7 builds on Model 6 by adding two variables that are motivated directly by our error analysis and knowledge of Philadelphia‚Äôs spatial and temporal structure: a holiday indicator and an interaction between distance to Center City and rush hour. The idea is that holidays and downtown peak periods are exactly when we observed the largest systematic errors in previous models. These additions slightly improve performance: the test MAE decreases from 0.6783 in Model 6 to 0.6762 in Model 7, and (R^2) rises only marginally from 0.3702 to 0.3751. This shows that, even after trying to encode what we learned about the error patterns, urban form, and time-of-day effects into new variables, most of the predictive power still comes from the core ingredients introduced earlier (time of day, temporal lags, and station fixed effects). The remaining errors are likely driven by noise and unobserved factors‚Äîsuch as special events or local disruptions‚Äîthat are difficult to capture with simple linear terms.\n\n\nModel 8: Try a poisson model for count data (base on model 7)\nIn addition to the linear regression models, we also estimate a Poisson regression with a log link to better reflect the count nature of the bike trip data. For this specification, we keep the same set of predictors as in Model 7 and only change the estimation method from OLS to Poisson. This allows us to directly compare the two approaches under an identical model structure. The Poisson model serves as a robustness check: it tests whether the main effects of time-of-day, weekend, weather, lagged demand, distance to Center City, and holidays remain consistent when we use a count model that is more appropriate for non-negative integer outcomes.\n\n\nCode\n# Poisson model with log link\npoisson_model7 &lt;- glm(\n  Trip_Count ~ \n    as.factor(hour) * weekday +       \n    dotw_simple +                     \n    Temperature + Rain_any + Heavy_rain +\n    lag1Hour + lag3Hours + lag1day +\n    as.factor(month) +\n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y +\n    dist_center_m*rush_hour +          \n    holiday_any +\n    as.factor(start_station),\n  family = poisson(link = \"log\"),\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(poisson_model7, test_data = test)\n\n\nModel: poisson_model7 \n  Type:               Poisson \n  R-squared (train):  0.3828 \n  Adj R-squared:      0.3824 \n  MAE (train):        0.6221 \n  MAE (test):         0.6461 \n\n\nInterpretation:\nFinally, we re-estimate Model 7 using a Poisson regression with a log link while keeping exactly the same set of predictors. This change in estimation method leads to a modest but clear improvement in predictive performance: the test MAE decreases from 0.6762 in the OLS version of Model 7 to 0.6461 in the Poisson model, and the (R^2) on the training set increases slightly from 0.3751 to 0.3828. The Poisson specification fits the count nature of bike trips better and reduces some of the systematic under- and over-prediction we saw in the linear model, but the gain is not dramatic, which suggests that most remaining errors are due to noise and unobserved shocks rather than the choice between OLS and Poisson."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#overall-model-evaluation",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#overall-model-evaluation",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Overall Model Evaluation",
    "text": "Overall Model Evaluation\n\n\nCode\n# Get predictions on test set\n\ntest &lt;- test %&gt;%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test),\n    pred6 = predict(model6, newdata = test),\n    pred7 = predict(model7, newdata = test),\n    pred8 = predict(poisson_model7, newdata = test, type = \"response\")\n  )\n\n# Calculate MAE for each model\nmae_results2 &lt;- data.frame(\n  \n  Model = paste0(\n    \"Model \", 1:8\n  ),\n  Description = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\",\n    \"6. Adjust variables (Base on M4)\",\n    \"7. + Holiday Indicator and Distance Interaction\",\n    \"8. Poisson using variables in model 7\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred6), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred7), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred8), na.rm = TRUE)\n  ) \n) %&gt;%\n  mutate(\n    improvement_from_baseline = round((MAE[1] - MAE) / MAE[1] * 100, 1),\n    improvement_from_lags = round((MAE[6] - MAE) / MAE[2] * 100, 1)\n  )\n\nkable(mae_results2, \n      digits = 4,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"Description\", \"MAE (trips)\", \"% Better than M1\", \"% Better than M6\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nDescription\nMAE (trips)\n% Better than M1\n% Better than M6\n\n\n\n\nModel 1\n1. Time + Weather\n0.8293\n0.0\n-22.0\n\n\nModel 2\n2. + Temporal Lags\n0.6857\n17.3\n-1.1\n\n\nModel 3\n3. + Demographics\n0.6868\n17.2\n-1.2\n\n\nModel 4\n4. + Station FE\n0.6831\n17.6\n-0.7\n\n\nModel 5\n5. + Rush Hour Interaction\n0.6833\n17.6\n-0.7\n\n\nModel 6\n6. Adjust variables (Base on M4)\n0.6783\n18.2\n0.0\n\n\nModel 7\n7. + Holiday Indicator and Distance Interaction\n0.6762\n18.5\n0.3\n\n\nModel 8\n8. Poisson using variables in model 7\n0.6461\n22.1\n4.7\n\n\n\n\n\nInterpretation:\nFrom Model 3 to Model 7, we keep adding more variables and refining the specification, but the improvement over Model 2 is quite limited. After including temporal lags in Model 2, the MAE already drops from 0.83 to about 0.69 trips, and this remains the dominant gain. Models 3‚Äì5 add demographics, station fixed effects, and a rush-hour interaction, and Models 6‚Äì7 further adjust weather, weekend, distance, and holiday variables. However, the MAE stays in a very narrow range around 0.68‚Äì0.676, which is only a small improvement compared to Model 2.\nThis pattern suggests that once we control for time-of-day and recent demand history, most additional predictors only provide marginal extra information for short-term station-level demand. Demographics and station dummies mainly shift overall levels rather than change hour-to-hour variation, and the added interaction terms capture more subtle structure but do not drastically reduce prediction error on the test set.\nIn contrast, Model 8 changes the estimation method rather than the predictor set. It applies a Poisson regression using the same variables as Model 7. This shift to a count model leads to a more noticeable improvement: the MAE decreases to 0.6461 trips, which is substantially better than the OLS models with the same predictors. This indicates that better matching the distributional form of the outcome (non-negative counts) can yield additional gains even when the set of explanatory variables is already rich.\n\nVisualize Model Comparison\n\n\nCode\nggplot(mae_results2, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\",alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 4)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#assignment-overview",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\n\n# Load required packages\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(tigris)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(here)\nlibrary(units)\n\n# Load spatial data\ncensus_api_key(\"fef2af5588030f3fd4b474bd111e0d63a7ec2837\")\n\npa_county &lt;- st_read(\"data/Pennsylvania_County_Boundaries.shp\") \npa_hos &lt;- st_read(\"data/hospitals.geojson\")\npa_tracts &lt;- tracts(state = \"PA\", year = 2020, class = \"sf\")\n\n# Check that all data loaded correctly\n\nggplot(data = pa_county) +\n  geom_sf(fill = \"lightblue\", color = \"white\", size = 0.2) +\n  labs(title = \"Census Tracts in Philadelphia County, PA (2020)\",\n       caption = \"Source: US Census Bureau TIGER/Line\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(data = pa_tracts) +\n  geom_sf(fill = \"lightblue\", color = \"white\", size = 0.2) +\n  labs(title = \"Census Tracts in Philadelphia County, PA (2020)\",\n       caption = \"Source: US Census Bureau TIGER/Line\") +\n  theme_minimal()\n\n\n\n\n\n\n\nst_crs(pa_hos)\nst_crs(pa_county)\nst_crs(pa_tracts)\n\nQuestions to answer: - How many hospitals are in your dataset?\n223 rows in the data.\n\nHow many census tracts?\n\n67 rows in the data.\n\nWhat coordinate reference system is each dataset in?\n\nPA hospitals:EPSG 4326 PA county boundaries:EPSG 3857 PA census tracts:EPSG 4269\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables: - Total population - Median household income - Population 65 years and over (you may need to sum multiple age categories)\nYour Task:\n\n# Get demographic data from ACS\npa_pop &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B01003_001\",\n  state = \"PA\",\n  year = 2022,\n  geometry = FALSE\n)\n\npa_income &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  state = \"PA\",\n  year = 2022,\n  geometry = FALSE\n)\nage65_vars &lt;- c(\n  \"B01001_020\", \"B01001_021\", \"B01001_022\", \"B01001_023\", \"B01001_024\", \"B01001_025\", # Male 65+\n  \"B01001_044\", \"B01001_045\", \"B01001_046\", \"B01001_047\", \"B01001_048\", \"B01001_049\"  # Female 65+\n)\n\npa_age65 &lt;- get_acs(\n  geography = \"tract\",\n  variables = age65_vars,\n  state = \"PA\",\n  year = 2022,\n  geometry = FALSE\n) %&gt;%\n  group_by(GEOID) %&gt;%\n  summarize(age65_total = sum(estimate))\n\n\n\n# Join to tract boundaries\n\npa_demographics &lt;- pa_tracts %&gt;%\n  left_join(pa_pop %&gt;% select(GEOID, total_pop = estimate), by = \"GEOID\") %&gt;%\n  left_join(pa_income %&gt;% select(GEOID, median_income = estimate), by = \"GEOID\") %&gt;%\n  left_join(pa_age65, by = \"GEOID\") %&gt;%\n  mutate(pct_65plus = 100 * age65_total / total_pop)\npa_demographics\n\nSimple feature collection with 3446 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.51985 ymin: 39.7198 xmax: -74.68956 ymax: 42.51607\nGeodetic CRS:  NAD83\nFirst 10 features:\n   STATEFP COUNTYFP TRACTCE       GEOID    NAME             NAMELSAD MTFCC\n1       42      095  017703 42095017703  177.03  Census Tract 177.03 G5020\n2       42      101  012204 42101012204  122.04  Census Tract 122.04 G5020\n3       42      129  804300 42129804300    8043    Census Tract 8043 G5020\n4       42      129  804801 42129804801 8048.01 Census Tract 8048.01 G5020\n5       42      129  805200 42129805200    8052    Census Tract 8052 G5020\n6       42      101  012203 42101012203  122.03  Census Tract 122.03 G5020\n7       42      101  013602 42101013602  136.02  Census Tract 136.02 G5020\n8       42      101  034502 42101034502  345.02  Census Tract 345.02 G5020\n9       42      101  000902 42101000902    9.02    Census Tract 9.02 G5020\n10      42      101  001201 42101001201   12.01   Census Tract 12.01 G5020\n   FUNCSTAT   ALAND AWATER    INTPTLAT     INTPTLON total_pop median_income\n1         S 3708021   9639 +40.6498956 -075.3948647      3197         87232\n2         S  879459  56473 +40.0009407 -075.2120769      3915         48000\n3         S 1014739      0 +40.2918690 -079.5478131      2012         54318\n4         S 6852606      0 +40.3011416 -079.6013733      2281         48412\n5         S 1873022      0 +40.1523157 -079.8745928      1865         44375\n6         S  227696      0 +40.0074872 -075.2093403      1074         63750\n7         S  246812      0 +39.9716057 -075.1800748      3975        116375\n8         S 1021734      0 +40.0814101 -075.0384569      5299         55439\n9         S  155167      0 +39.9471596 -075.1566389      2736         89063\n10        S  327555  30728 +39.9470373 -075.1799169      4323        116935\n   age65_total                       geometry pct_65plus\n1         1001 MULTIPOLYGON (((-75.4109 40...  31.310604\n2         1118 MULTIPOLYGON (((-75.22066 4...  28.556833\n3          491 MULTIPOLYGON (((-79.55534 4...  24.403579\n4          513 MULTIPOLYGON (((-79.62842 4...  22.490136\n5          347 MULTIPOLYGON (((-79.88358 4...  18.605898\n6           24 MULTIPOLYGON (((-75.21389 4...   2.234637\n7          393 MULTIPOLYGON (((-75.18637 3...   9.886792\n8          946 MULTIPOLYGON (((-75.04729 4...  17.852425\n9          702 MULTIPOLYGON (((-75.16017 3...  25.657895\n10         428 MULTIPOLYGON (((-75.18714 3...   9.900532\n\nsum(is.na(pa_demographics$median_income))\n\n[1] 63\n\nmedian(pa_demographics$median_income, na.rm = TRUE)\n\n[1] 70188\n\n\npa_hos &lt;- st_transform(pa_hos, st_crs(pa_county)) pa_tracts &lt;- st_transform(pa_tracts, st_crs(pa_county))\nQuestions to answer: - What year of ACS data are you using?\n2022\n\nHow many tracts have missing income data?\n\n63\n\nWhat is the median income across all PA census tracts?\n\n$70188\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\n\n# bottom 25%\nlow_income_threshold &lt;- quantile(pa_demographics$median_income, 0.25, na.rm = TRUE)  \n# top 25%\nhigh_elderly_threshold &lt;- quantile(pa_demographics$pct_65plus, 0.75, na.rm = TRUE)   \n\n\npa_vulnerable &lt;- pa_demographics %&gt;%\n  mutate(\n    vulnerable_income = median_income &lt; low_income_threshold,\n    vulnerable_age = pct_65plus &gt; high_elderly_threshold,\n    vulnerable = vulnerable_income & vulnerable_age\n  )\n\nvulnerable_count &lt;- sum(pa_vulnerable$vulnerable, na.rm = TRUE)\ntotal_tracts &lt;- nrow(pa_vulnerable)\nvulnerable_pct &lt;- 100 * vulnerable_count / total_tracts\n\nvulnerable_count\n\n[1] 165\n\nvulnerable_pct\n\n[1] 4.78816\n\n\nQuestions to answer: - What income threshold did you choose and why?\nI chose the bottom 25% of tracts based on median household income, which corresponds to tracts with income below approximately $54,000. This threshold identifies communities that are relatively low-income compared to the rest of Pennsylvania.\n\nWhat elderly population threshold did you choose and why?\n\nElderly population threshold: I selected the top 25% of tracts by percentage of residents aged 65 and over, representing tracts where more than about 20.5% of residents are elderly. This captures areas with a significantly aging population that may face accessibility and healthcare challenges.\n\nHow many tracts meet your vulnerability criteria?\n\n165\n\nWhat percentage of PA census tracts are considered vulnerable by your definition?\n\n4.78816%\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n# Transform to appropriate projected CRS\npa_vul &lt;- pa_vulnerable %&gt;%\n  filter(vulnerable)\npa_vul_proj &lt;- st_transform(pa_vul, 26917)\npa_hos_proj &lt;- st_transform(pa_hos, 26917)\n\n# Calculate distance from each tract centroid to nearest hospital\n\nvul_center &lt;- pa_vul_proj %&gt;%\n  st_centroid()\n\nvul_center &lt;- vul_center %&gt;%\n  mutate(\n    nearest_idx = st_nearest_feature(., pa_hos_proj)  \n  ) %&gt;%\n  mutate(\n    dist_mi = st_distance(., pa_hos_proj[nearest_idx, ], by_element = TRUE) %&gt;%\n      set_units(\"mi\") %&gt;%\n      drop_units() \n  )\n\nsummary_stats &lt;- vul_center %&gt;%\n  summarize(\n    avg_dist_mi = mean(dist_mi, na.rm = TRUE),\n    max_dist_mi = max(dist_mi, na.rm = TRUE),\n    over15_count = sum(dist_mi &gt; 15, na.rm = TRUE),\n    total_vul = n()\n  ) %&gt;%\n  mutate(\n    pct_over15 = 100 * over15_count / total_vul\n  )\n\nsummary_stats %&gt;%\n  st_drop_geometry() %&gt;%\n  kable(\n    caption = \"Table. Hospital Access Among Vulnerable Census Tracts in Pennsylvania\",\n    digits = 2,\n    align = \"lrrrr\"\n  )\n\n\nTable. Hospital Access Among Vulnerable Census Tracts in Pennsylvania\n\n\navg_dist_mi\nmax_dist_mi\nover15_count\ntotal_vul\npct_over15\n\n\n\n\n4.75\n19.18\n9\n165\n5.45\n\n\n\n\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles - Explain why you chose your projection\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts?\n4.75miles\n\nWhat is the maximum distance?\n\n19.16miles\n\nHow many vulnerable tracts are more than 15 miles from the nearest hospital?\n\n9\n\n\n\nStep 5: Identify Underserved Areas\nDefine ‚Äúunderserved‚Äù as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n# Create underserved variable\n\nvul_center &lt;- vul_center %&gt;%\n  mutate(\n    underserved= (as.numeric(dist_mi&gt; 15))\n  )\n\nunder_count= sum(vul_center$underserved)  \nvul_pct= under_count/ nrow(vul_center)\nunder_count\n\n[1] 9\n\nvul_pct\n\n[1] 0.05454545\n\n\nQuestions to answer: - How many tracts are underserved?\n9\n\nWhat percentage of vulnerable tracts are underserved?\n\n5.45%\n\nDoes this surprise you? Why or why not?\n\nYes ‚Äî I didn‚Äôt expect any tracts to be this far from a hospital. This suggests that while most of Pennsylvania‚Äôs vulnerable areas have reasonable healthcare access, a few rural or remote tracts still face significant geographic barriers to medical services.\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\npa_county &lt;- st_transform(pa_county, crs=26917)\nvul_center&lt;- st_transform(vul_center, crs=st_crs(pa_county))\n\nst_county &lt;- st_join(\n  pa_county,\n  vul_center,\n  join  = st_contains,\n  left  = TRUE                # keep all counties\n)\n\n# Aggregate statistics by county\n\nagg_county &lt;- st_county %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarise(\n    vul_count= sum(vulnerable),\n    und_count= sum(underserved),\n    pct_vul_underserved = 100 * sum(vulnerable & underserved) / sum(vulnerable),\n    avg_dist_vulnerable = mean(dist_mi[vulnerable], na.rm = TRUE),\n    vul_pop = sum(total_pop[vulnerable]),\n    und_pop = sum(total_pop[underserved])\n  )\n\nagg_county &lt;- agg_county %&gt;%\n  mutate(across(where(is.numeric), ~replace(., is.na(.), 0)))\nagg_county\n\nSimple feature collection with 67 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 539820.7 ymin: 4396937 xmax: 1034915 ymax: 4680425\nProjected CRS: NAD83 / UTM zone 17N\n# A tibble: 67 √ó 8\n   COUNTY_NAM vul_count und_count pct_vul_underserved avg_dist_vulnerable\n * &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;               &lt;dbl&gt;               &lt;dbl&gt;\n 1 ADAMS              0         0                   0                0   \n 2 ALLEGHENY         22         0                   0                2.84\n 3 ARMSTRONG          1         0                   0               10.0 \n 4 BEAVER             5         0                   0                2.83\n 5 BEDFORD            2         0                   0                7.97\n 6 BERKS              0         0                   0                0   \n 7 BLAIR              3         0                   0                3.07\n 8 BRADFORD           1         1                 100               16.7 \n 9 BUCKS              0         0                   0                0   \n10 BUTLER             0         0                   0                0   \n# ‚Ñπ 57 more rows\n# ‚Ñπ 3 more variables: vul_pop &lt;dbl&gt;, und_pop &lt;dbl&gt;, geometry &lt;MULTIPOLYGON [m]&gt;\n\n\n\n# --- Step 6: Top 5 counties with highest % of underserved vulnerable tracts ---\ntop5_underserved &lt;- agg_county %&gt;%\n  st_drop_geometry() %&gt;%\n  arrange(desc(pct_vul_underserved)) %&gt;%\n  select(COUNTY_NAM, pct_vul_underserved, vul_count, und_count) %&gt;%\n  head(5)\n\n# --- Step 7: Top 5 counties with largest vulnerable population far from hospitals ---\ntop5_vulpop_far &lt;- agg_county %&gt;%\n  st_drop_geometry() %&gt;%\n  arrange(desc(und_pop)) %&gt;%\n  select(COUNTY_NAM, und_pop, vul_pop, avg_dist_vulnerable) %&gt;%\n  head(5)\n\n# --- Step 8: Display results ---\ntop5_underserved\n\n# A tibble: 5 √ó 4\n  COUNTY_NAM pct_vul_underserved vul_count und_count\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 BRADFORD                   100         1         1\n2 FOREST                     100         1         1\n3 JUNIATA                    100         1         1\n4 MONROE                     100         1         1\n5 SULLIVAN                   100         1         1\n\ntop5_vulpop_far\n\n# A tibble: 5 √ó 4\n  COUNTY_NAM und_pop vul_pop avg_dist_vulnerable\n  &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;               &lt;dbl&gt;\n1 BRADFORD      5466    5466               16.7 \n2 CLEARFIELD    5285   13056               11.4 \n3 DAUPHIN       4392    8410                9.96\n4 POTTER        4257    9062                9.55\n5 CRAWFORD      2842    9072                8.35\n\n\n\n\n\nRequired county-level statistics: - Number of vulnerable tracts - Number of underserved tracts\n- Percentage of vulnerable tracts that are underserved - Average distance to nearest hospital for vulnerable tracts - Total vulnerable population\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts?\nBRADFORD FOREST JUNIATA MONROE SULLIVAN\n\nWhich counties have the most vulnerable people living far from hospitals?\n\nBRADFORD CLEARFIELD DAUPHIN POTTER CRAWFORD\n\nAre there any patterns in where underserved counties are located?\n\nThese are typically larger rural counties where population is dispersed and hospitals are sparse. Urban counties (like Philadelphia or Allegheny) rarely appear because of dense hospital networks.\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\nagg_county &lt;- agg_county %&gt;%\n  mutate(\n    z_vul_pop     = scales::rescale(vul_pop),       # vulnerability\n    z_underserved = scales::rescale(pct_vul_underserved), # service deficit\n    z_distance    = scales::rescale(avg_dist_vulnerable), # accessibility\n    priority_score = 0.25*z_vul_pop + 0.1*z_underserved + 0.65*z_distance\n  )\n\npriority_table &lt;- agg_county %&gt;%\n  arrange(desc(priority_score)) %&gt;%\n  slice(1:10) %&gt;%\n  transmute(\n    `County` = COUNTY_NAM,\n    `Priority Score` = round(priority_score*100, 2),\n    `Vulnerable Population` = comma(vul_pop),\n    `Avg. Dist. to Hospital (mi)` = round(avg_dist_vulnerable, 2),\n    `Underserved (%)` = percent(pct_vul_underserved / 100, accuracy = 0.01),\n    `Vulnerable Tracts` = vul_count,\n    `Underserved Tracts` = und_count,\n  )\n\npriority_table %&gt;%\n  st_drop_geometry() %&gt;%\n  kable(\n    caption = \"***Table 1. Top 10 Priority Counties for Healthcare Investment in Pennsylvania***\",\n    align = \"lrrrrrr\",\n    format = \"pipe\"\n  )\n\n\nTable 1. Top 10 Priority Counties for Healthcare Investment in Pennsylvania\n\n\n\n\n\n\n\n\n\n\n\nCounty\nPriority Score\nVulnerable Population\nAvg. Dist. to Hospital (mi)\nUnderserved (%)\nVulnerable Tracts\nUnderserved Tracts\n\n\n\n\nFOREST\n76.09\n2,701\n18.12\n100.00%\n1\n1\n\n\nMONROE\n74.10\n1,299\n17.73\n100.00%\n1\n1\n\n\nBRADFORD\n72.03\n5,466\n16.68\n100.00%\n1\n1\n\n\nSULLIVAN\n71.10\n918\n16.93\n100.00%\n1\n1\n\n\nJUNIATA\n67.81\n1,782\n15.92\n100.00%\n1\n1\n\n\nHUNTINGDON\n51.77\n2,558\n14.15\n0.00%\n1\n0\n\n\nCLEARFIELD\n48.54\n13,056\n11.37\n25.00%\n4\n1\n\n\nDAUPHIN\n44.11\n8,410\n9.96\n50.00%\n2\n1\n\n\nNORTHUMBERLAND\n43.73\n9,087\n11.17\n0.00%\n4\n0\n\n\nPOTTER\n41.26\n9,062\n9.55\n33.33%\n3\n1\n\n\n\n\n\nRequirements: - Use knitr::kable() or similar for formatting - Include descriptive column names - Format numbers appropriately (commas for population, percentages, etc.) - Add an informative caption - Sort by priority (you decide the metric)"
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#part-2-comprehensive-visualization",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\npa_county_plot &lt;- st_transform(agg_county, crs = 26917)     # NAD83 / UTM zone 17N\npa_hospitals_plot &lt;- st_transform(pa_hos_proj, crs = st_crs(pa_county_plot))\n\n\nggplot() +\n  geom_sf(\n    data = pa_county_plot,\n    aes(fill = pct_vul_underserved),\n    color = \"white\",\n    size = 0.2\n  ) +\n  \n  #Hospital points\n  geom_sf(\n    data = pa_hospitals_plot,\n    aes(shape = \"Hospital\"),  \n    color=\"orange\",\n    size = 1.5,\n    stroke = 0.4\n  ) +\n  \n  scale_fill_gradientn(\n    colors = c(\"grey92\", \"#6baed6\", \"#08306b\"),\n    name = \"% Vulnerable Tracts\\nUnderserved\",\n    labels = label_percent(scale = 1, accuracy = 1)\n  ) +\n  \n  labs(\n    title = \"Healthcare Access Challenges in Pennsylvania\",\n    subtitle = \"Counties shaded by share of vulnerable tracts that are underserved\",\n    caption = \"Source: ACS & Healthcare Facility Data\"\n  ) +\n  \n  theme_void(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, margin = margin(b = 5)),\n    plot.caption = element_text(size = 9, color = \"gray30\"),\n    legend.title = element_text(size = 11, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\npa_tracts &lt;- st_transform(pa_tracts, crs=26917)\nvul_center &lt;- st_transform(vul_center, crs=26917)\nvul_tracts_poly &lt;- st_join(pa_tracts, vul_center, join= st_contains)\n\n# Create detailed tract-level map\nggplot() +\n  geom_sf(\n    data = vul_tracts_poly,\n    aes(fill = as.factor(underserved)),\n    color = \"grey80\",\n    size = 0.5\n  ) +\n  \n  geom_sf(\n    data = pa_hospitals_plot,\n    aes(shape = \"Hospital\", color = \"Hospital\"),  # üëà ÈÄöËøá aes() ËøõÂÖ•Âõæ‰æã\n    size = 1.2,\n    stroke = 0.4\n  ) +\n  \n scale_fill_manual(\n    name = \"Underserved Status\",\n    values = c(\"0\" = \"lightcoral\", \"1\" = \"firebrick3\"),  # üëà ÂÆö‰πâ 0/1/NA È¢úËâ≤\n    labels = c(\"0\"=\"Vulnerable\", \"1\" = \"Vulnerabale & Underserved\" ),\n )+\n  \n  scale_color_manual(\n    name = \"\",  \n    values = c(\"Hospital\" = \"orange\")\n  ) +\n  scale_shape_manual(\n    name = \"\",\n    values = c(\"Hospital\" = 16)\n  ) +\n  \n  labs(\n    title = \"Healthcare Access Challenges in Pennsylvania\",\n    subtitle = \"Counties shaded by share of vulnerable tracts that are underserved\",\n    caption = \"Source: ACS & Healthcare Facility Data\"\n  ) +\n  \n  theme_void(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, margin = margin(b = 5)),\n    plot.caption = element_text(size = 9, color = \"gray30\"),\n    legend.title = element_text(size = 11, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\n\nggplot(vul_tracts_poly, aes(x = dist_mi, y = total_pop)) +\n  geom_point(alpha = 0.6, color = \"#FF7F50\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray30\") +\n  labs(\n    title = \"Hospital Distance vs. Vulnerable Population Size\",\n    x = \"Distance to Nearest Hospital (miles)\",\n    y = \"Vulnerable Population (people)\",\n    caption = \"Tracts farther from hospitals tend to have smaller but more isolated vulnerable populations.\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs.¬†vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\n\nEducation & Youth Services\nOption A: Educational Desert Analysis - Data: Schools, Libraries, Recreation Centers, Census tracts (child population) - Question: ‚ÄúWhich neighborhoods lack adequate educational infrastructure for children?‚Äù - Operations: Buffer schools/libraries (0.5 mile walking distance), identify coverage gaps, overlay with child population density - Policy relevance: School district planning, library placement, after-school program siting\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: ‚ÄúAre school zones safe for walking/biking, or are they crime hotspots?‚Äù - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nEnvironmental Justice\nOption C: Green Space Equity - Data: Parks, Street Trees, Census tracts (race/income demographics) - Question: ‚ÄúDo low-income and minority neighborhoods have equitable access to green space?‚Äù - Operations: Buffer parks (10-minute walk = 0.5 mile), calculate tree canopy or park acreage per capita, compare by demographics - Policy relevance: Climate resilience, environmental justice, urban forestry investment ‚Äî\n\n\nPublic Safety & Justice\nOption D: Crime & Community Resources - Data: Crime Incidents, Recreation Centers, Libraries, Street Lights - Question: ‚ÄúAre high-crime areas underserved by community resources?‚Äù - Operations: Aggregate crime counts to census tracts or neighborhoods, count community resources per area, spatial correlation analysis - Policy relevance: Community investment, violence prevention strategies ‚Äî\n\n\nInfrastructure & Services\nOption E: Polling Place Accessibility - Data: Polling Places, SEPTA stops, Census tracts (elderly population, disability rates) - Question: ‚ÄúAre polling places accessible for elderly and disabled voters?‚Äù - Operations: Buffer polling places and transit stops, identify vulnerable populations, find areas lacking access - Policy relevance: Voting rights, election infrastructure, ADA compliance\n\n\n\nHealth & Wellness\nOption F: Recreation & Population Health - Data: Recreation Centers, Playgrounds, Parks, Census tracts (demographics) - Question: ‚ÄúIs lack of recreation access associated with vulnerable populations?‚Äù - Operations: Calculate recreation facilities per capita by neighborhood, buffer facilities for walking access, overlay with demographic indicators - Policy relevance: Public health investment, recreation programming, obesity prevention\n\n\n\nEmergency Services\nOption G: EMS Response Coverage - Data: Fire Stations, EMS stations, Population density, High-rise buildings - Question: ‚ÄúAre population-dense areas adequately covered by emergency services?‚Äù - Operations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas - Policy relevance: Emergency preparedness, station siting decisions\n\n\n\nArts & Culture\nOption H: Cultural Asset Distribution - Data: Public Art, Museums, Historic sites/markers, Neighborhoods - Question: ‚ÄúDo all neighborhoods have equitable access to cultural amenities?‚Äù - Operations: Count cultural assets per neighborhood, normalize by population, compare distribution across demographic groups - Policy relevance: Cultural equity, tourism, quality of life, neighborhood identity\n\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nRecommended Starting Points\nIf you‚Äôre feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure: - You can access the spatial data - You can perform at least 2 spatial operations\n\n\nYour Analysis\nYour Task:\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\n\nphi_school &lt;- read_csv(\"data_2/PaSchoolDistricts2024_03/2025-2026 Master School List (20251001).csv\")\nphi_crime &lt;- read_csv(\"data_2/PaSchoolDistricts2024_03/incidents_part1_part2.csv\")\nbike &lt;- st_read(\"data_2/PaSchoolDistricts2024_03/Bike_Network.shp\")\n\nReading layer `Bike_Network' from data source \n  `C:\\Users\\12345\\Documents\\GitHub\\portfolio-setup-jyxu48\\Assignments\\assignment_2\\data_2\\PaSchoolDistricts2024_03\\Bike_Network.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5225 features and 8 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -8378937 ymin: 4847835 xmax: -8345146 ymax: 4883978\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ncity_limit &lt;- st_read(\"data_2/PaSchoolDistricts2024_03/City_Limits.geojson\")\n\nReading layer `City_Limits' from data source \n  `C:\\Users\\12345\\Documents\\GitHub\\portfolio-setup-jyxu48\\Assignments\\assignment_2\\data_2\\PaSchoolDistricts2024_03\\City_Limits.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -75.28031 ymin: 39.86747 xmax: -74.95575 ymax: 40.13793\nGeodetic CRS:  WGS 84\n\nphi_school &lt;- phi_school %&gt;%\n  separate(`GPS Location`, into = c(\"lat\", \"lng\"), sep = \",\") %&gt;%\n  mutate(\n    lat = as.numeric(lat),\n    lng = as.numeric(lng)\n  )\nphi_crime &lt;- phi_crime %&gt;% drop_na(lng, lat)\n\nst_school &lt;- st_as_sf(phi_school, coords = c(\"lng\", \"lat\"), crs = 4326)\nst_crime &lt;- st_as_sf(phi_crime, coords = c(\"lng\", \"lat\"), crs = 4326)\n\ntarget_crs &lt;- 26918\n\nst_school &lt;- st_transform(st_school, crs = target_crs)\nst_crime  &lt;- st_transform(st_crime,  crs = target_crs)\nbike       &lt;- st_transform(bike, crs = target_crs)\ncity_limit &lt;- st_transform(city_limit, crs = target_crs)\n\nggplot() +\n  geom_sf(data = city_limit, fill = \"grey95\", color = \"grey60\", size = 0.4) + \n  geom_sf(data = bike, color = \"#2b83ba\", size = 0.5) +\n  geom_sf(data = st_school, color = \"coral\", shape = 16, size = 1.2) +\n  labs(\n    title = \"Philadelphia Schools, Crime Incidents, and Bike Network\",\n    caption = \"Source: OpenDataPhilly\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\nQuestions to answer: - What dataset did you choose and why?\nI choose School List of Philadelphia, Crime Incidents of Philadelphia and Bike Networks. I choose these data because it‚Äôs a question directly related to urban safety and equitable access for students. - What is the data source and date?\nThe data is from ‚Äúopendataphilly.org‚Äù and it is the data of 2025. (date of bike network data is unknown) - How many features does it contain?\nSchool List contains 1:Administrative info 2:Contact info 3:Location Crime Incident contains 1:Time 2:Location 3:Crime category Bike Networks contains 1:Name 2:road info(type,length,location) - What CRS is it in? Did you need to transform it?\nThey are all in EPSG:4326 (WGS84). Yes I need to transform them into another CRS(EPSG:26918) which is more accurate locally.\n\n\nPose a research question\n\nWrite a clear research statement that your analysis will answer.\nAre schools located in high-crime areas also underserved by safe biking and walking infrastructure? Examples: - ‚ÄúDo vulnerable tracts have adequate public transit access to hospitals?‚Äù - ‚ÄúAre EMS stations appropriately located near vulnerable populations?‚Äù - ‚ÄúDo areas with low vehicle access have worse hospital access?‚Äù\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+): - Buffers - Spatial joins - Spatial filtering with predicates - Distance calculations - Intersections or unions - Point-in-polygon aggregation\nYour Task:\n\n# Your spatial analysis\n\nst_school &lt;- st_school %&gt;%\n  rename(Publication_Name = `Publication Name`)\nschool_buffers &lt;- st_buffer(st_school, dist = 305)  # 1000 ft ‚âà 305 m\n\ncrime_in_buffer &lt;- st_join(st_crime, school_buffers, join = st_within)\n\ncrime_summary &lt;- crime_in_buffer %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(Publication_Name) %&gt;%\n  summarise(crime_n = n())\n\n# Spatial intersection: calculate total length of bike network inside each buffer\nbike_in_buffer &lt;- st_intersection(bike, school_buffers)\n\nbike_summary &lt;- bike_in_buffer %&gt;%\n  group_by(Publication_Name) %&gt;%\n  summarise(bike_length_m = sum(st_length(geometry))) %&gt;%\n  mutate(bike_length_mi = set_units(bike_length_m, \"mi\"))\n\n# Combine results into a single summary table\nschool_safety &lt;- school_buffers %&gt;%\n  left_join(st_drop_geometry(crime_summary), by = \"Publication_Name\") %&gt;%\n  left_join(st_drop_geometry(bike_summary),  by = \"Publication_Name\") %&gt;%\n  mutate(\n    crime_n = replace_na(crime_n, 0),\n    bike_length_mi = replace_na(as.numeric(bike_length_mi), 0),\n    safety_index = crime_n / (bike_length_mi + 0.1)\n  )\n\n# Summary statistics\nsummary_stats &lt;- school_safety %&gt;%\n  st_drop_geometry() %&gt;%\n  summarise(\n    mean_crime = mean(crime_n),\n    mean_bike = mean(as.numeric(bike_length_mi)),\n    high_crime_schools = sum(crime_n &gt; mean_crime),\n    low_bike_schools = sum(as.numeric(bike_length_mi) &lt; mean_bike)\n  )\n\nprint(summary_stats)\n\n# A tibble: 1 √ó 4\n  mean_crime mean_bike high_crime_schools low_bike_schools\n       &lt;dbl&gt;     &lt;dbl&gt;              &lt;int&gt;            &lt;int&gt;\n1       183.     0.430                135              203\n\n# Visualization: highlight high-crime, low-bike schools\np1 &lt;- ggplot() +\n  geom_sf(data = city_limit, fill = \"grey95\", color = \"grey60\") +\n  geom_sf(data = bike, color = \"grey70\", size = 0.2) +\n  geom_sf(\n    data = school_safety,\n    aes(fill = safety_index),\n    shape = 21, color = \"black\", size = 2\n  ) +\n  scale_fill_gradientn(\n    colors = c(\n      \"#313695\",  \n      \"#74add1\",  \n      \"#e0f3f8\",  \n      \"#fee090\",  \n      \"#f46d43\",  \n      \"#a50026\"   \n    ),\n    name = \"Crime to Bike_length\"\n  ) +\n  \n  labs(\n    title = \"Philadelphia School Safety Zones\",\n    subtitle = \"High values indicate more crime and less bike coverage\",\n  ) +\n  \n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 8),\n    legend.position = \"right\"\n  )\n\np2 &lt;- ggplot() +\n  geom_sf(data = city_limit, fill = \"grey95\", color = \"grey60\") +\n  geom_sf(data = bike, color = \"grey70\", size = 0.4) +\n  geom_sf(\n    data = school_safety,\n    aes(fill = crime_n),\n    shape = 21, color = \"black\", size = 2\n  ) +\n  scale_fill_gradientn(\n    colors = c(\n      \"#313695\",  \n      \"#74add1\",  \n      \"#e0f3f8\",  \n      \"#fee090\",  \n      \"#f46d43\",  \n      \"#a50026\" \n    ),\n    name = \"Crime Incidents\\n(Count)\"\n  ) +\n  labs(\n    title = \"Philadelphia School Safety Zones\",\n    subtitle = \"Crime incidents within 1000ft school safety zones\",\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 8),\n    legend.position = \"right\"\n  )\n\np3 &lt;- ggplot() +\n  geom_sf(data = city_limit, fill = \"grey95\", color = \"grey60\") +\n  geom_sf(data = bike, color = \"grey70\", size = 0.4) +\n  geom_sf(\n    data = school_safety,\n    aes(fill = bike_length_mi),\n    shape = 21, color = \"black\", size = 2\n  ) +\n  scale_fill_gradientn(\n    colors = c(\n      \"#a50026\",\n      \"#f46d43\", \n      \"#fee090\",\n      \"#e0f3f8\", \n      \"#74add1\", \n      \"#313695\"\n    ),\n    name = \"Bike Network\\n(Miles)\"\n  ) +\n  labs(\n    title = \"Philadelphia School Safety Zones\",\n    subtitle = \"Bike infrastructure length within 1000ft school safety zones\",\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 8),\n    legend.position = \"right\"\n  )\n\n(p1 | p2 | p3) +\n  plot_annotation(\n    title = \"Philadelphia School Safety Zone Analysis\",\n    caption = \"Data: OpenDataPhilly\"\n  )\n\n\n\n\n\n\n\n\nAnalysis requirements: - Clear code comments explaining each step - Appropriate CRS transformations - Summary statistics or counts - At least one map showing your findings - Brief interpretation of results (3-5 sentences)\nYour interpretation:\nThe maps show that high-crime and low-bike-coverage school zones largely overlap in central to northern and Western Philadelphia, indicating a clear spatial correlation between the two. This suggests that areas with more crime tend to have less biking infrastructure, highlighting potential safety and accessibility concerns for students in those neighborhoods."
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nTake a few moments to clean up your markdown document and then write a line or two or three about how you may have incorporated feedback that you recieved after your first assignment.\nI deleted some descriptive sentences and symbols so now the Assignment1 looks cleaner."
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#submission-requirements",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it‚Äôs a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Minnesota Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#scenario",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Minnesota Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#learning-objectives",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#data-retrieval",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = my_state,\n  variables = c(\n    median_income = \"B19013_001\",   # Median Household Income\n    total_population = \"B01003_001\" # Total Population\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\ncounty_data\n\n# A tibble: 87 √ó 6\n   GEOID NAME  median_incomeE median_incomeM total_populationE total_populationM\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n 1 27001 Aitk‚Ä¶          56406           2136             15859                NA\n 2 27003 Anok‚Ä¶          95782           1203            363985                NA\n 3 27005 Beck‚Ä¶          68683           2197             35202                NA\n 4 27007 Belt‚Ä¶          62173           3292             46274                NA\n 5 27009 Bent‚Ä¶          70346           3948             41300                NA\n 6 27011 Big ‚Ä¶          63024           5327              5161                NA\n 7 27013 Blue‚Ä¶          70906           3327             69022                NA\n 8 27015 Brow‚Ä¶          67038           3230             25880                NA\n 9 27017 Carl‚Ä¶          74660           4154             36362                NA\n10 27019 Carv‚Ä¶         116308           3386            107216                NA\n# ‚Ñπ 77 more rows\n\n# Clean the county names to remove state name and \"County\" \n\n# Hint: use mutate() with str_remove()\ncounty_data_clean &lt;- county_data %&gt;%\n  mutate(\n    NAME = str_remove(NAME, \",.*\"),         # remove state name after the comma\n    NAME = str_remove(NAME, \" County$\")     # remove trailing \"County\"\n  )\n# Display the first few rows\nhead(county_data_clean)\n\n# A tibble: 6 √ó 6\n  GEOID NAME   median_incomeE median_incomeM total_populationE total_populationM\n  &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1 27001 Aitkin          56406           2136             15859                NA\n2 27003 Anoka           95782           1203            363985                NA\n3 27005 Becker          68683           2197             35202                NA\n4 27007 Beltr‚Ä¶          62173           3292             46274                NA\n5 27009 Benton          70346           3948             41300                NA\n6 27011 Big S‚Ä¶          63024           5327              5161                NA"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#data-quality-assessment",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Create reliability categories\n# Calculate MOE percentage and reliability categories using mutate()\ncounty_data_clean &lt;- county_data_clean %&gt;%\n  mutate(MOE = (median_incomeM / median_incomeE) * 100)\n\ncounty_data_clean &lt;- county_data_clean %&gt;%\n  mutate(reliability = case_when(\n    MOE &lt; 5 ~ \"High Confidence\",\n    MOE &gt;= 5 & MOE &lt;= 10 ~ \"Moderate Confidence\",\n    MOE &gt; 10 ~ \"Low Confidence\"\n  ))\n\n\ncounty_data_clean &lt;- county_data_clean %&gt;%\n  mutate(unreliable_flag = MOE &gt; 10)\ncounty_data_clean\n\n# A tibble: 87 √ó 9\n   GEOID NAME  median_incomeE median_incomeM total_populationE total_populationM\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n 1 27001 Aitk‚Ä¶          56406           2136             15859                NA\n 2 27003 Anoka          95782           1203            363985                NA\n 3 27005 Beck‚Ä¶          68683           2197             35202                NA\n 4 27007 Belt‚Ä¶          62173           3292             46274                NA\n 5 27009 Bent‚Ä¶          70346           3948             41300                NA\n 6 27011 Big ‚Ä¶          63024           5327              5161                NA\n 7 27013 Blue‚Ä¶          70906           3327             69022                NA\n 8 27015 Brown          67038           3230             25880                NA\n 9 27017 Carl‚Ä¶          74660           4154             36362                NA\n10 27019 Carv‚Ä¶         116308           3386            107216                NA\n# ‚Ñπ 77 more rows\n# ‚Ñπ 3 more variables: MOE &lt;dbl&gt;, reliability &lt;chr&gt;, unreliable_flag &lt;lgl&gt;\n\n# Create a summary showing count of counties in each reliability category\n\nreliability_summary &lt;- county_data_clean %&gt;%\n  count(reliability) %&gt;%\n  mutate(percentage = (n / sum(n)) * 100)\n\nreliability_summary\n\n# A tibble: 3 √ó 3\n  reliability             n percentage\n  &lt;chr&gt;               &lt;int&gt;      &lt;dbl&gt;\n1 High Confidence        46      52.9 \n2 Low Confidence          1       1.15\n3 Moderate Confidence    40      46.0 \n\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#high-uncertainty-counties",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\ntop5 &lt;- county_data_clean %&gt;%\n  arrange(desc(MOE)) %&gt;%\n  slice(1:5) %&gt;%\n  select(\n    County = NAME,\n    Median_Income = median_incomeE,\n    Margin_of_Error = median_incomeM,\n    MOE_Percentage = MOE,\n    Reliability = reliability\n  )\n# Format as table with kable() - include appropriate column names and caption\nkable(top5, caption = \"Top 5 Counties with Highest MOE Percentage (Median Income)\")\n\n\nTop 5 Counties with Highest MOE Percentage (Median Income)\n\n\n\n\n\n\n\n\n\nCounty\nMedian_Income\nMargin_of_Error\nMOE_Percentage\nReliability\n\n\n\n\nTraverse\n63456\n6655\n10.487582\nLow Confidence\n\n\nCottonwood\n63586\n6049\n9.513100\nModerate Confidence\n\n\nLake of the Woods\n61667\n5812\n9.424814\nModerate Confidence\n\n\nChippewa\n62112\n5823\n9.375000\nModerate Confidence\n\n\nRed Lake\n73889\n6742\n9.124498\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\n[High MOE means higher reliability for decision making. Small sample size or hidden factors account for high MOEs.]"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#focus-area-selection",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\nhigh_conf &lt;- county_data_clean %&gt;%\n  filter(reliability == \"High Confidence\") %&gt;%\n  slice(2)\n\nmoderate_conf &lt;- county_data_clean %&gt;%\n  filter(reliability == \"Moderate Confidence\") %&gt;%\n  slice(9)\n\nlow_conf &lt;- county_data_clean %&gt;%\n  filter(reliability == \"Low Confidence\") %&gt;%\n  slice(1)\n\nselected_counties_allv &lt;- bind_rows(high_conf, moderate_conf, low_conf)\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nselected_counties &lt;- selected_counties_allv %&gt;%\n  select(NAME, median_incomeE, MOE, reliability, total_populationE)\nselected_counties\n\n# A tibble: 3 √ó 5\n  NAME     median_incomeE   MOE reliability         total_populationE\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt;\n1 Anoka             95782  1.26 High Confidence                363985\n2 Douglas           72472  6.57 Moderate Confidence             39081\n3 Traverse          63456 10.5  Low Confidence                   3345\n\n\nComment on the output: [The output is clear, but I don‚Äôt know if it‚Äôs good to ignore the GEOID :-]"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#tract-level-demographics",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You‚Äôll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\nrace &lt;- c(\n  white = \"B03002_003\",\n  black = \"B03002_004\",\n  hispanic = \"B03002_012\",\n  total_pop = \"B03002_001\"\n)\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\nselected_county_codes &lt;- selected_counties_allv %&gt;%\n  mutate(county_code = str_sub(GEOID, 3, 5)) %&gt;%  \n  pull(county_code)\n\ntract_data &lt;- get_acs(\n  geography = \"tract\",\n  state = \"27\",\n  county = selected_county_codes,\n  variables = race,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\ntract_data\n\n# A tibble: 103 √ó 10\n   GEOID       NAME   whiteE whiteM blackE blackM hispanicE hispanicM total_popE\n   &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 27003050107 Censu‚Ä¶   2776    244      8     13        27        38       2931\n 2 27003050108 Censu‚Ä¶   5072    342     97     94       240       227       5737\n 3 27003050109 Censu‚Ä¶   5296    592     19     26       273       229       5673\n 4 27003050110 Censu‚Ä¶   2553    415     11     16        97        96       2773\n 5 27003050111 Censu‚Ä¶   3286    413     55     82        35        36       3455\n 6 27003050114 Censu‚Ä¶   3076    355      0      9        20        31       3173\n 7 27003050115 Censu‚Ä¶   5140    550      0     13        52        59       5783\n 8 27003050116 Censu‚Ä¶   4089    254     34     35        77        68       4518\n 9 27003050208 Censu‚Ä¶   2698    512    428    337        32        29       3304\n10 27003050210 Censu‚Ä¶   3801    170      0      9        20        28       4180\n# ‚Ñπ 93 more rows\n# ‚Ñπ 1 more variable: total_popM &lt;dbl&gt;\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\ntract_data2 &lt;- tract_data %&gt;%\n  mutate(pct_white = (whiteE / total_popE) * 100) %&gt;%\n  mutate(pct_black = (blackE / total_popE) * 100) %&gt;%\n  mutate(pct_hispanic = (hispanicE / total_popE) * 100)\n\ntract_data2 \n\n# A tibble: 103 √ó 13\n   GEOID       NAME   whiteE whiteM blackE blackM hispanicE hispanicM total_popE\n   &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 27003050107 Censu‚Ä¶   2776    244      8     13        27        38       2931\n 2 27003050108 Censu‚Ä¶   5072    342     97     94       240       227       5737\n 3 27003050109 Censu‚Ä¶   5296    592     19     26       273       229       5673\n 4 27003050110 Censu‚Ä¶   2553    415     11     16        97        96       2773\n 5 27003050111 Censu‚Ä¶   3286    413     55     82        35        36       3455\n 6 27003050114 Censu‚Ä¶   3076    355      0      9        20        31       3173\n 7 27003050115 Censu‚Ä¶   5140    550      0     13        52        59       5783\n 8 27003050116 Censu‚Ä¶   4089    254     34     35        77        68       4518\n 9 27003050208 Censu‚Ä¶   2698    512    428    337        32        29       3304\n10 27003050210 Censu‚Ä¶   3801    170      0      9        20        28       4180\n# ‚Ñπ 93 more rows\n# ‚Ñπ 4 more variables: total_popM &lt;dbl&gt;, pct_white &lt;dbl&gt;, pct_black &lt;dbl&gt;,\n#   pct_hispanic &lt;dbl&gt;\n\n# Add readable tract and county name columns using str_extract() or similar\n\ntract_data3 &lt;- tract_data2 %&gt;%\n  mutate(\n    tract_id = str_extract(NAME, \"Census Tract \\\\d+\"),     \n    tract_id = str_remove(tract_id, \"Census Tract \"),\n    \n    county_name = str_extract(NAME, \"(?&lt;=; ).* County\"),     \n    county_name = str_remove(county_name, \" County$\"),\n    \n    state_name  = str_extract(NAME, \"(?&lt;=; )[^;]+$\")         \n  ) %&gt;%\n  relocate(tract_id, county_name, state_name, .after = NAME)\n\ntract_data3\n\n# A tibble: 103 √ó 16\n   GEOID       NAME  tract_id county_name state_name whiteE whiteM blackE blackM\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 27003050107 Cens‚Ä¶ 501      Anoka       Minnesota    2776    244      8     13\n 2 27003050108 Cens‚Ä¶ 501      Anoka       Minnesota    5072    342     97     94\n 3 27003050109 Cens‚Ä¶ 501      Anoka       Minnesota    5296    592     19     26\n 4 27003050110 Cens‚Ä¶ 501      Anoka       Minnesota    2553    415     11     16\n 5 27003050111 Cens‚Ä¶ 501      Anoka       Minnesota    3286    413     55     82\n 6 27003050114 Cens‚Ä¶ 501      Anoka       Minnesota    3076    355      0      9\n 7 27003050115 Cens‚Ä¶ 501      Anoka       Minnesota    5140    550      0     13\n 8 27003050116 Cens‚Ä¶ 501      Anoka       Minnesota    4089    254     34     35\n 9 27003050208 Cens‚Ä¶ 502      Anoka       Minnesota    2698    512    428    337\n10 27003050210 Cens‚Ä¶ 502      Anoka       Minnesota    3801    170      0      9\n# ‚Ñπ 93 more rows\n# ‚Ñπ 7 more variables: hispanicE &lt;dbl&gt;, hispanicM &lt;dbl&gt;, total_popE &lt;dbl&gt;,\n#   total_popM &lt;dbl&gt;, pct_white &lt;dbl&gt;, pct_black &lt;dbl&gt;, pct_hispanic &lt;dbl&gt;"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#demographic-analysis",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\ntop_hispanic &lt;- tract_data3 %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) \n\ntop_hispanic2 &lt;- top_hispanic %&gt;%\n  select(tract_id,county_name,state_name,total_popM,pct_white,pct_black,pct_hispanic)\n\nkable(\n  top_hispanic2, \n  caption = \"Tract with Highest Hispanic/Latino Percentage\"\n) \n\n\nTract with Highest Hispanic/Latino Percentage\n\n\n\n\n\n\n\n\n\n\n\ntract_id\ncounty_name\nstate_name\ntotal_popM\npct_white\npct_black\npct_hispanic\n\n\n\n\n512\nAnoka\nMinnesota\n593\n50.65913\n12.24105\n28.24859\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\ncounty_summary &lt;- tract_data3 %&gt;%\n  group_by(county_name) %&gt;%\n  summarise(\n    n_tracts = n(),\n    avg_white = mean(pct_white, na.rm = TRUE),\n    avg_black = mean(pct_black, na.rm = TRUE),\n    avg_hispanic = mean(pct_hispanic, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\n# Create a nicely formatted table of your results using kable()\nkable(\n  county_summary,\n  digits = 1,  \n  caption = \"Average Demographics by County\"\n)\n\n\nAverage Demographics by County\n\n\ncounty_name\nn_tracts\navg_white\navg_black\navg_hispanic\n\n\n\n\nAnoka\n90\n77.4\n7.8\n5.3\n\n\nDouglas\n11\n94.3\n0.3\n2.3\n\n\nTraverse\n2\n85.2\n0.5\n3.8"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#moe-analysis-for-demographic-variables",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\ntract_moe &lt;- tract_data3 %&gt;%\n  mutate(\n    white_moe_pct    = (whiteM / whiteE) * 100,\n    black_moe_pct    = (blackM / blackE) * 100,\n    hispanic_moe_pct = (hispanicM / hispanicE) * 100\n  )\ntract_moe\n\n# A tibble: 103 √ó 19\n   GEOID       NAME  tract_id county_name state_name whiteE whiteM blackE blackM\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 27003050107 Cens‚Ä¶ 501      Anoka       Minnesota    2776    244      8     13\n 2 27003050108 Cens‚Ä¶ 501      Anoka       Minnesota    5072    342     97     94\n 3 27003050109 Cens‚Ä¶ 501      Anoka       Minnesota    5296    592     19     26\n 4 27003050110 Cens‚Ä¶ 501      Anoka       Minnesota    2553    415     11     16\n 5 27003050111 Cens‚Ä¶ 501      Anoka       Minnesota    3286    413     55     82\n 6 27003050114 Cens‚Ä¶ 501      Anoka       Minnesota    3076    355      0      9\n 7 27003050115 Cens‚Ä¶ 501      Anoka       Minnesota    5140    550      0     13\n 8 27003050116 Cens‚Ä¶ 501      Anoka       Minnesota    4089    254     34     35\n 9 27003050208 Cens‚Ä¶ 502      Anoka       Minnesota    2698    512    428    337\n10 27003050210 Cens‚Ä¶ 502      Anoka       Minnesota    3801    170      0      9\n# ‚Ñπ 93 more rows\n# ‚Ñπ 10 more variables: hispanicE &lt;dbl&gt;, hispanicM &lt;dbl&gt;, total_popE &lt;dbl&gt;,\n#   total_popM &lt;dbl&gt;, pct_white &lt;dbl&gt;, pct_black &lt;dbl&gt;, pct_hispanic &lt;dbl&gt;,\n#   white_moe_pct &lt;dbl&gt;, black_moe_pct &lt;dbl&gt;, hispanic_moe_pct &lt;dbl&gt;\n\n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\ntract_moe &lt;- tract_moe %&gt;%\n  mutate(\n    high_moe_flag = ifelse(\n      white_moe_pct &gt; 10 | black_moe_pct &gt; 10 | hispanic_moe_pct &gt; 10,\n      TRUE, FALSE\n    )\n  )\n\n# Create summary statistics showing how many tracts have data quality issues\nmoe_summary &lt;- tract_moe %&gt;%\n  summarise(\n    total_tracts = n(),\n    tracts_high_moe = sum(high_moe_flag, na.rm = TRUE),\n    pct_high_moe = (tracts_high_moe / total_tracts) * 100\n  )\n\nmoe_summary\n\n# A tibble: 1 √ó 3\n  total_tracts tracts_high_moe pct_high_moe\n         &lt;int&gt;           &lt;int&gt;        &lt;dbl&gt;\n1          103             103          100"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#pattern-analysis",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n# Use group_by() and summarize() to create this comparison\\\n\npattern_sum &lt;- tract_moe %&gt;%\n  group_by(high_moe_flag) %&gt;%\n  summarise(\n    n_tracts       = n(),\n    avg_pop = mean(total_popE, na.rm = TRUE),\n    avg_white_pct  = mean(pct_white, na.rm = TRUE),\n    avg_black_pct  = mean(pct_black, na.rm = TRUE),\n    avg_hispanic_pct = mean(pct_hispanic, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\n\n# Create a professional table showing the patterns\n\nkable(\n  pattern_sum,\n  digits = 2,\n  caption = \"Comparison of Tracts by High vs. Low MOE\"\n)\n\n\nComparison of Tracts by High vs.¬†Low MOE\n\n\n\n\n\n\n\n\n\n\nhigh_moe_flag\nn_tracts\navg_pop\navg_white_pct\navg_black_pct\navg_hispanic_pct\n\n\n\n\nTRUE\n103\n3945.74\n79.39\n6.82\n4.93\n\n\n\n\n# INCIDENTÔºö The result showed that 100% of tracts were flagged as TRUE.\n# Therefore, the next step is to take a more detailed approach\n\ntract_moe1 &lt;- tract_moe %&gt;%\n  mutate(\n    white_high_moe    = white_moe_pct &gt; 10,\n    black_high_moe    = black_moe_pct &gt; 10,\n    hispanic_high_moe = hispanic_moe_pct &gt; 10\n  )\n\n\nrace_moe_sum &lt;- tract_moe1 %&gt;%\n  summarise(\n    white_flagged    = sum(white_high_moe, na.rm = TRUE),\n    black_flagged    = sum(black_high_moe, na.rm = TRUE),\n    hispanic_flagged = sum(hispanic_high_moe, na.rm = TRUE)\n  )\n\nrace_moe_sum\n\n# A tibble: 1 √ó 3\n  white_flagged black_flagged hispanic_flagged\n          &lt;int&gt;         &lt;int&gt;            &lt;int&gt;\n1            83           103              103\n\n\nPattern Analysis: The analysis reveals a clear pattern: racial and ethnic minority populations have consistently less reliable data at the tract level. Specifically, the MOE exceeded 10% for all Black and Hispanic/Latino population estimates across the selected tracts, while White population estimates were somewhat more reliable, though still flagged in over half of the tracts."
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#analysis-integration-and-professional-summary",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\nOverall Pattern Identification The multi-level analysis ‚Äî spanning county-level median income estimates and tract-level demographic breakdowns ‚Äî reveals systematic data reliability challenges in ACS 5-year estimates County-level: At county scale, median household income estimates show notable variability. Using a 10% MOE cutoff, most counties fell into moderate or low confidence categories, highlighting that uncertainty exists even at higher aggregation levels Tract-level: Reliability issues intensify at the tract scale. Every tract exceeded the 10% MOE threshold for at least one demographic subgroup, 100% of tracts were flagged as high-MOE:(\nEquity Assessment The communities at greatest risk of algorithmic bias are: Smaller or rural tracts: Populations with fewer respondents also face higher sampling error, compounding reliability problems. Black and Hispanic/Latino communities: Their tract-level ACS estimates never meet the 10% reliability threshold. If an allocation algorithm treats these noisy estimates as precise, it will undervalue their needs and direct resources elsewhere.\nRoot Cause Analysis The patterns are rooted in structural features of ACS data collection: Sampling limits: Smaller population groups produce larger standard errors. Differential nonresponse: Minority and immigrant communities may less likely to respond due to barriers and distrust. Survey design constraints: The ACS 5-year estimates aggregate limited samples, and subgroup breakdowns at tract scale exceed the data‚Äôs design precision.\nStrategic Recommendations Data useÔºö Flag any tract with MOE% &gt; 10 and avoid treating estimates as precise since sample size is small in tract level. Use fallback geography (county) for variables with extreme MOE.\nStrengthen inputsÔºö Blend multiple indicators (poverty, rent burden, child poverty) to reduce reliance on a single noisy variable. Improve ACS participation via targeted outreach and language support in undercounted communities.\nOutside the data: Introduce human review for flagged tracts before final funding decisions. Pilot and monitor the algorithm annually, recalibrating thresholds as survey data quality evolves. Document and communicate uncertainty explicitly to policymakers and stakeholders."
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#specific-recommendations",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\nsum_table &lt;- county_data_clean %&gt;%\n  select(NAME, median_incomeE, MOE, reliability) %&gt;%\n  mutate(\n    recommendation = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  )\n\n# Format as a professional table with kable()\n\nkable(\n  sum_table,\n  digits = 1,\n  caption = \"Decision Framework for Algorithmic Implementation\"\n)\n\n\nDecision Framework for Algorithmic Implementation\n\n\n\n\n\n\n\n\n\nNAME\nmedian_incomeE\nMOE\nreliability\nrecommendation\n\n\n\n\nAitkin\n56406\n3.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAnoka\n95782\n1.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBecker\n68683\n3.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBeltrami\n62173\n5.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nBenton\n70346\n5.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nBig Stone\n63024\n8.5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nBlue Earth\n70906\n4.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBrown\n67038\n4.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCarlton\n74660\n5.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCarver\n116308\n2.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCass\n61970\n2.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nChippewa\n62112\n9.4\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nChisago\n97446\n3.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nClay\n75006\n4.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nClearwater\n62723\n5.1\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCook\n71937\n6.5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCottonwood\n63586\n9.5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCrow Wing\n65975\n3.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nDakota\n101360\n1.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nDodge\n92890\n4.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nDouglas\n72472\n6.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFaribault\n64000\n5.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFillmore\n73234\n4.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nFreeborn\n65679\n3.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGoodhue\n78338\n4.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGrant\n67600\n7.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHennepin\n92595\n1.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHouston\n71580\n8.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHubbard\n67197\n3.5\nHigh Confidence\nSafe for algorithmic decisions\n\n\nIsanti\n84063\n4.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nItasca\n63962\n4.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nJackson\n68368\n5.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nKanabec\n68995\n4.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKandiyohi\n73285\n4.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKittson\n66000\n6.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nKoochiching\n59779\n7.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLac qui Parle\n66967\n5.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLake\n73860\n4.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLake of the Woods\n61667\n9.4\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLe Sueur\n87180\n4.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLincoln\n64750\n7.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLyon\n68919\n4.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMcLeod\n73296\n5.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMahnomen\n52739\n5.5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMarshall\n69396\n4.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMartin\n61674\n8.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMeeker\n75926\n3.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMille Lacs\n68088\n4.5\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMorrison\n66264\n4.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMower\n66972\n5.7\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMurray\n71500\n5.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nNicollet\n79113\n5.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nNobles\n62973\n8.1\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nNorman\n65278\n6.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nOlmsted\n90420\n3.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOtter Tail\n67990\n3.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPennington\n71504\n6.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPine\n65059\n3.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPipestone\n68341\n8.5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPolk\n69540\n5.4\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPope\n71212\n2.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRamsey\n78108\n1.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRed Lake\n73889\n9.1\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nRedwood\n65617\n5.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nRenville\n66313\n3.5\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRice\n78214\n3.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRock\n75060\n8.8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nRoseau\n70122\n3.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSt.¬†Louis\n66491\n2.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nScott\n118268\n3.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSherburne\n99431\n4.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSibley\n74781\n4.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nStearns\n73105\n2.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSteele\n79722\n4.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nStevens\n69737\n8.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSwift\n58362\n8.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nTodd\n63216\n5.5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nTraverse\n63456\n10.5\nLow Confidence\nRequires manual review or additional data\n\n\nWabasha\n75063\n4.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWadena\n54747\n4.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWaseca\n71856\n5.1\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWashington\n110828\n2.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWatonwan\n65197\n8.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWilkin\n67114\n5.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWinona\n66162\n6.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWright\n102980\n3.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYellow Medicine\n70605\n6.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: Aitkin, Anoka, Becker, Blue Earth, Brown, Carver, Cass, Chisago, Clay, Crow Wing, Dakota, Dodge, Fillmore, Freeborn, Goodhue, Hennepin, Hubbard, Isanti, Itasca, Kanabec, Kandiyohi, Lake, Le Sueur, Lyon, Marshall, Meeker, Mille Lacs, Morrison, Olmsted, Otter Tail, Pine, Pope, Ramsey, Renville, Rice, Roseau, St.¬†Louis, Scott, Sherburne, Sibley, Stearns, Steele, Wabasha, Wadena, Washington, Wright\n\nCounties listed above have suitable sample size and stronger ACS samples, which means lower MOE. Demographic estimates are robust enough to support algorithm-driven funding without significant reliability concerns. Algorithms here can be implemented immediately with standard fairness monitoring.\n\nCounties requiring additional oversight: Beltrami, Benton, Big Stone, Carlton, Chippewa, Clearwater, Cook, Cottonwood, Douglas, Faribault, Grant, Houston, Jackson, Kittson, Koochiching, Lac qui Parle, Lincoln, McLeod, Mahnomen, Martin, Mower, Murray, Nicollet, Nobles, Norman, Pennington, Pipestone, Polk, Red Lake, Redwood, Rock, Swift, Todd, Waseca, Watonwan, Wilkin, Winona, Yellow Medicine.\n\nRecommended monitoring actions for counties above: Outcome audits: Compare algorithmic allocations against program participation data. Sensitivity testing: Run models with upper/lower confidence bounds (estimate ¬± MOE) to test robustness. Flagging system: Automatically flag tracts/counties with borderline estimates for human review.\n\nCounties needing alternative approaches:ss Traverse\n\nSuggested alternatives: Manual review: Use staff expertise and local knowledge to adjust allocations. Supplemental data: Incorporate administrative records (school eligibility, Medicaid enrollment, unemployment claims) to cross-validate needs. Community engagement: Consult local service providers or conduct targeted surveys to supplement missing precision."
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#questions-for-further-investigation",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\nBeyond race/ethnicity, do other vulnerable groups (e.g., low-income households, elderly residents, non-English speakers) also experience systematically higher MOE? Could combining ACS with administrative datasets (SNAP, TANF, Medicaid) reduce disparities in data reliability? How stable are tract-level estimates over time, and do reliability issues persist across survey cycles?"
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html",
    "title": "Spatial Predictive Analysis",
    "section": "",
    "text": "In this Assignment, I will build a spatial predictive model for burglaries using count regression and spatial features.I will document the process, and interpret results."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#assignment-overview",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#assignment-overview",
    "title": "Spatial Predictive Analysis",
    "section": "",
    "text": "In this Assignment, I will build a spatial predictive model for burglaries using count regression and spatial features.I will document the process, and interpret results."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-1.1-load-chicago-spatial-data",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-1.1-load-chicago-spatial-data",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 1.1: Load Chicago Spatial Data",
    "text": "Exercise 1.1: Load Chicago Spatial Data\n\n\nCode\n# Load police districts (used for spatial cross-validation)\npoliceDistricts &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(District = dist_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 25 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load police beats (smaller administrative units)\npoliceBeats &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(Beat = beat_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 277 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load Chicago boundary\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\") %&gt;%\n  st_transform('ESRI:102271')\n\n\nReading layer `chicagoBoundary' from data source \n  `https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -87.8367 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304\nGeodetic CRS:  WGS 84\n\n\nCode\ncat(\"‚úì Loaded spatial boundaries\\n\")\n\n\n‚úì Loaded spatial boundaries\n\n\nCode\ncat(\"  - Police districts:\", nrow(policeDistricts), \"\\n\")\n\n\n  - Police districts: 25 \n\n\nCode\ncat(\"  - Police beats:\", nrow(policeBeats), \"\\n\")\n\n\n  - Police beats: 277 \n\n\n\n\n\n\n\n\nNoteCoordinate Reference System\n\n\n\nWe‚Äôre using ESRI:102271 (Illinois State Plane East, NAD83, US Feet). This is appropriate for Chicago because:\n\nIt minimizes distortion in this region\nUses feet (common in US planning)\nAllows accurate distance calculations"
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-1.2-load-burglary-data",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-1.2-load-burglary-data",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 1.2: Load Burglary Data",
    "text": "Exercise 1.2: Load Burglary Data\n\n\nCode\n# Load from provided data file (downloaded from Chicago open data portal)\nburglaries &lt;- st_read(here(\"labs/lab4/data\", \"burglaries.shp\")) %&gt;% \n  st_transform('ESRI:102271')\n\n\nReading layer `burglaries' from data source \n  `C:\\Users\\12345\\Documents\\GitHub\\portfolio-setup-jyxu48\\labs\\lab4\\data\\burglaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 7482 features and 22 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 340492 ymin: 552959.6 xmax: 367153.5 ymax: 594815.1\nProjected CRS: NAD83(HARN) / Illinois East\n\n\nCode\n# Check the data\ncat(\"\\n‚úì Loaded burglary data\\n\")\n\n\n\n‚úì Loaded burglary data\n\n\nCode\ncat(\"  - Number of burglaries:\", nrow(burglaries), \"\\n\")\n\n\n  - Number of burglaries: 7482 \n\n\nCode\ncat(\"  - CRS:\", st_crs(burglaries)$input, \"\\n\")\n\n\n  - CRS: ESRI:102271 \n\n\nCode\ncat(\"  - Date range:\", min(burglaries$Date, na.rm = TRUE), \"to\", \n    max(burglaries$Date, na.rm = TRUE), \"\\n\")\n\n\n  - Date range: 17167 to 17532 \n\n\nInterpretation 1.1: There are 7482 rows of burglaries in the dataset. The time period is from 2017/01/01 to 2018/01/01, indicates that it contains 366 days of data.\nIt is essential that all datasets share the same Coordinate Reference System (CRS). If layers are kept in different CRS formats, spatial operations will produce incorrect or misaligned results.\nFor this project, because we are working in Chicago, we use the specific projected CRS ‚ÄúESRI: 102271 ‚Äî Illinois StatePlane East (meters)‚Äù that provides accurate distance and area measurements for Chicago.\n\n\n\n\n\n\nWarningCritical Pause #1: Data Provenance\n\n\n\nBefore proceeding, consider where this data came from:\nWho recorded this data? Chicago Police Department officers and detectives\nWhat might be missing?\n\nUnreported burglaries (victims didn‚Äôt call police)\nIncidents police chose not to record\nDowngraded offenses (burglary recorded as trespassing)\nSpatial bias (more patrol = more recorded crime)\n\nThink About Was there a Department of Justice investigation of CPD during this period? What did they find about data practices?"
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-1.3-visualize-point-data",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-1.3-visualize-point-data",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 1.3: Visualize Point Data",
    "text": "Exercise 1.3: Visualize Point Data\n\n\nCode\n# Simple point map\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = burglaries, color = \"#d62828\", size = 0.1, alpha = 0.4) +\n  labs(\n    title = \"Burglary Locations\",\n    subtitle = paste0(\"Chicago 2017, n = \", nrow(burglaries))\n  )\n\n# Density surface using modern syntax\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(burglaries)),\n    aes(X, Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"  # Modern ggplot2 syntax (not guide = FALSE)\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Kernel density estimation\"\n  )\n\n# Combine plots using patchwork (modern approach)\np1 + p2 + \n  plot_annotation(\n    title = \"Spatial Distribution of Burglaries in Chicago\",\n    tag_levels = 'A'\n  )\n\n\n\n\n\n\n\n\n\nInterpretation 1.2:\nThe spatial pattern of burglaries in Chicago is clustered rather than evenly distributed. The point map shows that incidents concentrate in several distinct corridors rather than being spread uniformly across the city. The KDE surface reinforces this pattern with three major hotspot zones:\n1.Near North / River North area 2.West Side around Humboldt Park and Logan Square 3.South Side corridor extending from Washington Park toward Englewood\nThese locations share certain environmental features commonly associated with elevated burglary risk. In addition, some hotspots overlap areas with documented social and economic disadvantage, vacant housing, and higher levels of physical disorder‚Äîfactors linked to burglary opportunity structures in environmental criminology."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-2.1-understanding-the-fishnet",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-2.1-understanding-the-fishnet",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 2.1: Understanding the Fishnet",
    "text": "Exercise 2.1: Understanding the Fishnet\nA fishnet grid converts irregular point data into a regular grid of cells where we can:\n\nAggregate counts\nCalculate spatial features\nApply regression models\n\n\n\nCode\n# Create 500m x 500m grid\nfishnet &lt;- st_make_grid(\n  chicagoBoundary,\n  cellsize = 500,  # 500 meters per cell\n  square = TRUE\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Keep only cells that intersect Chicago\nfishnet &lt;- fishnet[chicagoBoundary, ]\n\n# View basic info\ncat(\"‚úì Created fishnet grid\\n\")\n\n\n‚úì Created fishnet grid\n\n\nCode\ncat(\"  - Number of cells:\", nrow(fishnet), \"\\n\")\n\n\n  - Number of cells: 2458 \n\n\nCode\ncat(\"  - Cell size:\", 500, \"x\", 500, \"meters\\n\")\n\n\n  - Cell size: 500 x 500 meters\n\n\nCode\ncat(\"  - Cell area:\", round(st_area(fishnet[1,])), \"square meters\\n\")\n\n\n  - Cell area: 250000 square meters\n\n\nInterpretation 2.1:\nI use a regular grid because it gives us equal-sized, neutral spatial units instead of boundaries that were drawn for political or administrative reasons. This makes the analysis cleaner: cell areas are comparable, distance-based features are easier to compute, and we reduce some of the MAUP problems that come from highly uneven, irregular neighborhoods or census tracts.\nThe trade-off is that grid cells are analytically convenient but socially artificial. They do not correspond to real neighborhoods, police beats, or planning districts, so the results are harder to explain to practitioners and often need to be translated back into those official units.\nIn short: grids are better for modelling, official boundaries are better for communication."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-2.2-aggregate-burglaries-to-grid",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-2.2-aggregate-burglaries-to-grid",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 2.2: Aggregate Burglaries to Grid",
    "text": "Exercise 2.2: Aggregate Burglaries to Grid\n\n\nCode\n# Spatial join: which cell contains each burglary?\nburglaries_fishnet &lt;- st_join(burglaries, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(countBurglaries = n())\n\n# Join back to fishnet (cells with 0 burglaries will be NA)\nfishnet &lt;- fishnet %&gt;%\n  left_join(burglaries_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(countBurglaries = replace_na(countBurglaries, 0))\n\n# Summary statistics\ncat(\"\\nBurglary count distribution:\\n\")\n\n\n\nBurglary count distribution:\n\n\nCode\nsummary(fishnet$countBurglaries)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   3.042   5.000  40.000 \n\n\nCode\ncat(\"\\nCells with zero burglaries:\", \n    sum(fishnet$countBurglaries == 0), \n    \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$countBurglaries == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero burglaries: 781 / 2458 ( 31.8 %)\n\n\n\n\nCode\n# Visualize aggregated counts\nggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  geom_sf(data = chicagoBoundary, fill = NA, color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(\n    name = \"Burglaries\",\n    option = \"plasma\",\n    trans = \"sqrt\",  # Square root for better visualization of skewed data\n    breaks = c(0, 1, 5, 10, 20, 40)\n  ) +\n  labs(\n    title = \"Burglary Counts by Grid Cell\",\n    subtitle = \"500m x 500m cells, Chicago 2017\"\n  ) +\n  theme_crime()\n\n\n\n\n\n\n\n\n\nInterpretation 2.2:\nThe burglary counts are skewed: a large number of grid cells have little or zero incidents, while a small number of cells have relatively high counts. This produces a classic zero-inflated and overdispersed distribution, where the variance is much larger than the mean.\nSo many zeros appear because burglary is a spatially clustered event. Most parts of the city experience little or no burglary activity, while a few hotspots absorb most incidents. This distribution is not well-fit by a standard Poisson model, which assumes that the mean and variance are equal. Instead, the overdispersion suggests that models like the Negative Binomial or a zero-inflated count model are more appropriate."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.1-load-311-sanitation-code-complaints",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.1-load-311-sanitation-code-complaints",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 4.1: Load 311 Sanitation Code Complaints",
    "text": "Exercise 4.1: Load 311 Sanitation Code Complaints\n\n\nCode\nsanitation_coms &lt;- read_csv(\"data/311_Service_Requests_-_Sanitation_Code_Complaints_-_Historical_20251113.csv\")%&gt;%\n    filter(\n    mdy(`Creation Date`) &gt;= mdy(\"12/31/2016\") &\n    mdy(`Creation Date`) &lt;  mdy(\"01/02/2018\")\n  ) %&gt;% \n  filter(!is.na(Latitude), !is.na(Longitude)) %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102271')\n\ncat(\"‚úì Loaded Sanitation Code Complaints\\n\")\n\n\n‚úì Loaded Sanitation Code Complaints\n\n\nCode\ncat(\"  - Number of calls:\", nrow(sanitation_coms), \"\\n\")\n\n\n  - Number of calls: 19767 \n\n\n\n\n\n\n\n\nNoteData Loading Note\n\n\n\nThe data was downloaded from Chicago‚Äôs Open Data Portal. You can now request an api from the Chicago portal and tap into the data there.\nConsider: How might the 311 reporting system itself be biased? Who calls 311? What neighborhoods have better 311 awareness?\nThe 311 system is not a neutral measure of neighborhood conditions. Reporting depends on who knows about 311, who trusts government systems, and who feels empowered to complain. Higher-income and higher-resource neighborhoods tend to have greater 311 awareness, more digital access, and stronger expectations of government responsiveness, which leads to more reporting. In contrast, lower-income communities, immigrant neighborhoods, and historically over-policed areas may underreport due to limited awareness, language barriers, distrust of city services, or simply different norms around what is ‚Äúworth‚Äù reporting."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.2-count-of-sanitation-code-complaints-per-cell",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.2-count-of-sanitation-code-complaints-per-cell",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 4.2: Count of Sanitation Code Complaints per Cell",
    "text": "Exercise 4.2: Count of Sanitation Code Complaints per Cell\n\n\nCode\n# Aggregate Sanitation Code Complaints to fishnet\n\nsanitation_fishnet &lt;- st_join(sanitation_coms, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(sanitation_coms = n())\n\n# Join to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(sanitation_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(sanitation_coms = replace_na(sanitation_coms, 0))\n\ncat(\"Sanitation code complaints distribution:\\n\")\n\n\nSanitation code complaints distribution:\n\n\nCode\nsummary(sanitation_coms)\n\n\n Creation Date         Status          Completion Date   \n Length:19767       Length:19767       Length:19767      \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n Service Request Number Type of Service Request\n Length:19767           Length:19767           \n Class :character       Class :character       \n Mode  :character       Mode  :character       \n                                               \n                                               \n                                               \n                                               \n What is the Nature of this Code Violation? Street Address        ZIP Code    \n Length:19767                               Length:19767       Min.   :60601  \n Class :character                           Class :character   1st Qu.:60618  \n Mode  :character                           Mode  :character   Median :60628  \n                                                               Mean   :60630  \n                                                               3rd Qu.:60641  \n                                                               Max.   :60827  \n                                                               NA's   :38     \n  X Coordinate      Y Coordinate          Ward       Police District\n Min.   :1108635   Min.   :1814788   Min.   : 1.00   Min.   : 1.00  \n 1st Qu.:1152208   1st Qu.:1858904   1st Qu.:13.50   1st Qu.: 7.00  \n Median :1163111   Median :1896362   Median :25.00   Median :12.00  \n Mean   :1162881   Mean   :1889196   Mean   :24.43   Mean   :12.62  \n 3rd Qu.:1172575   3rd Qu.:1916944   3rd Qu.:35.00   3rd Qu.:18.00  \n Max.   :1205116   Max.   :1951488   Max.   :50.00   Max.   :25.00  \n                                                                    \n Community Area    Location                  geometry    \n Min.   : 1.00   Length:19767       POINT        :19767  \n 1st Qu.:19.00   Class :character   epsg:NA      :    0  \n Median :29.00   Mode  :character   +proj=tmer...:    0  \n Mean   :36.12                                           \n 3rd Qu.:58.00                                           \n Max.   :77.00                                           \n                                                         \n\n\n\n\nCode\nfishnet$san_cont &lt;- pmin(fishnet$sanitation_coms, 60)\n\nfishnet$san_over60 &lt;- fishnet$sanitation_coms &gt; 60\n\n# Sanitation code complaints\np1 &lt;- ggplot() +\n  geom_sf(\n    data = fishnet,\n    aes(fill = san_cont),\n    color = NA\n  ) +\n  scale_fill_viridis_c(\n    option = \"plasma\",     # purple ‚Üí yellow\n    limits = c(0, 60),     \n    name = \"Sanitation\"\n  ) +\n  geom_sf(\n    data = subset(fishnet, san_over60),\n    fill = \"#f0f921\",    \n    color = NA\n  ) +\n  labs(title = \"Sanitation Code Complaints (Continuous Gradient, 60+ Highlighted)\") +\n  theme_crime()\n\n\n# Burglaries\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\") +\n  labs(title = \"Burglaries\") +\n  theme_crime()\n\np1 + p2 +\n  plot_annotation(\n    title = \"Comparison of Sanitation Complaints and Burglaries (Fixed Bins)\"\n  )\n\n\n\n\n\n\n\n\n\nInterpretation 4.1:\nBoth maps show broadly overlapping hotspot corridors, especially in the south side. This visual overlap suggests that physical disorder and burglary risk cluster in similar environments‚Äîplaces with poor maintenance, dumping, or abandoned properties also show elevated burglary activity. This aligns with environmental criminology and ‚Äúbroken windows‚Äù dynamics: neighborhoods with more visible disorder may signal lower guardianship and create more opportunities for property crime.\nIn short: the two patterns are not identical, but they cluster in many of the same spaces, indicating a potential relationship worth modeling statistically."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.3-nearest-neighbor-features",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.3-nearest-neighbor-features",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 4.3: Nearest Neighbor Features",
    "text": "Exercise 4.3: Nearest Neighbor Features\nCount in a cell is one measure. Distance to the nearest 3 sanitation complaints captures local context.\n\n\nCode\n# Calculate mean distance to 3 nearest sanitation complaints\n# (Do this OUTSIDE of mutate to avoid sf conflicts)\n\n# Get coordinates\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\nsanitation_coords &lt;- st_coordinates(sanitation_coms)\n\n# Calculate k nearest neighbors and distances\nnn_result &lt;- get.knnx(sanitation_coords, fishnet_coords, k = 3)\n\n# Add to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    sanitation_coms.nn = rowMeans(nn_result$nn.dist)\n  )\n\ncat(\"‚úì Calculated nearest neighbor distances\\n\")\n\n\n‚úì Calculated nearest neighbor distances\n\n\nCode\nsummary(fishnet$sanitation_coms.nn)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   1.031  108.565  181.196  294.538  332.417 2213.320 \n\n\nInterpretation 4.2:\nA low value of sanitation_coms.nn means that the cell is very close to several sanitation code complaints.This indicates the cell is surrounded by visible physical disorder.\nA high value means the nearest sanitation complaints are far away, suggesting that the surrounding area has few sanitation-related problems or lower reporting activity.\nThis measure is informative because distance to sanitation complaints captures the spatial context of disorder, not just what happens inside a single grid cell. Even if a cell has zero complaints itself, being close to areas with repeated sanitation violations may still signal neighborhood sanitation problem ‚Äîcondition may associated with higher burglary risk."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.4-distance-to-hot-spots",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.4-distance-to-hot-spots",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 4.4: Distance to Hot Spots",
    "text": "Exercise 4.4: Distance to Hot Spots\nLet‚Äôs identify clusters of sanitation complaints using Local Moran‚Äôs I, then calculate distance to these hot spots.\n\n\nCode\n# Function to calculate Local Moran's I\ncalculate_local_morans &lt;- function(data, variable, k = 5) {\n  \n  # Create spatial weights\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n  \n  # Calculate Local Moran's I\n  local_moran &lt;- localmoran(data[[variable]], weights)\n  \n  # Classify clusters\n  mean_val &lt;- mean(data[[variable]], na.rm = TRUE)\n  \n  data %&gt;%\n    mutate(\n      local_i = local_moran[, 1],\n      p_value = local_moran[, 5],\n      is_significant = p_value &lt; 0.05,\n      \n      moran_class = case_when(\n        !is_significant ~ \"Not Significant\",\n        local_i &gt; 0 & .data[[variable]] &gt; mean_val ~ \"High-High\",\n        local_i &gt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-Low\",\n        local_i &lt; 0 & .data[[variable]] &gt; mean_val ~ \"High-Low\",\n        local_i &lt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-High\",\n        TRUE ~ \"Not Significant\"\n      )\n    )\n}\n\n# Apply to sanitation code complaints\nfishnet &lt;- calculate_local_morans(fishnet, \"sanitation_coms\", k = 5)\n\n\n\n\nCode\n# Visualize hot spots\nggplot() +\n  geom_sf(\n    data = fishnet, \n    aes(fill = moran_class), \n    color = NA\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"High-High\" = \"#d7191c\",\n      \"High-Low\" = \"#fdae61\",\n      \"Low-High\" = \"#abd9e9\",\n      \"Low-Low\" = \"#2c7bb6\",\n      \"Not Significant\" = \"gray90\"\n    ),\n    name = \"Cluster Type\"\n  ) +\n  labs(\n    title = \"Local Moran's I: sanitation code complatins Clusters\",\n    subtitle = \"High-High = Hot spots of disorder\"\n  ) +\n  theme_crime()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Get centroids of \"High-High\" cells (hot spots)\nhotspots &lt;- fishnet %&gt;%\n  filter(moran_class == \"High-High\") %&gt;%\n  st_centroid()\n\n# Calculate distance from each cell to nearest hot spot\nif (nrow(hotspots) &gt; 0) {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(\n      dist_to_hotspot = as.numeric(\n        st_distance(st_centroid(fishnet), hotspots %&gt;% st_union())\n      )\n    )\n  \n  cat(\"‚úì Calculated distance to sanitation code comlaints hot spots\\n\")\n  cat(\"  - Number of hot spot cells:\", nrow(hotspots), \"\\n\")\n} else {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(dist_to_hotspot = 0)\n  cat(\"‚ö† No significant hot spots found\\n\")\n}\n\n\n‚úì Calculated distance to sanitation code comlaints hot spots\n  - Number of hot spot cells: 203 \n\n\nInterpretation 4.3:\nDistance to a cluster of sanitation complaints is more informative than distance to a single complaint because clusters capture persistent, spatially concentrated disorder, not just isolated events. A single sanitation complaint could be random, one-time, or caused by a single resident. But when many nearby cells all show high sanitation complaints, it signals a structural problem in the poor sanitation maintenance.\nLocal Moran‚Äôs I identifies exactly these patterns by detecting spatial autocorrelation. High-High clusters indicate areas where a cell with high sanitation complaints is surrounded by other high-value cells. These are true hotspots of disorder, not random noise. Low-High areas reveal local outliers, where a relatively clean cell sits next to a disorder hotspot.\n\n\n\n\n\n\nNote\n\n\n\nLocal Moran‚Äôs I identifies:\n\nHigh-High: Hot spots (high values surrounded by high values)\nLow-Low: Cold spots (low values surrounded by low values)\nHigh-Low / Low-High: Spatial outliers\n\nThis helps us understand spatial clustering patterns."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-6.1-poisson-regression",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-6.1-poisson-regression",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 6.1: Poisson Regression",
    "text": "Exercise 6.1: Poisson Regression\nBurglary counts are count data (0, 1, 2, 3‚Ä¶). We‚Äôll use Poisson regression.\n\n\nCode\n# Create clean modeling dataset\nfishnet_model &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    uniqueID,\n    District,\n    countBurglaries,\n    sanitation_coms,\n    sanitation_coms.nn,\n    dist_to_hotspot\n  ) %&gt;%\n  na.omit()  # Remove any remaining NAs\n\ncat(\"‚úì Prepared modeling data\\n\")\n\n\n‚úì Prepared modeling data\n\n\nCode\ncat(\"  - Observations:\", nrow(fishnet_model), \"\\n\")\n\n\n  - Observations: 1708 \n\n\nCode\ncat(\"  - Variables:\", ncol(fishnet_model), \"\\n\")\n\n\n  - Variables: 6 \n\n\n\n\nCode\n# Fit Poisson regression\nmodel_poisson &lt;- glm(\n  countBurglaries ~ sanitation_coms + sanitation_coms.nn + \n    dist_to_hotspot,\n  data = fishnet_model,\n  family = \"poisson\"\n)\n\n# Summary\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = countBurglaries ~ sanitation_coms + sanitation_coms.nn + \n    dist_to_hotspot, family = \"poisson\", data = fishnet_model)\n\nCoefficients:\n                      Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept)         2.22789690  0.03588604  62.083 &lt;0.0000000000000002 ***\nsanitation_coms     0.00264926  0.00106871   2.479              0.0132 *  \nsanitation_coms.nn -0.00449667  0.00017781 -25.289 &lt;0.0000000000000002 ***\ndist_to_hotspot    -0.00014660  0.00001029 -14.247 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6710.3  on 1707  degrees of freedom\nResidual deviance: 4060.3  on 1704  degrees of freedom\nAIC: 8128.6\n\nNumber of Fisher Scoring iterations: 5\n\n\nInterpretation 6.1:\nInterpret the coefficients.\n\nsanitation_coms (positive coefficient) Estimate: +0.00265 Cells with more sanitation code complaints tend to have slightly higher burglary counts. This suggests that areas with more visible disorder (trash issues, sanitation violations) also experience more burglaries, consistent with environmental criminology.\nsanitation_coms.nn (negative coefficient) Estimate: ‚Äì0.00450 This variable measures the average distance to nearby sanitation complaints. A negative sign means: Closer to sanitation complaint clusters ‚Üí more burglaries Farther away ‚Üí fewer burglaries This strengthens the idea that being near clusters of disorder is more predictive of burglary risk than just having a complaint inside the cell.\ndist_to_hotspot (negative coefficient) Estimate: ‚Äì0.00015 Cells closer to the burglary hotspot have higher burglary counts. As distance increases, burglaries decline‚Äîtypical spatial decay around a known hotspot."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-6.2-check-for-overdispersion",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-6.2-check-for-overdispersion",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 6.2: Check for Overdispersion",
    "text": "Exercise 6.2: Check for Overdispersion\nPoisson regression assumes mean = variance. Real count data often violates this (overdispersion).\n\n\nCode\n# Calculate dispersion parameter\ndispersion &lt;- sum(residuals(model_poisson, type = \"pearson\")^2) / \n              model_poisson$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 2), \"\\n\")\n\n\nDispersion parameter: 2.55 \n\n\nCode\ncat(\"Rule of thumb: &gt;1.5 suggests overdispersion\\n\")\n\n\nRule of thumb: &gt;1.5 suggests overdispersion\n\n\nCode\nif (dispersion &gt; 1.5) {\n  cat(\"‚ö† Overdispersion detected! Consider Negative Binomial model.\\n\")\n} else {\n  cat(\"‚úì Dispersion looks okay for Poisson model.\\n\")\n}\n\n\n‚ö† Overdispersion detected! Consider Negative Binomial model."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-6.3-negative-binomial-regression",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-6.3-negative-binomial-regression",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 6.3: Negative Binomial Regression",
    "text": "Exercise 6.3: Negative Binomial Regression\nIf overdispersed, use Negative Binomial regression (more flexible).\n\n\nCode\n# Fit Negative Binomial model\nmodel_nb &lt;- glm.nb(\n  countBurglaries ~ sanitation_coms + sanitation_coms.nn + \n    dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Summary\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = countBurglaries ~ sanitation_coms + sanitation_coms.nn + \n    dist_to_hotspot, data = fishnet_model, init.theta = 2.394358425, \n    link = log)\n\nCoefficients:\n                      Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept)         2.26783716  0.06164584  36.788 &lt;0.0000000000000002 ***\nsanitation_coms     0.00338771  0.00206738   1.639               0.101    \nsanitation_coms.nn -0.00483958  0.00025566 -18.930 &lt;0.0000000000000002 ***\ndist_to_hotspot    -0.00014494  0.00001549  -9.359 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.3944) family taken to be 1)\n\n    Null deviance: 3090.5  on 1707  degrees of freedom\nResidual deviance: 1784.5  on 1704  degrees of freedom\nAIC: 7152.6\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.394 \n          Std. Err.:  0.154 \n\n 2 x log-likelihood:  -7142.640 \n\n\nCode\n# Compare AIC (lower is better)\ncat(\"\\nModel Comparison:\\n\")\n\n\n\nModel Comparison:\n\n\nCode\ncat(\"Poisson AIC:\", round(AIC(model_poisson), 1), \"\\n\")\n\n\nPoisson AIC: 8128.6 \n\n\nCode\ncat(\"Negative Binomial AIC:\", round(AIC(model_nb), 1), \"\\n\")\n\n\nNegative Binomial AIC: 7152.6 \n\n\nInterpretation 6.2:\nmodel comparison\nThe Negative Binomial model fits better, with an AIC of 7152.6, compared to the Poisson model‚Äôs AIC of 8128.6. Lower AIC indicates a superior model, so the Negative Binomial clearly provides a better description of the burglary counts.\nThis tells us again that the data exhibits overdispersion: the variance is much larger than the mean, which violates the core assumption of the Poisson model. The Negative Binomial handles this extra variability by adding a dispersion parameter (theta), allowing it to model the heavy-tailed, zero-inflated structure of the burglary data more accurately."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-8.1-generate-final-predictions",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-8.1-generate-final-predictions",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 8.1: Generate Final Predictions",
    "text": "Exercise 8.1: Generate Final Predictions\n\n\nCode\n# Fit final model on all data\nfinal_model &lt;- glm.nb(\n  countBurglaries ~ sanitation_coms + sanitation_coms.nn + \n    dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Add predictions back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_nb = predict(final_model, fishnet_model, type = \"response\")[match(uniqueID, fishnet_model$uniqueID)]\n  )\n\n# Also add KDE predictions (normalize to same scale as counts)\nkde_sum &lt;- sum(fishnet$kde_value, na.rm = TRUE)\ncount_sum &lt;- sum(fishnet$countBurglaries, na.rm = TRUE)\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_kde = (kde_value / kde_sum) * count_sum\n  )"
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-8.2-compare-model-vs.-kde-baseline",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-8.2-compare-model-vs.-kde-baseline",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 8.2: Compare Model vs.¬†KDE Baseline",
    "text": "Exercise 8.2: Compare Model vs.¬†KDE Baseline\n\n\nCode\n# Create three maps\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Actual Burglaries\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Model Predictions (Neg. Binomial)\") +\n  theme_crime()\n\np3 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"KDE Baseline Predictions\") +\n  theme_crime()\n\np1 + p2 + p3 +\n  plot_annotation(\n    title = \"Actual vs. Predicted Burglaries\",\n    subtitle = \"Does our complex model outperform simple KDE?\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate performance metrics\ncomparison &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %&gt;%\n  summarize(\n    model_mae = mean(abs(countBurglaries - prediction_nb)),\n    model_rmse = sqrt(mean((countBurglaries - prediction_nb)^2)),\n    kde_mae = mean(abs(countBurglaries - prediction_kde)),\n    kde_rmse = sqrt(mean((countBurglaries - prediction_kde)^2))\n  )\n\ncomparison %&gt;%\n  pivot_longer(everything(), names_to = \"metric\", values_to = \"value\") %&gt;%\n  separate(metric, into = c(\"approach\", \"metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = metric, values_from = value) %&gt;%\n  kable(\n    digits = 2,\n    caption = \"Model Performance Comparison\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nModel Performance Comparison\n\n\napproach\nmae\nrmse\n\n\n\n\nmodel\n2.21\n3.27\n\n\nkde\n2.06\n2.95\n\n\n\n\n\nInterpretation 8.1: Does the complex model outperform the simple KDE baseline? By how much? Is the added complexity worth it?\nThe KDE baseline achieves slightly better predictive accuracy (MAE 2.06 vs.¬†2.21), but this does not mean the regression model is unnecessary or ‚Äúnot worth it.‚Äù The two approaches serve fundamentally different purposes.\nKDE is a purely spatial smoother: it captures clustering extremely well because burglary is highly spatially dependent. Its strength lies in interpolating hotspots, not in explaining why those hotspots exist. In contrast, the regression model provides something KDE cannot: insight into the underlying mechanisms that correlate with burglary patterns. These findings are meaningful because they speak to environmental criminology and ‚Äúbroken windows‚Äù dynamics‚Äîrelationships that KDE, by design, cannot reveal.\nSo while KDE edges out the model in raw predictive accuracy, the regression model is still valuable because it helps us understand the social and spatial processes behind crime, not just map it."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-8.3-where-does-the-model-work-well",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-8.3-where-does-the-model-work-well",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 8.3: Where Does the Model Work Well?",
    "text": "Exercise 8.3: Where Does the Model Work Well?\n\n\nCode\n# Calculate errors\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    error_nb = countBurglaries - prediction_nb,\n    error_kde = countBurglaries - prediction_kde,\n    abs_error_nb = abs(error_nb),\n    abs_error_kde = abs(error_kde)\n  )\n\n# Map errors\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#2166ac\", mid = \"white\", high = \"#b2182b\",\n    midpoint = 0,\n    limits = c(-10, 10)\n  ) +\n  labs(title = \"Model Errors (Actual - Predicted)\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Abs. Error\", option = \"magma\") +\n  labs(title = \"Absolute Model Errors\") +\n  theme_crime()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nInterpretation 8.2: Where does the model make the biggest errors? Are there spatial patterns in the errors? What might this reveal?\nThe largest model errors appear in a few concentrated spatial areas, especially in the strong burglary hotspots, and several zones where burglary levels change sharply from one grid cell to the next.\nThe absolute-error map shows that the largest absolute errors are spatially clustered, not randomly scattered. This suggests the model struggles most in places with very high burglary levels, and places with abrupt local variation, where neighboring cells differ strongly.\nThe model‚Äôs predictors capture broad trends but cannot fully account for the highly localized, uneven nature of burglary hotspots, which leads to systematic errors in those areas."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-9.1-model-summary-table",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-9.1-model-summary-table",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 9.1: Model Summary Table",
    "text": "Exercise 9.1: Model Summary Table\n\n\nCode\n# Create nice summary table\nmodel_summary &lt;- broom::tidy(final_model, exponentiate = TRUE) %&gt;%\n  mutate(\n    across(where(is.numeric), ~round(., 3))\n  )\n\nmodel_summary %&gt;%\n  kable(\n    caption = \"Final Negative Binomial Model Coefficients (Exponentiated)\",\n    col.names = c(\"Variable\", \"Rate Ratio\", \"Std. Error\", \"Z\", \"P-Value\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = \"Rate ratios &gt; 1 indicate positive association with burglary counts.\"\n  )\n\n\n\nFinal Negative Binomial Model Coefficients (Exponentiated)\n\n\nVariable\nRate Ratio\nStd. Error\nZ\nP-Value\n\n\n\n\n(Intercept)\n9.658\n0.062\n36.788\n0.000\n\n\nsanitation_coms\n1.003\n0.002\n1.639\n0.101\n\n\nsanitation_coms.nn\n0.995\n0.000\n-18.930\n0.000\n\n\ndist_to_hotspot\n1.000\n0.000\n-9.359\n0.000\n\n\n\nNote: \n\n\n\n\n\n\n Rate ratios &gt; 1 indicate positive association with burglary counts."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-9.2-key-findings-summary",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-9.2-key-findings-summary",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 9.2: Key Findings Summary",
    "text": "Exercise 9.2: Key Findings Summary\nTechnical Performance:\n\nCross-validation MAE: 2.51\nModel vs.¬†KDE: [KDE performs slightly better (MAE 2.21 vs.¬†2.06)]\nMost predictive variable: [Distance to sanitation complaint clusters (sanitation_coms.nn)]\n\nSpatial Patterns:\n\nBurglaries are [clustered]\nHot spots are located in [South Side, the West Side, and parts of the Near North]\nModel errors show [systematic] patterns\n\nModel Limitations:\n\nOverdispersion: [Yes]\nSpatial autocorrelation in residuals: [Haven‚Äôt tested yet]\nCells with zero counts: [781 / 2458 ( 31.8 %)]"
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Load necessary libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE, tigris_class = \"sf\")\nlibrary(MASS)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(nngeo)\nlibrary(car)\nlibrary(knitr)\nlibrary(readr)\nlibrary(patchwork)\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  \n\n\n1.1 Load and clean Philadelphia sales data:\n\n1.1.1 Load data\n\n\n\nCode\nlibrary(tidyverse)\nopa &lt;- read_csv(\"opa_properties_public1.csv\")\n\n\n\n1.1.2 Filter to residential properties, 2023-2024 sales\n\n\n\nCode\n# data in 2022 will be used as predictor, so keep them as well.\nopa_clean &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# record the amount of data we will focus on\nopa_clean2 &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# Select relevant variables \nopa_var &lt;- opa_clean %&gt;%\n  dplyr::select(\n    sale_date, sale_price, market_value, building_code_description,\n    total_livable_area, number_of_bedrooms, number_of_bathrooms,\n    number_stories, garage_spaces, central_air, quality_grade,\n    interior_condition, exterior_condition, year_built, \n    off_street_open, zip_code, census_tract, zoning, owner_1,\n    category_code_description, shape, fireplaces\n  )\n\n\n\n1.1.3 Remove obvious errors & Handle missing values\n\n\n\nCode\n# ! check before remove NA value\n\ncat(\"&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\")\n\n\n&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\n\n\nCode\n#remove errors and drop rows with small NA counts in specific columns\nopa_var &lt;- opa_var %&gt;% \n  distinct() %&gt;%  #Remove duplicate lines\n  filter(\n    !is.na(total_livable_area) & total_livable_area &gt; 0,\n    !is.na(year_built) & year_built &gt; 0 & year_built &lt; 2025,\n    !is.na(number_of_bathrooms),\n    !is.na(fireplaces),\n    !is.na(interior_condition),\n    garage_spaces&lt;30\n  )   \n\n\n\n1.1.4 Other cleaning decisions\n\n\n\nCode\n#numeric quality_grade\nvalid_grades &lt;- c(\"A+\", \"A\", \"A-\", \n                  \"B+\", \"B\", \"B-\", \n                  \"C+\", \"C\", \"C-\", \n                  \"D+\", \"D\", \"D-\", \n                  \"E+\", \"E\", \"E-\")\n\nopa_var &lt;- opa_var %&gt;%\n  filter(quality_grade %in% valid_grades) %&gt;%\n  mutate(\n    quality_grade = factor(quality_grade, levels = valid_grades, ordered = TRUE),\n    quality_grade_num = rev(seq_along(valid_grades))[as.numeric(quality_grade)]\n  )\n\n#central_air (keep and transform the large NA values)\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    central_air_dummy = case_when(\n      central_air %in% c(1, \"Y\", \"y\") ~ 1,\n      central_air %in% c(0, \"N\", \"n\") ~ 0,\n      TRUE ~ NA_real_\n    ),\n    central_air_missing = if_else(is.na(central_air), 1, 0),\n    central_air_dummy = if_else(is.na(central_air_dummy), 0, central_air_dummy)\n  )\n\n#house age\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    house_age = 2025 - year_built\n  )\n\n\n\n1.1.5 Explore structural data biases and identify non-market transactions\n\n\n\nCode\n# check the relation of Sale Price ~ Market Value\noptions(scipen = 999)\nplot(opa_var$sale_price, opa_var$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value (Predicted)  vs  Sale Price (Actual)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# create a new column to record the identified non-market transactions\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    non_market = \n      (((sale_price &lt; 0.10*market_value) | sale_price &lt; 2000) | (sale_price&gt; 5*market_value))\n  )\n\n\nInterpretation: - The goal of this model is to predict typical, market-driven sale prices. The provided scatter plot of Market Value (Predicted) vs.¬†Sale Price (Actual) is an essential diagnostic tool for assessing data quality. - The red line on the plot represents a perfect 1-to-1 relationship (Y=X), where the property‚Äôs actual sale price is exactly equal to its predicted market value. While most data points cluster tightly around this line‚Äîindicating the ‚ÄúMarket Value‚Äù is a strong predictor‚Äîthe plot clearly reveals two distinct types of extreme outliers that do not represent typical market transactions. 1. Removing Non-Market Sales (The Sale Price &lt; 0.05 * Market Value Rule) - There is a dense cluster of points stacked vertically along the y-axis, where Sale Price (X) is near zero, but Market Value (Y) is high (e.g., $1M, $2M, even $4M). These points represent transactions where a property was ‚Äúsold‚Äù for a price that is a tiny fraction of its assessed worth (e.g., a $2,000,000 house sold for $50,000). - These are non-arm‚Äôs-length transactions, not true market sales. Examples include sales between family members, inheritance transfers, or other legal transactions where the price does not reflect the market. 2. Removing Anomalous High Sales (The Sale Price &gt; 5 * Market Value Rule) - There are several data points scattered far to the right and bottom of the plot, far below the red Y=X line. These points represent properties where the Sale Price (X) was massively higher than its Market Value (Y). For example, a property with an assessed value of $500,000 (Y) selling for $4,000,000 (X). - These are also not typical sales. They could represent (a) data entry errors, (b) sales where the ‚ÄúMarket Value‚Äù figure is obsolete (e.g., a run-down property sold for land value/development potential), or (c) properties that underwent major renovations not yet captured in the assessed value. - Retaining these two groups of outliers would severely skew the model‚Äôs coefficients and reduce its predictive accuracy for the vast majority of normal, market-based transactions. This filtering rule is a necessary step to clean the data, ensure the model learns from valid transactions, and improve its overall reliability.\n\n1.1.6 enhance the sales data\n\nWe have 8000+ non market transactions, that is 1/4 of the total sales data! That‚Äôs too big to let go, The model that we want to generate will become much more stable if we can make use of them.\n\n\nCode\n# filter the REAL MARKET data in the time period we need\nopa_selected &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n  )   \n\n#filter the NON MARKET data in the time period we need\nopa_non_market &lt;- opa_var %&gt;%\n  filter(\n    non_market ==1,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n    )\n\nopa_selected2 &lt;- opa_var\nopa_bonus &lt;- opa_var \n\n\n\n\nCode\n#try to find the relationship between market_value and sale_price\noptions(scipen = 999)\nplot(opa_selected$sale_price, opa_selected$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value vs  Sale Price (cleaned)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# That's quite linear, let's try to build a simple OLS model\nopa_mdata &lt;- opa_bonus %&gt;%\n  filter(\n    non_market == 0\n    )\n\nmodel_non &lt;- lm(sale_price ~ market_value, data = opa_mdata)\nsummary(model_non)\n\n\n\nCall:\nlm(formula = sale_price ~ market_value, data = opa_mdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1432832   -35207    -1669    30014  1975714 \n\nCoefficients:\n                 Estimate   Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  -9189.286424   663.771782  -13.84 &lt;0.0000000000000002 ***\nmarket_value     1.027924     0.001773  579.62 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 86310 on 40927 degrees of freedom\nMultiple R-squared:  0.8914,    Adjusted R-squared:  0.8914 \nF-statistic: 3.36e+05 on 1 and 40927 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\npred &lt;- predict(model_non, newdata = opa_mdata)\nresid &lt;- opa_mdata$sale_price - pred\n\n# RMSE\nrmse_value &lt;- sqrt(mean(resid^2, na.rm = TRUE))\nrmse_value\n\n\n[1] 86311.29\n\n\nThe results demonstrate a very strong linear relationship between sale_price and market_value. The market_value variable in the dataset effectively predicts the sale_price. Therefore, we can leverage this relationship to estimate the normal market prices for non-market transactions. We record these estimated values as sale_price_predicted. By doing this, we enhance our data!\n\n\nCode\n#bring data back\nopa_non_market$sale_price_predicted &lt;- predict(model_non, newdata = opa_non_market)\n\n#join back to the main data\nopa_selected &lt;- opa_selected %&gt;%\n  mutate(sale_price_predicted= sale_price)\n\nset.seed(123)\nopa_bind &lt;- bind_rows(opa_selected, opa_non_market) %&gt;%\n  slice_sample(prop = 1)\n\n\n1.2 Load and clean secondary dataset:\n\n1.2.1 Census data (tidycensus):\n\n\n\nCode\n# Transform to sf object \nopa_bind &lt;- st_as_sf(opa_bind, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(4326)\nopa_sf&lt;- opa_bind\n\n\n\n\nCode\n# Load Census data for Philadelphia tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    \n    ba_degree = \"B15003_022\",\n    total_edu = \"B15003_001\",\n    \n    median_income = \"B19013_001\",\n   \n    labor_force = \"B23025_003\",\n    unemployed = \"B23025_005\",\n    \n    total_housing = \"B25002_001\",\n    vacant_housing = \"B25002_003\"\n  ),\n  year = 2023,\n  state = \"PA\",\n  county = \"Philadelphia\",\n  geometry = TRUE\n) %&gt;%\n  dplyr::select(GEOID, variable, estimate, geometry) %&gt;%   \n  tidyr::pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  dplyr::mutate(\n    ba_rate = 100 * ba_degree / total_edu,\n    unemployment_rate = 100 * unemployed / labor_force,\n    vacancy_rate = 100 * vacant_housing / total_housing\n  ) %&gt;%\n  st_transform(st_crs(opa_sf))\n\n# Spatial join of OPA data with Census data\nopa_census &lt;- st_join(opa_sf, philly_census, join = st_within) %&gt;%\n  filter(!is.na(median_income))\n\n\n\n1.2.2 Spatial amenities (OpenDataPhilly)\n\n\n\nCode\n#load crime,poi,transit,hospital\nopa_census &lt;- st_transform(opa_census, 3857)\n\ncrime &lt;- read_csv(\"crime_sel.csv\") %&gt;% \n  filter(!is.na(lat) & !is.na(lng)) \ncrime_sf &lt;- st_as_sf(crime, coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census)) \n\npoi_sf &lt;- st_read(\"data/gis_osm_pois_a_free_1.shp\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census)) \n\nTransit &lt;- read_csv(\"Transit.csv\")\ntransit_sf &lt;- st_as_sf(Transit, coords = c(\"Lon\", \"Lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census))\n\nhospital_sf &lt;- st_read(\"hospitals.geojson\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census))\n\n\n1.3 Summary tables showing before/after dimensions\n\n\nCode\n# Original data dimensions\nopa_dims &lt;- tibble(\n  dataset = \"raw CSV\",\n  rows = nrow(opa),\n  columns = ncol(opa)\n)\n\n# cleaned data dimensions\nopa_filter_dims &lt;- tibble(\n  dataset = \"after fixed criteria\",\n  rows = nrow(opa_clean2),\n  columns = ncol(opa_clean2)\n)\n\nopa_selected_dims &lt;- tibble(\n  dataset = \"after cleaned\",\n  rows = nrow(opa_bind),\n  columns = ncol(opa_bind)\n)\n# data dimensions (within census tracts)\nopa_census_dims &lt;- tibble(\n  dataset = \"after census joined\",\n  rows = nrow(opa_census),\n  columns = ncol(opa_census)\n)\n\n# create summary table\nsummary_table &lt;- bind_rows(opa_dims, opa_filter_dims,opa_selected_dims, opa_census_dims)\nlibrary(knitr)\nsummary_table %&gt;%\n  kable(caption = \"Summary of OPA data before and after cleaning\")\n\n\n\nSummary of OPA data before and after cleaning\n\n\ndataset\nrows\ncolumns\n\n\n\n\nraw CSV\n153267\n79\n\n\nafter fixed criteria\n34559\n79\n\n\nafter cleaned\n31968\n28\n\n\nafter census joined\n31613\n40\n\n\n\n\n\nPrinciples of Data Processing\n\nopa Data\n\nFilter sale_date between 2023-01-01 and 2024-12-31.\n\nKeep only residential properties (category_code == 1).\n\nRemove records with missing values in total_livable_area, sale_price, or number_of_bathrooms.\n\nFilter records year_built &gt; 0 .\n\nCensus data\n\nLoad data including total_pop, ba_degree, total_edu, median_income, labor_force, unemployed, total_housing, vacant_housing.\nTransform to spatial format and remove records with missing values.\n\nSpatial amenities\n\nLoad datasets Transit, crime, POIs, Hospitals.\nTransform to spatial format and remove records with missing values.\n\n\nInterpretation: The selected variables can be grouped into several meaningful categories that are theoretically and empirically linked to housing prices:\n- Neighborhood Safety: - Crime data: Areas with lower crime rates are generally more desirable, leading to higher property values. Including crime incidents helps capture the effect of public safety on housing prices. - Accessibility and Transportation: - Transit points: Proximity to public transportation (e.g., bus stops, subway stations) increases accessibility and convenience, which is often capitalized into higher home values. - Points of Interest (POIs): Nearby amenities such as shops, restaurants, and parks improve quality of life and can positively influence housing prices. - Healthcare Access: - Hospitals: Easy access to healthcare facilities is a valued neighborhood characteristic, especially for families and older residents, and can contribute to higher property values. - Socioeconomic and Demographic Context (from Census data): - Total population: Indicates population density, which can affect demand for housing. - Educational attainment (e.g., % with BA degree): Higher education levels are often correlated with higher income and neighborhood desirability. - Median income: Directly influences purchasing power and demand for housing in an area. - Employment status (labor force and unemployment): Reflects economic stability and local job market health, which affect housing demand. - Housing market conditions (total and vacant housing): Vacancy rates can signal neighborhood decline or oversupply, both of which impact prices. - Together, these variables provide a multidimensional view of a neighborhood‚Äôs appeal, safety, convenience, and economic health‚Äîall key determinants of housing prices."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-1-data-preparation",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-1-data-preparation",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Load necessary libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE, tigris_class = \"sf\")\nlibrary(MASS)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(nngeo)\nlibrary(car)\nlibrary(knitr)\nlibrary(readr)\nlibrary(patchwork)\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  \n\n\n1.1 Load and clean Philadelphia sales data:\n\n1.1.1 Load data\n\n\n\nCode\nlibrary(tidyverse)\nopa &lt;- read_csv(\"opa_properties_public1.csv\")\n\n\n\n1.1.2 Filter to residential properties, 2023-2024 sales\n\n\n\nCode\n# data in 2022 will be used as predictor, so keep them as well.\nopa_clean &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# record the amount of data we will focus on\nopa_clean2 &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# Select relevant variables \nopa_var &lt;- opa_clean %&gt;%\n  dplyr::select(\n    sale_date, sale_price, market_value, building_code_description,\n    total_livable_area, number_of_bedrooms, number_of_bathrooms,\n    number_stories, garage_spaces, central_air, quality_grade,\n    interior_condition, exterior_condition, year_built, \n    off_street_open, zip_code, census_tract, zoning, owner_1,\n    category_code_description, shape, fireplaces\n  )\n\n\n\n1.1.3 Remove obvious errors & Handle missing values\n\n\n\nCode\n# ! check before remove NA value\n\ncat(\"&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\")\n\n\n&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\n\n\nCode\n#remove errors and drop rows with small NA counts in specific columns\nopa_var &lt;- opa_var %&gt;% \n  distinct() %&gt;%  #Remove duplicate lines\n  filter(\n    !is.na(total_livable_area) & total_livable_area &gt; 0,\n    !is.na(year_built) & year_built &gt; 0 & year_built &lt; 2025,\n    !is.na(number_of_bathrooms),\n    !is.na(fireplaces),\n    !is.na(interior_condition),\n    garage_spaces&lt;30\n  )   \n\n\n\n1.1.4 Other cleaning decisions\n\n\n\nCode\n#numeric quality_grade\nvalid_grades &lt;- c(\"A+\", \"A\", \"A-\", \n                  \"B+\", \"B\", \"B-\", \n                  \"C+\", \"C\", \"C-\", \n                  \"D+\", \"D\", \"D-\", \n                  \"E+\", \"E\", \"E-\")\n\nopa_var &lt;- opa_var %&gt;%\n  filter(quality_grade %in% valid_grades) %&gt;%\n  mutate(\n    quality_grade = factor(quality_grade, levels = valid_grades, ordered = TRUE),\n    quality_grade_num = rev(seq_along(valid_grades))[as.numeric(quality_grade)]\n  )\n\n#central_air (keep and transform the large NA values)\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    central_air_dummy = case_when(\n      central_air %in% c(1, \"Y\", \"y\") ~ 1,\n      central_air %in% c(0, \"N\", \"n\") ~ 0,\n      TRUE ~ NA_real_\n    ),\n    central_air_missing = if_else(is.na(central_air), 1, 0),\n    central_air_dummy = if_else(is.na(central_air_dummy), 0, central_air_dummy)\n  )\n\n#house age\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    house_age = 2025 - year_built\n  )\n\n\n\n1.1.5 Explore structural data biases and identify non-market transactions\n\n\n\nCode\n# check the relation of Sale Price ~ Market Value\noptions(scipen = 999)\nplot(opa_var$sale_price, opa_var$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value (Predicted)  vs  Sale Price (Actual)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# create a new column to record the identified non-market transactions\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    non_market = \n      (((sale_price &lt; 0.10*market_value) | sale_price &lt; 2000) | (sale_price&gt; 5*market_value))\n  )\n\n\nInterpretation: - The goal of this model is to predict typical, market-driven sale prices. The provided scatter plot of Market Value (Predicted) vs.¬†Sale Price (Actual) is an essential diagnostic tool for assessing data quality. - The red line on the plot represents a perfect 1-to-1 relationship (Y=X), where the property‚Äôs actual sale price is exactly equal to its predicted market value. While most data points cluster tightly around this line‚Äîindicating the ‚ÄúMarket Value‚Äù is a strong predictor‚Äîthe plot clearly reveals two distinct types of extreme outliers that do not represent typical market transactions. 1. Removing Non-Market Sales (The Sale Price &lt; 0.05 * Market Value Rule) - There is a dense cluster of points stacked vertically along the y-axis, where Sale Price (X) is near zero, but Market Value (Y) is high (e.g., $1M, $2M, even $4M). These points represent transactions where a property was ‚Äúsold‚Äù for a price that is a tiny fraction of its assessed worth (e.g., a $2,000,000 house sold for $50,000). - These are non-arm‚Äôs-length transactions, not true market sales. Examples include sales between family members, inheritance transfers, or other legal transactions where the price does not reflect the market. 2. Removing Anomalous High Sales (The Sale Price &gt; 5 * Market Value Rule) - There are several data points scattered far to the right and bottom of the plot, far below the red Y=X line. These points represent properties where the Sale Price (X) was massively higher than its Market Value (Y). For example, a property with an assessed value of $500,000 (Y) selling for $4,000,000 (X). - These are also not typical sales. They could represent (a) data entry errors, (b) sales where the ‚ÄúMarket Value‚Äù figure is obsolete (e.g., a run-down property sold for land value/development potential), or (c) properties that underwent major renovations not yet captured in the assessed value. - Retaining these two groups of outliers would severely skew the model‚Äôs coefficients and reduce its predictive accuracy for the vast majority of normal, market-based transactions. This filtering rule is a necessary step to clean the data, ensure the model learns from valid transactions, and improve its overall reliability.\n\n1.1.6 enhance the sales data\n\nWe have 8000+ non market transactions, that is 1/4 of the total sales data! That‚Äôs too big to let go, The model that we want to generate will become much more stable if we can make use of them.\n\n\nCode\n# filter the REAL MARKET data in the time period we need\nopa_selected &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n  )   \n\n#filter the NON MARKET data in the time period we need\nopa_non_market &lt;- opa_var %&gt;%\n  filter(\n    non_market ==1,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n    )\n\nopa_selected2 &lt;- opa_var\nopa_bonus &lt;- opa_var \n\n\n\n\nCode\n#try to find the relationship between market_value and sale_price\noptions(scipen = 999)\nplot(opa_selected$sale_price, opa_selected$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value vs  Sale Price (cleaned)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# That's quite linear, let's try to build a simple OLS model\nopa_mdata &lt;- opa_bonus %&gt;%\n  filter(\n    non_market == 0\n    )\n\nmodel_non &lt;- lm(sale_price ~ market_value, data = opa_mdata)\nsummary(model_non)\n\n\n\nCall:\nlm(formula = sale_price ~ market_value, data = opa_mdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1432832   -35207    -1669    30014  1975714 \n\nCoefficients:\n                 Estimate   Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  -9189.286424   663.771782  -13.84 &lt;0.0000000000000002 ***\nmarket_value     1.027924     0.001773  579.62 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 86310 on 40927 degrees of freedom\nMultiple R-squared:  0.8914,    Adjusted R-squared:  0.8914 \nF-statistic: 3.36e+05 on 1 and 40927 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\npred &lt;- predict(model_non, newdata = opa_mdata)\nresid &lt;- opa_mdata$sale_price - pred\n\n# RMSE\nrmse_value &lt;- sqrt(mean(resid^2, na.rm = TRUE))\nrmse_value\n\n\n[1] 86311.29\n\n\nThe results demonstrate a very strong linear relationship between sale_price and market_value. The market_value variable in the dataset effectively predicts the sale_price. Therefore, we can leverage this relationship to estimate the normal market prices for non-market transactions. We record these estimated values as sale_price_predicted. By doing this, we enhance our data!\n\n\nCode\n#bring data back\nopa_non_market$sale_price_predicted &lt;- predict(model_non, newdata = opa_non_market)\n\n#join back to the main data\nopa_selected &lt;- opa_selected %&gt;%\n  mutate(sale_price_predicted= sale_price)\n\nset.seed(123)\nopa_bind &lt;- bind_rows(opa_selected, opa_non_market) %&gt;%\n  slice_sample(prop = 1)\n\n\n1.2 Load and clean secondary dataset:\n\n1.2.1 Census data (tidycensus):\n\n\n\nCode\n# Transform to sf object \nopa_bind &lt;- st_as_sf(opa_bind, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(4326)\nopa_sf&lt;- opa_bind\n\n\n\n\nCode\n# Load Census data for Philadelphia tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    \n    ba_degree = \"B15003_022\",\n    total_edu = \"B15003_001\",\n    \n    median_income = \"B19013_001\",\n   \n    labor_force = \"B23025_003\",\n    unemployed = \"B23025_005\",\n    \n    total_housing = \"B25002_001\",\n    vacant_housing = \"B25002_003\"\n  ),\n  year = 2023,\n  state = \"PA\",\n  county = \"Philadelphia\",\n  geometry = TRUE\n) %&gt;%\n  dplyr::select(GEOID, variable, estimate, geometry) %&gt;%   \n  tidyr::pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  dplyr::mutate(\n    ba_rate = 100 * ba_degree / total_edu,\n    unemployment_rate = 100 * unemployed / labor_force,\n    vacancy_rate = 100 * vacant_housing / total_housing\n  ) %&gt;%\n  st_transform(st_crs(opa_sf))\n\n# Spatial join of OPA data with Census data\nopa_census &lt;- st_join(opa_sf, philly_census, join = st_within) %&gt;%\n  filter(!is.na(median_income))\n\n\n\n1.2.2 Spatial amenities (OpenDataPhilly)\n\n\n\nCode\n#load crime,poi,transit,hospital\nopa_census &lt;- st_transform(opa_census, 3857)\n\ncrime &lt;- read_csv(\"crime_sel.csv\") %&gt;% \n  filter(!is.na(lat) & !is.na(lng)) \ncrime_sf &lt;- st_as_sf(crime, coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census)) \n\npoi_sf &lt;- st_read(\"data/gis_osm_pois_a_free_1.shp\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census)) \n\nTransit &lt;- read_csv(\"Transit.csv\")\ntransit_sf &lt;- st_as_sf(Transit, coords = c(\"Lon\", \"Lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census))\n\nhospital_sf &lt;- st_read(\"hospitals.geojson\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census))\n\n\n1.3 Summary tables showing before/after dimensions\n\n\nCode\n# Original data dimensions\nopa_dims &lt;- tibble(\n  dataset = \"raw CSV\",\n  rows = nrow(opa),\n  columns = ncol(opa)\n)\n\n# cleaned data dimensions\nopa_filter_dims &lt;- tibble(\n  dataset = \"after fixed criteria\",\n  rows = nrow(opa_clean2),\n  columns = ncol(opa_clean2)\n)\n\nopa_selected_dims &lt;- tibble(\n  dataset = \"after cleaned\",\n  rows = nrow(opa_bind),\n  columns = ncol(opa_bind)\n)\n# data dimensions (within census tracts)\nopa_census_dims &lt;- tibble(\n  dataset = \"after census joined\",\n  rows = nrow(opa_census),\n  columns = ncol(opa_census)\n)\n\n# create summary table\nsummary_table &lt;- bind_rows(opa_dims, opa_filter_dims,opa_selected_dims, opa_census_dims)\nlibrary(knitr)\nsummary_table %&gt;%\n  kable(caption = \"Summary of OPA data before and after cleaning\")\n\n\n\nSummary of OPA data before and after cleaning\n\n\ndataset\nrows\ncolumns\n\n\n\n\nraw CSV\n153267\n79\n\n\nafter fixed criteria\n34559\n79\n\n\nafter cleaned\n31968\n28\n\n\nafter census joined\n31613\n40\n\n\n\n\n\nPrinciples of Data Processing\n\nopa Data\n\nFilter sale_date between 2023-01-01 and 2024-12-31.\n\nKeep only residential properties (category_code == 1).\n\nRemove records with missing values in total_livable_area, sale_price, or number_of_bathrooms.\n\nFilter records year_built &gt; 0 .\n\nCensus data\n\nLoad data including total_pop, ba_degree, total_edu, median_income, labor_force, unemployed, total_housing, vacant_housing.\nTransform to spatial format and remove records with missing values.\n\nSpatial amenities\n\nLoad datasets Transit, crime, POIs, Hospitals.\nTransform to spatial format and remove records with missing values.\n\n\nInterpretation: The selected variables can be grouped into several meaningful categories that are theoretically and empirically linked to housing prices:\n- Neighborhood Safety: - Crime data: Areas with lower crime rates are generally more desirable, leading to higher property values. Including crime incidents helps capture the effect of public safety on housing prices. - Accessibility and Transportation: - Transit points: Proximity to public transportation (e.g., bus stops, subway stations) increases accessibility and convenience, which is often capitalized into higher home values. - Points of Interest (POIs): Nearby amenities such as shops, restaurants, and parks improve quality of life and can positively influence housing prices. - Healthcare Access: - Hospitals: Easy access to healthcare facilities is a valued neighborhood characteristic, especially for families and older residents, and can contribute to higher property values. - Socioeconomic and Demographic Context (from Census data): - Total population: Indicates population density, which can affect demand for housing. - Educational attainment (e.g., % with BA degree): Higher education levels are often correlated with higher income and neighborhood desirability. - Median income: Directly influences purchasing power and demand for housing in an area. - Employment status (labor force and unemployment): Reflects economic stability and local job market health, which affect housing demand. - Housing market conditions (total and vacant housing): Vacancy rates can signal neighborhood decline or oversupply, both of which impact prices. - Together, these variables provide a multidimensional view of a neighborhood‚Äôs appeal, safety, convenience, and economic health‚Äîall key determinants of housing prices."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-2-feature-engineering",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-2-feature-engineering",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 2: Feature Engineering",
    "text": "Phase 2: Feature Engineering\n2.1 Buffer-based features:\n\n2.1.1 neighborhood avg sale price in the past year\n\n\n\nCode\n#filter the past sales data\n\nopa_census &lt;- opa_census %&gt;%\n  mutate(sale_id = row_number())\n\n\n\nopa_past &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2022-12-31\"\n  ) \n\nopa_past &lt;- opa_past %&gt;%\n  mutate(sale_id2 = row_number())\n\nopa_past &lt;- st_as_sf(opa_past, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(3857)\n\nopa_census &lt;- st_transform(opa_census, 3857)\nopa_past   &lt;- st_transform(opa_past, 3857)\n\nopa_census_buffer &lt;- st_buffer(opa_census, 300)\ndrop_cols &lt;- names(opa_census_buffer) %in% c(\"sale_price\", \"total_livable_area\",\n                                             \"sale_price.y\", \"total_livable_area.y\")\nopa_census_buffer &lt;- opa_census_buffer[ , !drop_cols, drop = FALSE]\n\njoin_result &lt;- st_join(\n  opa_census_buffer,\n  opa_past,\n  join = st_intersects,\n  left = TRUE\n)\n\n\njoin_result &lt;- st_join(\n  opa_census_buffer,\n  opa_past,\n  join = st_intersects,\n  left = TRUE\n)\njoin_dedup &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  distinct(sale_id, sale_id2, .keep_all = TRUE)\n\nopa_summary &lt;- join_dedup %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(\n    past_count = sum(!is.na(sale_id2)),\n    avg_past_price_density = ifelse(\n      sum(!is.na(total_livable_area)) == 0, NA_real_,\n      sum(sale_price, na.rm = TRUE) / sum(total_livable_area, na.rm = TRUE)\n    ),\n    .groups = \"drop\"\n  )\nopa_census &lt;- opa_census %&gt;%\n  left_join(opa_summary, by = \"sale_id\")\n\n\n\n2.1.2 crime numbers\n\n\n\nCode\nradius_cri &lt;- 250 \n\nopa_census$crime_count &lt;- lengths(st_is_within_distance(opa_census, crime_sf, dist = radius_cri)) \n\n\n\n2.1.3 POI numbers\n\n\n\nCode\nopa_census_m &lt;- st_transform(opa_census, 3857)\npoi_sf_m     &lt;- st_transform(poi_sf, 3857)\n\n\nradius_poi &lt;- 400\nopa_census_buffer &lt;- st_buffer(opa_census_m, radius_poi)\n\njoin_result &lt;- st_join(opa_census_buffer, poi_sf_m, join = st_intersects, left = TRUE)\n\npoi_summary &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(poi_count = sum(!is.na(osm_id)))  \n\nopa_census &lt;- opa_census_m %&gt;%\n  left_join(poi_summary, by = \"sale_id\")\n\n\n\n2.1.4 Transit numbers\n\n\n\nCode\nopa_census_m &lt;- st_transform(opa_census, 3857)\ntransit_sf_m &lt;- st_transform(transit_sf, 3857)\n\nradius_ts &lt;- 400\nopa_census_buffer &lt;- st_buffer(opa_census_m, radius_ts)\n\njoin_result &lt;- st_join(opa_census_buffer, transit_sf_m, join = st_intersects, left = TRUE)\n\ntransit_summary &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(transit_count = sum(!is.na(FID)))  \n\nopa_census &lt;- opa_census_m %&gt;%\n  left_join(transit_summary, by = \"sale_id\")\n\n\n2.2 k-Nearest Neighbor features:\n\n2.2.1 Hospitals (KNN-3)\n\n\n\nCode\nnearest_hospital_index &lt;- st_nn(\n  opa_census,\n  hospital_sf,\n  k = 3,            \n  returnDist = TRUE \n)\n\nopa_census$nearest_hospital_knn3 &lt;- sapply(nearest_hospital_index$dist, mean)\n\n\n2.3 Weights:\n\n\nCode\n# add different weights to actual and non market transactions\n\nopa_census_all &lt;- opa_census\n\nnon_market_share&lt;- mean(opa_census_all$non_market == 1, na.rm = TRUE)\nnon_market_share\n\n\n[1] 0.261791\n\n\nCode\nopa_census_all &lt;- opa_census %&gt;%\n  mutate(weight_mix = ifelse(non_market == 1, non_market_share, 1))\n\n\n2.4 Transformation:\n\n\nCode\n#standardize houseage and median income\nmean_age_all &lt;- mean(opa_census_all$house_age, na.rm = TRUE)\nmean_income_log &lt;- mean(log(opa_census_all$median_income), na.rm = TRUE)\n\n# centralize\nopa_census_all &lt;- opa_census_all %&gt;%\n  mutate(\n    house_age_c  = house_age - mean_age_all,\n    house_age_c2 = house_age_c^2,\n    income_log   = log(median_income),\n    income_scaled = income_log - mean_income_log\n  )\n\nopa_census_2&lt;- opa_census_all %&gt;%\n  filter(\n    non_market==0\n  )"
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-3-exploratory-data-analysis",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-3-exploratory-data-analysis",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 3: Exploratory Data Analysis",
    "text": "Phase 3: Exploratory Data Analysis\n\n3.1 Distribution of sale prices (histogram)\n\n\nCode\nggplot(opa_census_all, aes(x = sale_price_predicted)) +\n  geom_histogram(\n    bins = 50,                 # Ë∞ÉÊï¥ÂàÜÁÆ±Êï∞Èáè\n    fill = \"#6A1B9A\",          # Êü±Â≠êÈ¢úËâ≤\n    color = \"white\",           # ËæπÊ°ÜÈ¢úËâ≤\n    alpha = 0.8                # ÈÄèÊòéÂ∫¶\n  ) +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Distribution of  Sale Prices\",\n    x = \"Predicted Sale Price (USD)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(opa_census_all, aes(x = log(sale_price_predicted))) +\n  geom_histogram(\n    bins = 50,                 # Ë∞ÉÊï¥ÂàÜÁÆ±Êï∞Èáè\n    fill = \"#6A1B9A\",          # Êü±Â≠êÈ¢úËâ≤\n    color = \"white\",           # ËæπÊ°ÜÈ¢úËâ≤\n    alpha = 0.8                # ÈÄèÊòéÂ∫¶\n  ) +\n  scale_x_continuous() +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Distribution of  Log(Sale Prices)\",\n    x = \"Log(Predicted Sale Price) \",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\nKey Findings: - Highly Right-Skewed: The bulk of the distribution is concentrated on the left side (low price range), while the right tail is very long, extending towards higher prices. This means that the majority of houses have lower prices, and very expensive houses are very few in number.\n- Concentration and Outliers: The count of houses in each bin drops rapidly as the price increases. The number of houses above $2,500,000 is very small, indicating the presence of extreme high-price outliers.\n- Preprocessing Requirement: This distribution strongly suggests that before building a house price prediction model, the Sale Price variable will need a transformation, most commonly a log transformation, to make the distribution more closely resemble a normal distribution.\n\n\n3.2 Spatial distribution of sale prices (map)\n\n\nCode\nopa_census_all &lt;- opa_census_all %&gt;%\n  mutate(price_quartile = ntile(sale_price_predicted, 4))\n\nggplot() +\n  geom_sf(data = philly_census, fill = \"lightgrey\", color = \"white\") +\n  geom_sf(data = opa_census_all, aes(color = factor(price_quartile)), size = 0.5, alpha = 0.7) +\n  scale_color_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    labels = c(\"0%-25%\", \"25%-50%\", \"50%-75%\", \"75%-100%\")\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Housing Sales Price in Philadelphia (2023‚Äì2024)\",\n    color = \"Sale Price Quartile\"\n  )  +\n  guides(color = guide_legend(override.aes = list(size = 3)))\n\n\n\n\n\n\n\n\n\nKey Findings:\n- Spatial Concentration of Housing Prices: Highest Prices are clearly concentrated in the Center City/Downtown area of Philadelphia and its immediate surroundings, which indicates that the most expensive property transactions occur in the high-value areas of and around the city center.\n- Discontinuous Price Transitions: Housing prices do not exhibit smooth gradients across the city; instead, they show abrupt changes between different price quartiles, forming distinct spatial clusters. This suggests that price variations are influenced by fixed boundaries such as neighborhoods, infrastructure, or socio-economic factors, rather than continuous spatial diffusion. - Peripheral Price Patterns: Lower-price quartiles (0%-25% and 25%-50%) are predominantly located in the outer regions of the city, particularly in the northern and southern peripheries, indicating a clear core-periphery divide in housing values.\n\n\n3.3 Price vs.¬†structural features (scatter plots)\n\n\nCode\nopa_census_plot &lt;- opa_census_all %&gt;%\n  mutate(log_sale_price = log(sale_price_predicted))\n\n# 1Ô∏è‚É£ total_livable_area\np1 &lt;- ggplot(opa_census_plot, aes(x = log(total_livable_area), y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Log(Livable Area)\",\n    x = \"Log(Total Livable Area)\",\n    y = \"Log(Sale Price)\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 2Ô∏è‚É£ number_of_bathrooms\np2 &lt;- ggplot(opa_census_plot, aes(x = number_of_bathrooms, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Bathrooms\",\n    x = \"Number of Bathrooms\",  \n    y = \"\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 3Ô∏è‚É£ interior_condition\np3 &lt;- ggplot(opa_census_plot, aes(x = interior_condition, y = log_sale_price)) +\n  geom_jitter(alpha = 0.5, color = \"#6A1B9A\", width = 0.2, height = 0) +  \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Interior Condition\",\n    x = \"Interior Condition\",\n    y = \"Log(Sale Price)\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 4Ô∏è‚É£ house_age\np4 &lt;- ggplot(opa_census_plot, aes(x = house_age^2, y = log_sale_price)) + \n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. House Age¬≤\",\n    x = \"House Age¬≤ (2025 - Year Built)¬≤\",\n    y = \"\"\n  ) +\n  theme_minimal(base_size = 10)\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\nquarto check Key Findings:\n- Strong Positive Correlation with Size and Bathrooms: There is a clear positive linear relationship between the log of sale price and both the log of total livable area and the number of bathrooms. This indicates that larger properties and those with more bathrooms command significantly higher market prices. - Non-Linear Relationship with Interior Condition: While a general positive trend exists, the relationship between log sale price and interior condition rating is not perfectly linear. The data dispersion suggests that the effect of interior condition on price may be subject to diminishing marginal returns or other non-linear dynamics. - Negative Correlation with House Age Squared: A significant negative relationship is observed between log sale price and the squared term of house age. This indicates that property values depreciate as homes get older, and this depreciation effect may accelerate over time, reflecting a non-linear aging effect on housing value.\n\n\n3.4 Price vs.¬†spatial & Social features (scatter plots)\n\n\nCode\nopa_census_plot &lt;- opa_census_all %&gt;%\n  mutate(\n    log_sale_price = log(sale_price_predicted),\n    sqrt_crime_count = sqrt(crime_count)\n  )\n\np1 &lt;- ggplot(opa_census_plot, aes(x = ba_rate, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  labs(title = \"Log(Sale Price) vs. BA Rate\",\n       x = \"Bachelor's Degree Rate\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np2 &lt;- ggplot(opa_census_plot, aes(x = unemployment_rate, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  labs(title = \"Log(Sale Price) vs. Unemployment Rate\",\n       x = \"Unemployment Rate\", y = \"\") +\n  theme_minimal(base_size = 10)\n\np3 &lt;- ggplot(opa_census_plot, aes(x = median_income, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = dollar_format()) +\n  labs(title = \"Log(Sale Price) vs. Median Income\",\n       x = \"Median Household Income (USD)\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np4 &lt;- ggplot(opa_census_plot, aes(x = avg_past_price_density, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. Past Price Density\",\n       x = \"Average Past Price Density\", y = \"\") +\n  theme_minimal(base_size = 10)\n\np5 &lt;- ggplot(opa_census_plot, aes(x = sqrt_crime_count, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. ‚àö(Crime Count)\",\n       x = \"Square Root of Crime Count\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np6 &lt;- ggplot(opa_census_plot, aes(x = transit_count, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. Transit Count\",\n       x = \"Number of Transit Stops Nearby\", y = \"\") +\n  theme_minimal(base_size = 10)\n\n(p1 | p2 | p3) / (p4 | p5 | p6)\n\n\n\n\n\n\n\n\n\nKey Findings:\n- Strong Socioeconomic Influence: Housing prices show clear positive correlations with key socioeconomic indicators. Both bachelor‚Äôs degree rate and median household income exhibit strong positive relationships with sale prices, indicating that neighborhoods with higher educational attainment and income levels command substantially higher property values. - Negative Impact of Crime and Unemployment: There are evident negative relationships between housing prices and both crime levels (measured by square root of crime count) and unemployment rates. This demonstrates that public safety and local economic vitality are significant determinants of property values in Philadelphia. - Positive Effects of Historical Prices and Transit Access: Sale prices maintain a positive relationship with both historical price density and accessibility to public transportation. This suggests that areas with established high-value characteristics and better transit infrastructure maintain their premium in the housing market, reflecting path dependence in neighborhood valuation and the value of transportation accessibility.\n\n\nOther visualization\n\n\nCode\ntract_price &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(avg_past_price_density = mean(avg_past_price_density, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_price, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = avg_past_price_density), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"white\", high = \"#6A1B9A\",\n    name = \"Mean Sale Price Per sqft\",\n    labels = scales::dollar_format(),\n    na.value = \"grey60\"\n  ) +\n  scale_alpha(range = c(0.2, 0.8), name = \"Interior Condition\") +\n  \n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Buffered Area Mean Sold Price of 2022\",\n    subtitle = \"clustered in census tracts\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ntract_condition &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_interior_condition = mean(interior_condition, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_condition, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_interior_condition), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"#6A1B9A\", high = \"white\",      # ÊµÖÊ©ô‚ÜíÊ∑±Á∫¢ÔºåÊõ¥Áõ¥ËßÇË°®Áé∞È´ò‰Ωé\n    name = \"Avg Interior Condition\",\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Average Interior Condition by Census Tract\",\n    subtitle = \"Darker color = better average condition\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ntract_area &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_total_livable_area = mean(total_livable_area, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_area, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_total_livable_area), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"white\", high = \"#6A1B9A\",   \n    name = \"Avg Livable Area (sqft)\",\n    labels = scales::comma_format(),\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Average Livable Area by Census Tract\",\n    subtitle = \"Darker color indicates larger average livable area\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\nKey Findings:\n- Spatial Variation in Property Values: The average sale price per square foot shows significant geographic clustering across census tracts, with distinct high-value areas concentrated in specific neighborhoods. This indicates strong spatial autocorrelation in housing prices, where adjacent tracts tend to have similar price levels. - Correlation Between Property Condition and Location: Better average interior conditions are systematically concentrated in particular geographic areas, suggesting that housing maintenance and quality are not randomly distributed but follow spatial patterns that may correlate with neighborhood characteristics and property values. - Heterogeneous Distribution of Housing Size: The average livable area varies substantially across census tracts, with larger properties clustered in specific regions. This spatial patterning of housing size complements the price distribution, indicating that both property characteristics and location factors contribute to the overall housing market structure in Philadelphia."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-4-model-building",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-4-model-building",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 4: Model Building",
    "text": "Phase 4: Model Building\n\nBuild models progressively\n4.1 Structural features only:\n\n\nCode\nmodel_1 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +  #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing, \n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_1)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing, data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-3.2930 -0.2144  0.0510  0.2911  2.4585 \n\nCoefficients:\n                            Estimate   Std. Error t value            Pr(&gt;|t|)\n(Intercept)              6.435857858  0.077629460  82.905 &lt;0.0000000000000002\nlog(total_livable_area)  0.744348571  0.010689344  69.635 &lt;0.0000000000000002\nnumber_of_bathrooms      0.051667122  0.005580674   9.258 &lt;0.0000000000000002\nhouse_age_c              0.000002273  0.000118374   0.019               0.985\nhouse_age_c2             0.000046457  0.000001746  26.607 &lt;0.0000000000000002\ninterior_condition      -0.112636239  0.004358818 -25.841 &lt;0.0000000000000002\nquality_grade_num        0.069613279  0.002849720  24.428 &lt;0.0000000000000002\nfireplaces               0.116405937  0.010884143  10.695 &lt;0.0000000000000002\ngarage_spaces            0.140429585  0.006549989  21.440 &lt;0.0000000000000002\ncentral_air_dummy        0.458743112  0.008036173  57.085 &lt;0.0000000000000002\ncentral_air_missing     -0.237637667  0.008809496 -26.975 &lt;0.0000000000000002\n                           \n(Intercept)             ***\nlog(total_livable_area) ***\nnumber_of_bathrooms     ***\nhouse_age_c                \nhouse_age_c2            ***\ninterior_condition      ***\nquality_grade_num       ***\nfireplaces              ***\ngarage_spaces           ***\ncentral_air_dummy       ***\ncentral_air_missing     ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4911 on 31602 degrees of freedom\nMultiple R-squared:  0.5212,    Adjusted R-squared:  0.521 \nF-statistic:  3439 on 10 and 31602 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation:\n- log(total_livable_area) (0.752): An elasticity coefficient. A 1% increase in livable area is associated with a 0.752% increase in price. This is a strong positive driver. - number_of_bathrooms (0.046): Each additional bathroom is associated with a 4.6% increase in price. - house_age_c (-0.00001): The linear term for house age is statistically insignificant (p=0.929). - house_age_c2 (0.000047): The squared term for age is positive and significant. Combined with the insignificant linear term, this suggests a slight U-shaped relationship, where new homes and very old homes (perhaps with historical value) command a premium over middle-aged homes. - interior_condition (-0.114): Assuming a higher value means worse condition, each one-unit worsening in condition is associated with an 11.4% decrease in price. - quality_grade_num (0.070): Each one-unit increase in the quality grade is associated with a 7.0% increase in price. - fireplaces (0.117): Each additional fireplace is associated with a 11.7% increase in price. - garage_spaces (0.143): Each additional garage space is associated with a 14.3% increase in price. - central_air_dummy (0.458): Homes with central air are estimated to be 45.8% more expensive than the baseline (e.g., no AC). This is a very significant amenity premium. - central_air_missing (-0.230): Homes where central air data is missing are 23.0% cheaper than the baseline.\n4.2 Census variables:\n\n\nCode\nmodel_2 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census\n                  ba_rate +\n                  unemployment_rate,\n  \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_2)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate, \n    data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-3.2514 -0.1415  0.0546  0.2237  2.0880 \n\nCoefficients:\n                            Estimate   Std. Error t value             Pr(&gt;|t|)\n(Intercept)              7.279501574  0.064443586 112.959 &lt; 0.0000000000000002\nlog(total_livable_area)  0.700431343  0.008755698  79.997 &lt; 0.0000000000000002\nnumber_of_bathrooms      0.057764788  0.004568488  12.644 &lt; 0.0000000000000002\nhouse_age_c             -0.000064246  0.000097377  -0.660                0.509\nhouse_age_c2             0.000009814  0.000001468   6.686    0.000000000023255\ninterior_condition      -0.126482820  0.003572164 -35.408 &lt; 0.0000000000000002\nquality_grade_num        0.001702051  0.002417186   0.704                0.481\nfireplaces               0.064233267  0.008917462   7.203    0.000000000000602\ngarage_spaces            0.168324520  0.005472052  30.761 &lt; 0.0000000000000002\ncentral_air_dummy        0.219136581  0.006851612  31.983 &lt; 0.0000000000000002\ncentral_air_missing     -0.155840316  0.007252663 -21.487 &lt; 0.0000000000000002\nincome_scaled            0.453407655  0.009052596  50.086 &lt; 0.0000000000000002\nba_rate                  0.012813063  0.000358216  35.769 &lt; 0.0000000000000002\nunemployment_rate       -0.006619614  0.000527486 -12.549 &lt; 0.0000000000000002\n                           \n(Intercept)             ***\nlog(total_livable_area) ***\nnumber_of_bathrooms     ***\nhouse_age_c                \nhouse_age_c2            ***\ninterior_condition      ***\nquality_grade_num          \nfireplaces              ***\ngarage_spaces           ***\ncentral_air_dummy       ***\ncentral_air_missing     ***\nincome_scaled           ***\nba_rate                 ***\nunemployment_rate       ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4017 on 31599 degrees of freedom\nMultiple R-squared:  0.6796,    Adjusted R-squared:  0.6794 \nF-statistic:  5155 on 13 and 31599 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation:\n- Coefficient Evolution (vs.¬†Model 1): - log(total_livable_area) (0.710 vs 0.752): The elasticity of area decreased. This suggests Model 1 overestimated the impact of area. Why? Because larger homes are often located in wealthier neighborhoods. Model 1 incorrectly attributed some of the ‚Äúwealthy neighborhood‚Äù premium to ‚Äúlarge area.‚Äù - quality_grade_num (0.0015 vs 0.070): The coefficient for quality grade became statistically insignificant (p=0.520). This is a key finding: home quality is highly correlated with neighborhood income. Once we directly control for income (income_scaled), the independent effect of quality grade disappears. - central_air_dummy (0.219 vs 0.458): The premium for central air was halved. This also indicates that central air is more common in affluent areas, and Model 1 suffered from significant Omitted Variable Bias (OVB). - New Variable (Census) Interpretation: - income_scaled (0.455): A one-unit increase in standardized census tract income is associated with a 45.5% increase in price. A very strong positive effect. - ba_rate (0.0129): A 1 percentage point increase in the neighborhood‚Äôs bachelor‚Äôs degree attainment rate is associated with a 1.3% price increase. - unemployment_rate (-0.0066): A 1 percentage point increase in the unemployment rate is associated with a 0.66% decrease in price.\n4.3 Spatial features:\n\n\nCode\nmodel_3 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census \n                  ba_rate +\n                  unemployment_rate +\n                \n                  transit_count+\n                  avg_past_price_density+      #Spatial \n                  sqrt(crime_count) +\n                  log(nearest_hospital_knn3),\n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_3)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate + \n    transit_count + avg_past_price_density + sqrt(crime_count) + \n    log(nearest_hospital_knn3), data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-3.03217 -0.09905  0.05666  0.18464  2.17114 \n\nCoefficients:\n                               Estimate   Std. Error t value\n(Intercept)                 6.562309985  0.084718417  77.460\nlog(total_livable_area)     0.742766329  0.007908779  93.917\nnumber_of_bathrooms         0.057510788  0.004057815  14.173\nhouse_age_c                -0.000093444  0.000087876  -1.063\nhouse_age_c2               -0.000005113  0.000001322  -3.866\ninterior_condition         -0.161139310  0.003179231 -50.685\nquality_grade_num          -0.029091664  0.002254432 -12.904\nfireplaces                  0.031025554  0.008023515   3.867\ngarage_spaces               0.096993528  0.005071631  19.125\ncentral_air_dummy           0.099045830  0.006176862  16.035\ncentral_air_missing        -0.164351875  0.006393736 -25.705\nincome_scaled               0.166039882  0.008635180  19.228\nba_rate                     0.003250996  0.000349765   9.295\nunemployment_rate          -0.002568034  0.000466955  -5.500\ntransit_count               0.000311944  0.000171205   1.822\navg_past_price_density      0.003018698  0.000039395  76.626\nsqrt(crime_count)          -0.049042531  0.001408152 -34.828\nlog(nearest_hospital_knn3)  0.081937118  0.006616703  12.383\n                                       Pr(&gt;|t|)    \n(Intercept)                &lt; 0.0000000000000002 ***\nlog(total_livable_area)    &lt; 0.0000000000000002 ***\nnumber_of_bathrooms        &lt; 0.0000000000000002 ***\nhouse_age_c                            0.287626    \nhouse_age_c2                           0.000111 ***\ninterior_condition         &lt; 0.0000000000000002 ***\nquality_grade_num          &lt; 0.0000000000000002 ***\nfireplaces                             0.000110 ***\ngarage_spaces              &lt; 0.0000000000000002 ***\ncentral_air_dummy          &lt; 0.0000000000000002 ***\ncentral_air_missing        &lt; 0.0000000000000002 ***\nincome_scaled              &lt; 0.0000000000000002 ***\nba_rate                    &lt; 0.0000000000000002 ***\nunemployment_rate                  0.0000000384 ***\ntransit_count                          0.068458 .  \navg_past_price_density     &lt; 0.0000000000000002 ***\nsqrt(crime_count)          &lt; 0.0000000000000002 ***\nlog(nearest_hospital_knn3) &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3536 on 31481 degrees of freedom\n  (114 observations deleted due to missingness)\nMultiple R-squared:  0.7505,    Adjusted R-squared:  0.7504 \nF-statistic:  5571 on 17 and 31481 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation:\n- Coefficient Evolution (vs.¬†Model 2): - income_scaled (0.146 vs 0.455): The effect of income dropped sharply (by ~2/3). This again reveals OVB in Model 2. The large ‚Äúincome‚Äù effect in Model 2 was confounded with ‚Äúspatial amenities‚Äù‚Äîhigh-income individuals tend to live in low-crime, accessible areas. - ba_rate (0.0027 vs 0.0129): The education premium also dropped significantly for the same reason. - garage_spaces (0.095 vs 0.170): The garage premium decreased, likely because spatial variables (like density or transit access) have captured related information. - New Variable (Spatial) Interpretation: - transit_count (0.00029): Each additional nearby public transit stop is associated with a 0.029% increase in price. - avg_past_price_density (0.0032): As a proxy for local market heat or locational value, each unit increase is associated with a 0.32% price increase. - sqrt(crime_count) (-0.040): A one-unit increase in the square root of the crime count is associated with a 4.0% decrease in price. - log(nearest_hospital_knn3) (0.087): A 1% increase in the distance from the nearest hospital is associated with a 0.087% increase in price. This suggests people prefer to live further away from hospitals (perhaps to avoid noise, traffic, or sirens), not closer.\n4.4 Interactions and fixed effects:\n\n\nCode\nmodel_4 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census \n                  ba_rate +\n                  unemployment_rate +\n                \n                  transit_count+\n                  avg_past_price_density+      #Spatial \n                  sqrt(crime_count) +\n                  log(nearest_hospital_knn3)+\n                \n                  (interior_condition * income_scaled)+  #FE & Interaction\n                  factor(zip_code),\n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_4)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate + \n    transit_count + avg_past_price_density + sqrt(crime_count) + \n    log(nearest_hospital_knn3) + (interior_condition * income_scaled) + \n    factor(zip_code), data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-2.86007 -0.09067  0.05500  0.16974  2.17553 \n\nCoefficients:\n                                     Estimate   Std. Error t value\n(Intercept)                       7.147174262  0.125631514  56.890\nlog(total_livable_area)           0.762702187  0.007951214  95.923\nnumber_of_bathrooms               0.062545809  0.003951966  15.827\nhouse_age_c                       0.000081673  0.000091303   0.895\nhouse_age_c2                      0.000002396  0.000001333   1.797\ninterior_condition               -0.167693153  0.003180473 -52.726\nquality_grade_num                -0.020614687  0.002369293  -8.701\nfireplaces                        0.040919276  0.008065426   5.073\ngarage_spaces                     0.062755291  0.005224896  12.011\ncentral_air_dummy                 0.088693723  0.006224102  14.250\ncentral_air_missing              -0.145545244  0.006529396 -22.291\nincome_scaled                    -0.308308801  0.022099101 -13.951\nba_rate                           0.004727434  0.000434125  10.890\nunemployment_rate                -0.002009575  0.000512327  -3.922\ntransit_count                     0.000210309  0.000188166   1.118\navg_past_price_density            0.002676947  0.000053851  49.710\nsqrt(crime_count)                -0.040746561  0.001633811 -24.940\nlog(nearest_hospital_knn3)       -0.007368586  0.012879355  -0.572\nfactor(zip_code)19103            -0.191888874  0.038623356  -4.968\nfactor(zip_code)19104             0.043168643  0.044596804   0.968\nfactor(zip_code)19106            -0.072911591  0.039431744  -1.849\nfactor(zip_code)19107             0.002870411  0.041113568   0.070\nfactor(zip_code)19111             0.092033842  0.042730139   2.154\nfactor(zip_code)19114             0.046991631  0.045784241   1.026\nfactor(zip_code)19115             0.036786258  0.045577032   0.807\nfactor(zip_code)19116             0.074693711  0.047046924   1.588\nfactor(zip_code)19118             0.042277408  0.050964443   0.830\nfactor(zip_code)19119            -0.005571634  0.045415173  -0.123\nfactor(zip_code)19120            -0.001979921  0.042451411  -0.047\nfactor(zip_code)19121            -0.079268931  0.042967693  -1.845\nfactor(zip_code)19122            -0.046067555  0.043677075  -1.055\nfactor(zip_code)19123            -0.124474312  0.042021722  -2.962\nfactor(zip_code)19124            -0.042922550  0.042779720  -1.003\nfactor(zip_code)19125            -0.053924641  0.040136398  -1.344\nfactor(zip_code)19126            -0.096134211  0.050099576  -1.919\nfactor(zip_code)19127            -0.054620383  0.046526938  -1.174\nfactor(zip_code)19128            -0.063379705  0.041111172  -1.542\nfactor(zip_code)19129            -0.081840032  0.044983229  -1.819\nfactor(zip_code)19130             0.004465219  0.038447508   0.116\nfactor(zip_code)19131            -0.124963371  0.044224170  -2.826\nfactor(zip_code)19132            -0.353731640  0.042632020  -8.297\nfactor(zip_code)19133            -0.360578685  0.045760207  -7.880\nfactor(zip_code)19134            -0.167019067  0.042150583  -3.962\nfactor(zip_code)19135             0.066237684  0.045406809   1.459\nfactor(zip_code)19136             0.093091097  0.045399194   2.051\nfactor(zip_code)19137             0.003922355  0.050119037   0.078\nfactor(zip_code)19138            -0.061547285  0.045851962  -1.342\nfactor(zip_code)19139            -0.125514020  0.043923079  -2.858\nfactor(zip_code)19140            -0.243638620  0.042070333  -5.791\nfactor(zip_code)19141            -0.104140516  0.045656220  -2.281\nfactor(zip_code)19142            -0.117894811  0.045192100  -2.609\nfactor(zip_code)19143            -0.121037876  0.042647300  -2.838\nfactor(zip_code)19144            -0.119218035  0.044443147  -2.682\nfactor(zip_code)19145             0.000416732  0.040490607   0.010\nfactor(zip_code)19146            -0.065296494  0.038364344  -1.702\nfactor(zip_code)19147            -0.022134932  0.038396424  -0.576\nfactor(zip_code)19148             0.034927326  0.039935049   0.875\nfactor(zip_code)19149             0.163264326  0.043056311   3.792\nfactor(zip_code)19150            -0.044481749  0.048200789  -0.923\nfactor(zip_code)19151            -0.087776350  0.045219855  -1.941\nfactor(zip_code)19152             0.091653850  0.044520396   2.059\nfactor(zip_code)19153            -0.033554531  0.052491824  -0.639\nfactor(zip_code)19154             0.056967215  0.044374083   1.284\ninterior_condition:income_scaled  0.119746969  0.005588581  21.427\n                                             Pr(&gt;|t|)    \n(Intercept)                      &lt; 0.0000000000000002 ***\nlog(total_livable_area)          &lt; 0.0000000000000002 ***\nnumber_of_bathrooms              &lt; 0.0000000000000002 ***\nhouse_age_c                                   0.37105    \nhouse_age_c2                                  0.07241 .  \ninterior_condition               &lt; 0.0000000000000002 ***\nquality_grade_num                &lt; 0.0000000000000002 ***\nfireplaces                        0.00000039295484010 ***\ngarage_spaces                    &lt; 0.0000000000000002 ***\ncentral_air_dummy                &lt; 0.0000000000000002 ***\ncentral_air_missing              &lt; 0.0000000000000002 ***\nincome_scaled                    &lt; 0.0000000000000002 ***\nba_rate                          &lt; 0.0000000000000002 ***\nunemployment_rate                 0.00008784000945812 ***\ntransit_count                                 0.26371    \navg_past_price_density           &lt; 0.0000000000000002 ***\nsqrt(crime_count)                &lt; 0.0000000000000002 ***\nlog(nearest_hospital_knn3)                    0.56724    \nfactor(zip_code)19103             0.00000067928689067 ***\nfactor(zip_code)19104                         0.33306    \nfactor(zip_code)19106                         0.06446 .  \nfactor(zip_code)19107                         0.94434    \nfactor(zip_code)19111                         0.03126 *  \nfactor(zip_code)19114                         0.30472    \nfactor(zip_code)19115                         0.41960    \nfactor(zip_code)19116                         0.11238    \nfactor(zip_code)19118                         0.40680    \nfactor(zip_code)19119                         0.90236    \nfactor(zip_code)19120                         0.96280    \nfactor(zip_code)19121                         0.06507 .  \nfactor(zip_code)19122                         0.29156    \nfactor(zip_code)19123                         0.00306 ** \nfactor(zip_code)19124                         0.31571    \nfactor(zip_code)19125                         0.17911    \nfactor(zip_code)19126                         0.05501 .  \nfactor(zip_code)19127                         0.24042    \nfactor(zip_code)19128                         0.12316    \nfactor(zip_code)19129                         0.06887 .  \nfactor(zip_code)19130                         0.90754    \nfactor(zip_code)19131                         0.00472 ** \nfactor(zip_code)19132            &lt; 0.0000000000000002 ***\nfactor(zip_code)19133             0.00000000000000339 ***\nfactor(zip_code)19134             0.00007435204084637 ***\nfactor(zip_code)19135                         0.14464    \nfactor(zip_code)19136                         0.04032 *  \nfactor(zip_code)19137                         0.93762    \nfactor(zip_code)19138                         0.17951    \nfactor(zip_code)19139                         0.00427 ** \nfactor(zip_code)19140             0.00000000705409283 ***\nfactor(zip_code)19141                         0.02256 *  \nfactor(zip_code)19142                         0.00909 ** \nfactor(zip_code)19143                         0.00454 ** \nfactor(zip_code)19144                         0.00731 ** \nfactor(zip_code)19145                         0.99179    \nfactor(zip_code)19146                         0.08876 .  \nfactor(zip_code)19147                         0.56429    \nfactor(zip_code)19148                         0.38180    \nfactor(zip_code)19149                         0.00015 ***\nfactor(zip_code)19150                         0.35610    \nfactor(zip_code)19151                         0.05225 .  \nfactor(zip_code)19152                         0.03953 *  \nfactor(zip_code)19153                         0.52268    \nfactor(zip_code)19154                         0.19922    \ninterior_condition:income_scaled &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3423 on 31435 degrees of freedom\n  (114 observations deleted due to missingness)\nMultiple R-squared:  0.7665,    Adjusted R-squared:  0.7661 \nF-statistic:  1638 on 63 and 31435 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\nvif(model_4)\n\n\n                                       GVIF Df GVIF^(1/(2*Df))\nlog(total_livable_area)            1.523908  1        1.234467\nnumber_of_bathrooms                1.611921  1        1.269614\nhouse_age_c                        1.580710  1        1.257263\nhouse_age_c2                       1.470007  1        1.212439\ninterior_condition                 1.533402  1        1.238306\nquality_grade_num                  1.779716  1        1.334060\nfireplaces                         1.295084  1        1.138018\ngarage_spaces                      1.586272  1        1.259473\ncentral_air_dummy                  2.084180  1        1.443669\ncentral_air_missing                1.453277  1        1.205519\nincome_scaled                     24.491928  1        4.948932\nba_rate                            6.178507  1        2.485660\nunemployment_rate                  2.016927  1        1.420186\ntransit_count                      1.715169  1        1.309645\navg_past_price_density             7.837830  1        2.799612\nsqrt(crime_count)                  2.815547  1        1.677959\nlog(nearest_hospital_knn3)         8.102261  1        2.846447\nfactor(zip_code)                 565.126609 45        1.072950\ninterior_condition:income_scaled  20.090358  1        4.482227\n\n\nCoefficient Interpretation:\n- Fixed Effects Interpretation: - These coefficients represent the price difference for each zip code relative to the ‚Äúreference zip code‚Äù (which is omitted from the list, e.g., 19102). - Example: factor(zip_code)19106 (-0.107): A home in zip code 19106 is, on average, 10.7% less expensive than a home in the reference zip code, holding all other variables constant. - Example: factor(zip_code)19149 (0.183): A home in zip code 19149 is, on average, 18.3% more expensive. - interior_condition:income_scaled (0.117) (Interaction Term): - This is one of the most interesting findings. It shows that the impact of interior_condition depends on income_scaled. - The total marginal effect of interior_condition is:\\[= -0.1695 + 0.1165 \\times \\text{income\\_scaled}\\] - At the baseline income level (income_scaled = 0), each one-unit worsening in condition is associated with a 17.0% price decrease (-0.1695). However, this penalty is mitigated (lessened) in higher-income areas. For each one-unit increase in income_scaled, the negative penalty of poor condition is reduced by 11.7 percentage points. This may imply that in high-income neighborhoods, ‚Äúfixer-uppers‚Äù (homes in poor condition) are seen as investment opportunities with high renovation potential. Therefore, the market penalty for ‚Äúpoor condition‚Äù is smaller."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-5-model-validation",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-5-model-validation",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 5: Model Validation",
    "text": "Phase 5: Model Validation\n\n10-fold cross-validation\n5.1 Compare all 4 models:"
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-6-model-diagnostics",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-6-model-diagnostics",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 6: Model Diagnostics",
    "text": "Phase 6: Model Diagnostics\n\nCheck assumptions for best model:\n6.1 Residual plot:\n\n\nCode\nmodel_data &lt;- data.frame(\n  Fitted = fitted(model_4),\n  Residuals = resid(model_4)\n)\n\np_resid_fitted &lt;- ggplot(model_data, aes(x = Fitted, y = Residuals)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\", size = 2) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"loess\", color = \"black\", se = FALSE, linewidth = 0.8) +\n  labs(\n    title = \"Residuals vs Fitted Values\",\n    subtitle = \"Checking linearity and homoscedasticity for Model 4\",\n    x = \"Fitted Values (Log(Sale Price))\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_resid_fitted\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\nresid_full &lt;- rep(NA, nrow(opa_census_all))\nresid_full[-as.numeric(model_4$na.action)] &lt;- resid(model_4)\n\nopa_census_all$residuals &lt;- resid_full\n\ntract_resid &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_residual = mean(residuals, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_resid, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_residual), color = \"white\", size = 0.2) +\n  scale_fill_gradient2(\n    low = \"#6A1B9A\", mid = \"white\", high = \"#FFB300\",\n    midpoint = 0,\n    limits = c(-0.5, 0.5),\n    name = \"Mean Log Residual\",\n    breaks = c(-0.3, 0, 0.3),\n    labels = c(\"Overestimated\", \"Accurate\", \"Underestimated\"),\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Hardest to Predict Neighborhoods in Philadelphia\",\n    subtitle = \"Yellow = underestimation | Purple = overestimation\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\nInterpretation:\n- Central Tendency of Residuals: The residuals are generally centered around the zero line, showing no systematic deviation. This indicates that the model captures the overall linear relationship between predictors and the response variable effectively.\n- Homoscedasticity: The residuals show greater variability and dispersion in the lower fitted value range (approximately 10‚Äì12), while they appear more stable at higher fitted values. This suggests that the model performs less effectively for observations with lower predicted values.\n- Model Assumption Assessment: Overall, the assumptions of linearity and homoscedasticity are largely satisfied, indicating a sound model fit, with only slight deviations to monitor at higher fitted values.\n6.2 Q-Q plot:\n\n\nCode\np_qq &lt;- ggplot(model_data, aes(sample = Residuals)) +\n  stat_qq(color = \"#6A1B9A\", size = 2, alpha = 0.6) +\n  stat_qq_line(color = \"red\",linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Normal Q-Q Plot\",\n    subtitle = \"Checking normality of residuals for Model 4\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_qq\n\n\n\n\n\n\n\n\n\nInterpretation:\n- Overall Shape of the Plot: The residual points generally follow the diagonal line, indicating that the overall distribution of residuals is roughly consistent with a normal distribution.\n- Good Fit in the Central Range: In the central range (around -1 to 1 quantiles), the sample quantiles align closely with the theoretical quantiles, suggesting that most residuals conform well to the assumption of normality.\n- Deviation in the Tails: At both tails‚Äîespecially the upper quantiles‚Äîthe points deviate noticeably from the red dashed line, indicating that the residuals have heavier tails than a normal distribution, suggesting slight non-normality.\n- Assessment of Normality Assumption: Although deviations appear in the tails, the overall alignment with the reference line is strong, indicating that the normality assumption largely holds, with only minor deviations for extreme residuals.\n6.3 Cook‚Äôs distance:\n\n\nCode\ncooks_d &lt;- cooks.distance(model_4)\nmodel_data &lt;- data.frame(\n  Index = 1:length(cooks_d),\n  CooksD = cooks_d\n)\nthreshold &lt;- 4 / nrow(model_4$model)\n\np_cook &lt;- ggplot(model_data, aes(x = Index, y = CooksD)) +\n  geom_segment(aes(xend = Index, yend = 0), color = \"#6A1B9A\", alpha = 0.7) +  # vertical lines\n  geom_point(color = \"#6A1B9A\", size = 0.15) +\n  geom_hline(yintercept = threshold, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Cook's Distance\",\n    subtitle = \"Identifying influential observations for Model 4\",\n    x = \"Observation Index\",\n    y = \"Cook's Distance\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_cook\n\n\n\n\n\n\n\n\n\nInterpretation:\n- Overall Distribution Pattern: Most observations have Cook‚Äôs Distance values close to zero, indicating that the dataset‚Äôs overall influence on the model is balanced, with no widespread undue impact.\n- Presence of Influential Points: A few vertical spikes rise noticeably above the rest, indicating the presence of some influential observations that may affect the estimated model coefficients. - Model Robustness Conclusion: Overall, the model appears robust, with no single observation exerting excessive influence."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-7-conclusions-recommendations",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-7-conclusions-recommendations",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 7: Conclusions & Recommendations",
    "text": "Phase 7: Conclusions & Recommendations\n\nConclusion:\n\nOur final model‚Äôs R¬≤ is 0.84, indicating that the model explains 84% of the variance in log sale prices. The RMSE is , showing that the model‚Äôs predictions are reasonably close to the observed values.\n\nLivable Area of the House matters most for Philadelphia prices.\n\n\n\nRecommendations:\n\nEquity concerns\n\nWhich neighborhoods are hardest to predict?\n\nThe largest prediction errors occur primarily in central Philadelphia, where both overestimation and underestimation coexist within close proximity. This pattern indicates that the model struggles most in areas with high housing heterogeneity- neighborhoods that contain a mix of old row houses, newly renovated apartments, and varying property types within short distances.\nIn contrast, outer neighborhoods such as those in the northwest and northeast tend to have more consistent housing characteristics, leading to smaller residuals. The central tracts‚Äô larger residuals suggest that cultural or historical features have introduced variability that the model‚Äôs current features (mainly physical characteristics) cannot fully explain.\n\nAny data bias?\n\nThe observed spatial pattern of prediction errors reflects inherent data bias. The dataset likely overrepresents mid-range housing conditions and underrepresents both luxury and low-income housing. As shown in the livable area and interior condition maps, data coverage in wealthier areas (especially the northwest) is limited, and these tracts often contain more unique, high-value properties that the model cannot generalize well.\n\nWealthier tracts are more likely to be well-documented, while poorer areas lack records, creating spatial imbalance in model performance.\n\nHigh-priced homes typically have larger negotiation margins, meaning their final sale prices are often lower than the listed prices. In contrast, low-priced homes sell closer to their listing prices. As a result, model tends to overestimate expensive homes and underestimate affordable ones, introducing systematic bias.\n\n\nRecommendations to government\n\n\nImmediate System Calibration: We recommend utilizing our model‚Äôs findings to immediately adjust property assessments in systematically overvalued low-income communities. This action will ensure a fair distribution of the tax burden and address current inequities.\nIntegrate Advanced Spatial Features: We advise the Office of Property Assessment (OPA) to permanently integrate the effective spatial characteristics identified by our study‚Äîspecifically Comparable Sales Proxies (surrounding transaction prices) and Neighborhood Fixed Effects‚Äîinto the next-generation AVM. This will significantly enhance the model‚Äôs responsiveness to rapidly changing market dynamics.\nExtreme high values almost exclusively stem from corporate transactions and require manual review for outliers.\n\n\nLimitations and nest steps\n\nLimitations: Inherent Data Biases\n\nOur predictive accuracy is constrained by inherent data biases which affect equity. We find a dual challenge in data coverage: in affluent areas, high-value, unique properties suffer from data sparsity, making generalization difficult. Conversely, lower-income areas often show data incompleteness, leading to less reliable predictions and higher residual errors. Critically, we observed a price-tier bias: high-priced homes tend to sell below their list price, while low-priced homes transact closer to it. This systemic pattern means the model is prone to over-assessing expensive properties and under-assessing affordable ones, creating a structural risk for vertical inequity in the tax system.\n\nNext Steps: Enhancing Data Quality and Fairness\n\nTo address these limitations, our next steps focus on data enrichment and equitable optimization. The City should partner with us to integrate non-public data, such as detailed appraisal records and permit data, which can account for unobserved renovation quality and close the data gap. Furthermore, to combat the systematic price-tier bias, we recommend integrating fairness metrics directly into the AVM‚Äôs optimization process. This will shift the model‚Äôs objective beyond simple average accuracy (RMSE) to ensure that prediction errors are uniformly low across all price tiers and all Philadelphia neighborhoods, securing both accuracy and equity."
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#the-problem",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#the-problem",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "The Problem",
    "text": "The Problem\nEvery year, thousands of Philadelphians buy or sell homes, and every transaction tells a story about how people value location, convenience, and opportunity.\nYet the city‚Äôs current Automated Valuation Model doesn‚Äôt always capture these stories. Some neighborhoods are undervalued, while others bear unfairly high assessments.\n\n\n\n\n\n\nWhat we are trying to explore?\n\n\n\nPrice Gap: Why do two homes with similar size and design sell for very different prices in Philadelphia?\nSpatial Drivers: Which neighborhood, accessibility, and socioeconomic factors drive these differences?\nFair Valuation: How can understanding them help the city create a fairer, smarter system for property tax assessment?"
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#data-sources",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#data-sources",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Data Sources",
    "text": "Data Sources\n\n\nüè†Property sales\n(n = 34,559 2023-2024)\n\nLivable Area\nBedrooms Counts\nBathrooms Counts\nHouse Age\nInterior Condition\nQuality Grade\nFireplaces\nGarage Spaces\nCentral Air\nMarket Value\nSale Price\n\n\nüî¢Census ACS\n(n = 2023)\n\nMedian Income\nEducation Level\nUnemployment Rate\n\nüèõOpenDataPhilly\n\nTransit\nParks and Recreation center\nHospitals\nCrime(2023)\nPoint of Interest"
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#where-are-expensive-homes",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#where-are-expensive-homes",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Where Are Expensive Homes?",
    "text": "Where Are Expensive Homes?\n\n\n\n\n\n\nResidential sale prices exhibited clear signs of spatial clustering.\nhighest-priced properties: Center City core, University City.\nMiddle & Lower-Price tiers : North Philadelphia, West Philadelphia (outside the university sphere), and South Philadelphia.\nAccount for the specific ‚Äúmicro-market‚Äù in which a property is situated."
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#why-are-they-expensive",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#why-are-they-expensive",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Why Are They Expensive?",
    "text": "Why Are They Expensive?"
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#highlight",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#highlight",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Highlight",
    "text": "Highlight\n\nOur adjustment follows established mass appraisal practices, where implausible sale prices are identified and adjusted based on their ratio to assessed market value (Deaf Smith CAD, Mass Appraisal and Ratio Study Manual, 2023).\nPrincipleÔºöRetain all information that the data tells us (including outliers) rather than deleting it.\nObjectiveÔºöEnsure equity.\nRelationship Formula: Sale Price = -9189.29 + 1.03*Market value\nApproach: Unreliable sale price(low weight)~ Reliable sale price(high weight)"
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#model-building-and-validation",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#model-building-and-validation",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Model Building and Validation",
    "text": "Model Building and Validation\nModel Performance Improves with Each Layer\n\n\n\nModel\nCV RMSE (log)\nR¬≤\nRMSE\n\n\n\n\nStructural Only\n0.5497\n0.5235\n221675.8\n\n\n+ Census\n0.4519\n0.6779\n178383.4\n\n\n+ Spatial\n0.3994\n0.7486\n132547.4\n\n\n+ Interactions/FE\n0.389\n0.7611\n124417.4\n\n\n\n\nR¬≤ = 0.76 =&gt; The model explains 76% of the variation in residential sale prices.\nRMSE = 124,417.4 =&gt; The average error between the model‚Äôs predicted sale price and the actual sale price is approximately \\(\\$124,417.4\\).\n\n\n\n\n\n\n\nImportant\n\n\n\nFor K-fold cross-validation, we use the entire dataset with replaced outliers for training to maintain consistency with the model derivation process. For the final test, however, we use a purified set that excludes all handled data, ensuring a reliable performance evaluation on genuine, trustworthy data."
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#key-factors",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#key-factors",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Key Factors",
    "text": "Key Factors\n\nLivable Area\nComparable Sales\nInterior Condition\nZip Code"
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#identifying-the-hardest-to-predict-neighborhoods",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#identifying-the-hardest-to-predict-neighborhoods",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Identifying the Hardest-to-Predict Neighborhoods",
    "text": "Identifying the Hardest-to-Predict Neighborhoods\n\n\n\n\n\nInterpretation\n\nUnderestimate zip code: 19130, 19123, 19104, 19144‚Ä¶\nOverestimate zip code: 19102, 19107, 19116, 19154‚Ä¶\nMarket is moving faster than the city‚Äôs valuation system can keep up."
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#recommendations-and-limitations",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#recommendations-and-limitations",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Recommendations and Limitations",
    "text": "Recommendations and Limitations\n\nCorrect the assessed values in low-income communities that have been systematically overestimated.\nRoutinely incorporate the effective spatial features‚Äîparticularly surrounding transaction prices and neighborhood fixed effects.\nExtreme high values almost exclusively stem from corporate transactions and require manual review for outliers.\n\n\n\n\n\n\n\nLimitations: Algorithmic Fairness\n\n\n\nSpatial Coverage Bias and Data Quality Bias\nFeature Omission Bias and Spatial Dependence Bias"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you‚Äôll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr.¬†Delmelle‚Äôs sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let‚Äôs personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (‚úèÔ∏è) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick ‚ÄúCommit changes‚Äù at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (‚úèÔ∏è) to edit\nUpdate the ‚ÄúAbout Me‚Äù section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you‚Äôre taking this course\n\nClick ‚ÄúCommit changes‚Äù\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (‚úèÔ∏è) to edit\nFill in your notes from the first class\nClick ‚ÄúCommit changes‚Äù\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the ‚ÄúSettings‚Äù tab at the top of your repository\nFind Pages: Scroll down and click ‚ÄúPages‚Äù in the left sidebar\nConfigure Source:\n\nSource: Select ‚ÄúDeploy from a branch‚Äù\nBranch: Select ‚Äúmain‚Äù\nFolder: Select ‚Äú/ docs‚Äù\n\nSave: Click ‚ÄúSave‚Äù\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you‚Äôll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you‚Äôll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected ‚Äúmain‚Äù branch and ‚Äú/docs‚Äù folder\n\n\n\n\n\nCheck permissions: Make sure you‚Äôre in YOUR repository, not the template\nSign in: Ensure you‚Äôre signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon‚Äôt panic! Every change is tracked in Git\nSee history: Click the ‚ÄúHistory‚Äù button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you‚Äôve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don‚Äôt struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you‚Äôll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr.¬†Delmelle‚Äôs sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let‚Äôs personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (‚úèÔ∏è) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick ‚ÄúCommit changes‚Äù at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (‚úèÔ∏è) to edit\nUpdate the ‚ÄúAbout Me‚Äù section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you‚Äôre taking this course\n\nClick ‚ÄúCommit changes‚Äù\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (‚úèÔ∏è) to edit\nFill in your notes from the first class\nClick ‚ÄúCommit changes‚Äù\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the ‚ÄúSettings‚Äù tab at the top of your repository\nFind Pages: Scroll down and click ‚ÄúPages‚Äù in the left sidebar\nConfigure Source:\n\nSource: Select ‚ÄúDeploy from a branch‚Äù\nBranch: Select ‚Äúmain‚Äù\nFolder: Select ‚Äú/ docs‚Äù\n\nSave: Click ‚ÄúSave‚Äù\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you‚Äôll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you‚Äôll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected ‚Äúmain‚Äù branch and ‚Äú/docs‚Äù folder\n\n\n\n\n\nCheck permissions: Make sure you‚Äôre in YOUR repository, not the template\nSign in: Ensure you‚Äôre signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon‚Äôt panic! Every change is tracked in Git\nSee history: Click the ‚ÄúHistory‚Äù button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you‚Äôve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don‚Äôt struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 02 Notes",
    "section": "",
    "text": "Êú¨Âë®Â≠¶‰π†ÁöÑÈáçÁÇπ ‚Ä¶\n\n\n\n\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n\n\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#quarto",
    "href": "weekly-notes/week-02-notes.html#quarto",
    "title": "Week 02 Notes",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#running-code",
    "href": "weekly-notes/week-02-notes.html#running-code",
    "title": "Week 02 Notes",
    "section": "",
    "text": "When you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  }
]