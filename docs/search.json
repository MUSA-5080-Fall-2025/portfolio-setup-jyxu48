[
  {
    "objectID": "weekly-notes/week-12-notes.html",
    "href": "weekly-notes/week-12-notes.html",
    "title": "Week 12 Notes - Course Wrap-Up and Looking Forward",
    "section": "",
    "text": "Policy analytics as an iterative and reflective process\nImportance of integrating modeling, evaluation, fairness, and communication\nUnderstanding analytics as decision support rather than decision making\nViewing models as tools embedded in social and institutional contexts"
  },
  {
    "objectID": "weekly-notes/week-12-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-12-notes.html#key-concepts-learned",
    "title": "Week 12 Notes - Course Wrap-Up and Looking Forward",
    "section": "",
    "text": "Policy analytics as an iterative and reflective process\nImportance of integrating modeling, evaluation, fairness, and communication\nUnderstanding analytics as decision support rather than decision making\nViewing models as tools embedded in social and institutional contexts"
  },
  {
    "objectID": "weekly-notes/week-12-notes.html#coding-techniques",
    "href": "weekly-notes/week-12-notes.html#coding-techniques",
    "title": "Week 12 Notes - Course Wrap-Up and Looking Forward",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nIntegrating multiple analytical steps into a coherent workflow\nRevisiting earlier models with improved evaluation and interpretation\nTreating documentation and explanation as part of the final output\nPreparing analysis for presentation rather than exploration only"
  },
  {
    "objectID": "weekly-notes/week-12-notes.html#questions-challenges",
    "href": "weekly-notes/week-12-notes.html#questions-challenges",
    "title": "Week 12 Notes - Course Wrap-Up and Looking Forward",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nHow to balance technical rigor with real-world constraints\nKnowing when analysis is sufficient to inform action\nTranslating analytical work into policy-relevant narratives"
  },
  {
    "objectID": "weekly-notes/week-12-notes.html#connections-to-policy",
    "href": "weekly-notes/week-12-notes.html#connections-to-policy",
    "title": "Week 12 Notes - Course Wrap-Up and Looking Forward",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nEffective policy analytics requires judgment, not just computation\nAnalytical workflows must align with governance, accountability, and equity goals\nThe value of analytics depends on how responsibly it is applied"
  },
  {
    "objectID": "weekly-notes/week-12-notes.html#reflection",
    "href": "weekly-notes/week-12-notes.html#reflection",
    "title": "Week 12 Notes - Course Wrap-Up and Looking Forward",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: realizing how much thinking happens after the model is run\nThis course reframed analytics as a practice, not a checklist\nI now see models less as answers and more as questions generators\nAlso learned: if a result seems too clean, it probably deserves another look üôÇ"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html",
    "href": "weekly-notes/week-10-notes.html",
    "title": "Week 10 Notes - Interpreting and Communicating Model Results",
    "section": "",
    "text": "Importance of interpretation when translating models into policy insights\nDistinction between statistical output and substantive meaning\nRole of uncertainty and limitations in analytical conclusions\nCommunication as a critical step in the policy analytics process"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-10-notes.html#key-concepts-learned",
    "title": "Week 10 Notes - Interpreting and Communicating Model Results",
    "section": "",
    "text": "Importance of interpretation when translating models into policy insights\nDistinction between statistical output and substantive meaning\nRole of uncertainty and limitations in analytical conclusions\nCommunication as a critical step in the policy analytics process"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#coding-techniques",
    "href": "weekly-notes/week-10-notes.html#coding-techniques",
    "title": "Week 10 Notes - Interpreting and Communicating Model Results",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nExtracting and summarizing key model outputs\nPresenting results in clear, interpretable formats\nUsing tables or visual summaries to support explanation\nSeparating technical analysis from policy-facing communication"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#questions-challenges",
    "href": "weekly-notes/week-10-notes.html#questions-challenges",
    "title": "Week 10 Notes - Interpreting and Communicating Model Results",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nHow much technical detail is appropriate for different audiences\nAvoiding overstatement of model findings\nCommunicating uncertainty without undermining credibility"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#connections-to-policy",
    "href": "weekly-notes/week-10-notes.html#connections-to-policy",
    "title": "Week 10 Notes - Interpreting and Communicating Model Results",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nPolicy decisions depend on how analytical results are framed and explained\nMisinterpretation can lead to misguided or oversimplified policies\nClear communication supports transparency and informed decision-making"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#reflection",
    "href": "weekly-notes/week-10-notes.html#reflection",
    "title": "Week 10 Notes - Interpreting and Communicating Model Results",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: communication framed as part of analytical responsibility\nThis week emphasized that results do not speak for themselves\nI need to be more deliberate about how findings are presented to non-technical audiences"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7 Notes - Model Performance and Evaluation",
    "section": "",
    "text": "Importance of evaluating model performance beyond model fitting\nDifference between in-sample fit and out-of-sample performance\nRole of error metrics in assessing model usefulness\nUnderstanding trade-offs between different types of errors"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-07-notes.html#key-concepts-learned",
    "title": "Week 7 Notes - Model Performance and Evaluation",
    "section": "",
    "text": "Importance of evaluating model performance beyond model fitting\nDifference between in-sample fit and out-of-sample performance\nRole of error metrics in assessing model usefulness\nUnderstanding trade-offs between different types of errors"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#coding-techniques",
    "href": "weekly-notes/week-07-notes.html#coding-techniques",
    "title": "Week 7 Notes - Model Performance and Evaluation",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nSplitting data into training and testing sets\nEvaluating model performance using appropriate metrics\nComparing multiple models based on performance measures\nInterpreting evaluation results rather than optimizing a single metric"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#questions-challenges",
    "href": "weekly-notes/week-07-notes.html#questions-challenges",
    "title": "Week 7 Notes - Model Performance and Evaluation",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nChoosing evaluation metrics that align with policy goals\nUnderstanding how performance varies across different populations\nAvoiding over-optimization at the expense of interpretability"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#connections-to-policy",
    "href": "weekly-notes/week-07-notes.html#connections-to-policy",
    "title": "Week 7 Notes - Model Performance and Evaluation",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nModel performance affects the reliability of policy recommendations\nDifferent error types can have unequal policy consequences\nEvaluation choices should reflect policy priorities, not just accuracy"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#reflection",
    "href": "weekly-notes/week-07-notes.html#reflection",
    "title": "Week 7 Notes - Model Performance and Evaluation",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: performance evaluation framed as a value-based choice\nThis week emphasized that ‚Äúbetter‚Äù models depend on context and purpose\nI need to be more explicit about why certain evaluation metrics are used"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 Notes - Regression Diagnostics and Model Evaluation",
    "section": "",
    "text": "Importance of evaluating model assumptions after fitting regression\nRole of diagnostics in assessing model reliability\nDifference between model fit and model validity\nUnderstanding uncertainty and error in regression results"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "title": "Week 5 Notes - Regression Diagnostics and Model Evaluation",
    "section": "",
    "text": "Importance of evaluating model assumptions after fitting regression\nRole of diagnostics in assessing model reliability\nDifference between model fit and model validity\nUnderstanding uncertainty and error in regression results"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#coding-techniques",
    "href": "weekly-notes/week-05-notes.html#coding-techniques",
    "title": "Week 5 Notes - Regression Diagnostics and Model Evaluation",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nExamining residuals to assess model behavior\nUsing diagnostic plots to check assumptions\nComparing models using summary measures\nInterpreting diagnostic output alongside coefficients"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#questions-challenges",
    "href": "weekly-notes/week-05-notes.html#questions-challenges",
    "title": "Week 5 Notes - Regression Diagnostics and Model Evaluation",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nHow to decide when a model is ‚Äúgood enough‚Äù for policy use\nInterpreting conflicting diagnostic signals\nKnowing when diagnostics suggest revising the model versus rethinking the question"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#connections-to-policy",
    "href": "weekly-notes/week-05-notes.html#connections-to-policy",
    "title": "Week 5 Notes - Regression Diagnostics and Model Evaluation",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nModel diagnostics affect confidence in policy recommendations\nIgnoring diagnostics can lead to misleading conclusions\nTransparent reporting of model limitations is critical in public contexts"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#reflection",
    "href": "weekly-notes/week-05-notes.html#reflection",
    "title": "Week 5 Notes - Regression Diagnostics and Model Evaluation",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: diagnostics as an integral part of analysis, not an optional step\nThis week emphasized caution in trusting numerical results alone\nI need to be more disciplined about checking assumptions before interpretation"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes - Data Visualization for Policy Analysis",
    "section": "",
    "text": "Role of data visualization in policy analysis and communication\nDifference between exploratory and explanatory visualization\nImportance of audience, context, and intent when designing charts\nVisualization as an analytical decision, not a neutral presentation step"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3 Notes - Data Visualization for Policy Analysis",
    "section": "",
    "text": "Role of data visualization in policy analysis and communication\nDifference between exploratory and explanatory visualization\nImportance of audience, context, and intent when designing charts\nVisualization as an analytical decision, not a neutral presentation step"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3 Notes - Data Visualization for Policy Analysis",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nUsing ggplot2 as the primary visualization framework\nMapping variables to aesthetics (x, y, color, size)\nLayer-based plotting structure (data, geometry, scales)\nBasic customization to improve readability and interpretation"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3 Notes - Data Visualization for Policy Analysis",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nHow to balance visual simplicity with analytical completeness\nWhen visual choices may unintentionally bias interpretation\nDeciding which variables deserve visual emphasis in policy contexts"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3 Notes - Data Visualization for Policy Analysis",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nVisualizations can shape how policy problems are understood\nChart design choices influence public perception and decision-making\nPoorly designed visuals can obscure inequities or exaggerate trends"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3 Notes - Data Visualization for Policy Analysis",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: visualization framed as an ethical and analytical responsibility\nThis week reinforced that visuals are part of the analysis, not an afterthought\nI need to be more intentional about why I choose specific chart types"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Purpose of public policy analytics in urban planning\nDifference between public-sector analytics and private-sector data science\nImportance of reproducibility, transparency, and documentation\nCourse structure: portfolio-based assignments and concept-focused quizzes"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Purpose of public policy analytics in urban planning\nDifference between public-sector analytics and private-sector data science\nImportance of reproducibility, transparency, and documentation\nCourse structure: portfolio-based assignments and concept-focused quizzes"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBasic Git workflow: clone, commit, push\nUsing GitHub Classroom for assignment repositories\nIntroduction to Quarto for reproducible documents and websites\nOverview of R and tidyverse as core analysis tools"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nStill getting comfortable with Git terminology (commit vs push vs pull)\nUnderstanding how Quarto websites are structured across multiple .qmd files\nHow detailed portfolio documentation should be for grading"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nEmphasis on transparency and reproducibility reflects real public-sector accountability\nGitHub-based workflow mirrors professional policy analysis and open data practices\nAnalytical decisions can influence equity and governance outcomes"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: treating analytics work as a public-facing product, not just code\nThis course encourages building a long-term professional portfolio\nI will apply this workflow consistently to future assignments and projects"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jinyang Xu - MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nAssignment: Completed assignments and analyses\nMidterm: A House Price Prediction Model\nFinal Project: Capstone modeling challenge ‚Äì Modeling Crime Risk around SEPTA Bus Stops\n\n\n\n\n[MUSA student trying hard on data]\n\n\n\n\nEmail: [jyxu48@upenn.edu]\nGitHub: [@jyxu48]"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "Jinyang Xu - MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "Jinyang Xu - MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nAssignment: Completed assignments and analyses\nMidterm: A House Price Prediction Model\nFinal Project: Capstone modeling challenge ‚Äì Modeling Crime Risk around SEPTA Bus Stops"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Jinyang Xu - MUSA 5080 Portfolio",
    "section": "",
    "text": "[MUSA student trying hard on data]"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Jinyang Xu - MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: [jyxu48@upenn.edu]\nGitHub: [@jyxu48]"
  },
  {
    "objectID": "Final/Final_appendix.html",
    "href": "Final/Final_appendix.html",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "",
    "text": "This project investigates the complex relationship between transit ridership and public safety around SEPTA bus stops in Philadelphia. By employing Negative Binomial Regression, we explore the tension between two competing theories: whether high passenger volumes act as protective ‚ÄúEyes on the Street‚Äù or attract crime as ‚ÄúPotential Targets.‚Äù The analysis integrates multi-source data‚Äîincluding ridership, crime incidents, census demographics, and built environment features‚Äîand utilizes interaction terms to reveal how risk dynamics shift between weekdays and weekends. Our ultimate goal is to transition SEPTA Transit Police from reactive to predictive deployment, providing a data-driven framework to identify high-risk anomalies and optimize patrol resources for maximum safety impact."
  },
  {
    "objectID": "Final/Final_appendix.html#project-objective",
    "href": "Final/Final_appendix.html#project-objective",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "",
    "text": "This project investigates the complex relationship between transit ridership and public safety around SEPTA bus stops in Philadelphia. By employing Negative Binomial Regression, we explore the tension between two competing theories: whether high passenger volumes act as protective ‚ÄúEyes on the Street‚Äù or attract crime as ‚ÄúPotential Targets.‚Äù The analysis integrates multi-source data‚Äîincluding ridership, crime incidents, census demographics, and built environment features‚Äîand utilizes interaction terms to reveal how risk dynamics shift between weekdays and weekends. Our ultimate goal is to transition SEPTA Transit Police from reactive to predictive deployment, providing a data-driven framework to identify high-risk anomalies and optimize patrol resources for maximum safety impact."
  },
  {
    "objectID": "Final/Final_appendix.html#complete-data-cleaning-code",
    "href": "Final/Final_appendix.html#complete-data-cleaning-code",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "1.0 Complete data cleaning code",
    "text": "1.0 Complete data cleaning code\nLoad necessary libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE, tigris_class = \"sf\")\nlibrary(MASS)\nlibrary(spdep)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(nngeo)\nlibrary(car)\nlibrary(knitr)\nlibrary(readr)\nlibrary(patchwork)\nlibrary(kableExtra)\nlibrary(lubridate)\n\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  \n\n\nDefine themes\n\n\nCode\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 &lt;- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")"
  },
  {
    "objectID": "Final/Final_appendix.html#load-and-clean-bus-stop-ridership-data",
    "href": "Final/Final_appendix.html#load-and-clean-bus-stop-ridership-data",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "1.1 Load and clean bus stop ridership data:",
    "text": "1.1 Load and clean bus stop ridership data:\n\n1.1.1 Load bus stop ridership data\n2025 summer SEPTA bus ridership data\n\n\nCode\n# 1. Load Bus Data\nbus_raw &lt;- read_csv(\"data/Summer_2025_Stop_Summary_(Bus).csv\")\n\n\n\n\n1.1.2 Clean\n\n\nCode\n# 2. Get Philadelphia Boundary\nphilly_boundary &lt;- counties(state = \"PA\", cb = TRUE, class = \"sf\") %&gt;%\n  filter(NAME == \"Philadelphia\") %&gt;%\n  st_transform(2272)\n\n\nIn preparing the ridership data, we performed two critical structural transformations to align with our research goals.\nFirst, we aggregated bidirectional stops sharing the same location (including opposite sides of a street) into single spatial units. This prevents spatial autocorrelation and ensures our 400m buffers capture a unique street environment.\nSecond, we restructured the dataset into a ‚Äòlong format‚Äô (panel data), distinguishing between Weekday and Weekend ridership. This temporal split is vital, as it allows our model to capture the distinct behavioral patterns of both commuters and potential offenders during different times of the week.\n\n\nCode\n# 3. Process Ridership: Create Long Format (Aggregated + Weekday vs Weekend)\nbus_long &lt;- bus_raw %&gt;%\n  filter(!is.na(Lat) & !is.na(Lon)) %&gt;%\n  mutate(\n    Raw_Weekday = Weekdays_O + Weekdays_1,\n    # A. Weekend average = (Sat total + Sun total) / 2\n    Raw_Weekend = (Saturdays_ + Saturdays1 + Sundays_On + Sundays_Of) / 2\n  ) %&gt;%\n  \n  # B. Aggregation by Stop Name\n  group_by(Stop) %&gt;% \n  summarise(\n    # C. Add two direction ridership\n    Ridership_Weekday = sum(Raw_Weekday, na.rm = TRUE),\n    Ridership_Weekend = sum(Raw_Weekend, na.rm = TRUE),\n    Lat = mean(Lat, na.rm = TRUE),\n    Lon = mean(Lon, na.rm = TRUE),\n    # Keep one Stop_Code as unique ID\n    Stop_Code = first(Stop_Code),\n    .groups = \"drop\"\n  ) %&gt;%\n  \n  # D. Pivot to Long Format\n  pivot_longer(\n    cols = c(Ridership_Weekday, Ridership_Weekend),\n    names_to = \"Time_Type\",\n    values_to = \"Ridership\"\n  ) %&gt;%\n  \n  # E. Create Dummy Variable & Clean up\n  mutate(\n    is_weekend = if_else(Time_Type == \"Ridership_Weekend\", 1, 0),\n    Time_Category = if_else(is_weekend == 1, \"Weekend\", \"Weekday\")\n  ) %&gt;%\n  \n  # F. Convert to SF & Clip\n  st_as_sf(coords = c(\"Lon\", \"Lat\"), crs = 4326) %&gt;%\n  st_transform(2272) %&gt;%\n  st_intersection(philly_boundary) %&gt;%\n  dplyr::select(Stop_Code, Stop, Ridership, is_weekend, Time_Category, geometry)\n\n# Check results\ncat(\"Total Aggregated Stops (Unique Locations):\", nrow(bus_long) / 2, \"\\n\")\n\n\nTotal Aggregated Stops (Unique Locations): 5884 \n\n\nCode\ncat(\"Total Observation Rows (Panel Data):\", nrow(bus_long), \"\\n\")\n\n\nTotal Observation Rows (Panel Data): 11768 \n\n\n\n\n1.1.3 Visualization\n\n\nCode\nggplot() +\n  # A. Base Layer: Philadelphia County Background\n  geom_sf(data = philly_boundary, \n          fill = \"grey98\", \n          color = \"grey50\", \n          size = 0.5) +\n  \n  # B. Station Layer: Simple Points\n  geom_sf(data = bus_long %&gt;% filter(is_weekend == 0), \n          color = \"#3182bd\",  # SEPTA Blue\n          size = 0.1,         # Small dots to avoid clutter\n          alpha = 0.6) +      # Slight transparency\n  \n  # C. Styling\n  labs(\n    title = \"Spatial Coverage of SEPTA Bus Network\",\n    subtitle = paste0(\"Total Aggregated Stops: \", nrow(bus_long) / 2),\n    caption = \"Source: SEPTA Summer 2025 Stop Summary\"\n  ) +\n  theme_void() + # Clean look\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 10, color = \"grey40\", hjust = 0.5),\n    plot.margin = margin(1, 1, 1, 1, \"cm\")\n  )\n\n\n\n\n\n\n\n\n\nThe figure illustrates the extensive spatial coverage of the SEPTA bus network within Philadelphia. After aggregating bidirectional stops, our study includes 5,884 unique locations.\nThe distribution reveals a high density in the Center City and a grid-like arterial pattern extending into residential neighborhoods. This comprehensive coverage ensures that our analysis captures a diverse range of built environments, from dense commercial districts to suburban residential areas."
  },
  {
    "objectID": "Final/Final_appendix.html#load-and-clean-secondary-dataset",
    "href": "Final/Final_appendix.html#load-and-clean-secondary-dataset",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "1.2 Load and clean secondary dataset:",
    "text": "1.2 Load and clean secondary dataset:\n\n1.2.1 Crime data:\nCrime Incidents from 2024\nWe excluded domestic disputes or indoor financial crimes because SEPTA Transit Police cannot police inside people‚Äôs homes. Instead, we selected these specific crime types because they occur in the public right-of-way and directly impact a rider‚Äôs decision to use public transit.\n\n\nCode\n### 1.2.1 Crime data:\n\n# Define Street-Crime List\n# target_crime_types &lt;- c(\n#   \"Robbery Firearm\",\n#   \"Robbery No Firearm\",\n#   \"Aggravated Assault Firearm\",\n#   \"Aggravated Assault No Firearm\",\n#   \"Homicide - Criminal\",\n#   \"Rape\",\n#   \"Other Sex Offenses (Not Commercialized)\",\n#   \"Weapon Violations\",\n#   \"Narcotic / Drug Law Violations\",\n#   \"Vandalism/Criminal Mischief\",\n#   \"Prostitution and Commercialized Vice\",\n#   \"Public Drunkenness\"\n# )\n\ncrime_raw &lt;- read_csv(\"data/crime2024.csv\")\n\n# Filter & Transform\ncrime_raw &lt;- crime_raw %&gt;%\n  filter(!is.na(lat) & !is.na(lng))\n\ncrime_q1 &lt;- crime_raw %&gt;%\n  filter(lubridate::month(dispatch_date) %in% 1:3) %&gt;%\n  mutate(\n    crime_date = as.Date(dispatch_date), \n    day_of_week = wday(crime_date), # 1 is Sunday, 7 is Saturday\n    is_crime_weekend = if_else(day_of_week %in% c(1, 7), 1, 0)\n  ) %&gt;%\n    \n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(2272)  \n\n # Filter Apr to Jun\ncrime_sf &lt;- crime_raw %&gt;%\n  filter(lubridate::month(dispatch_date) %in% 4:6)%&gt;%\n  \n  #filter(text_general_code %in% target_crime_types) %&gt;%\n  \n  mutate(\n    crime_date = as.Date(dispatch_date), \n    day_of_week = wday(crime_date), # 1 is Sunday, 7 is Saturday\n    is_crime_weekend = if_else(day_of_week %in% c(1, 7), 1, 0)\n  ) %&gt;%\n  \n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(2272)\n\ncat(\"Total Selected Crimes (Count):\", nrow(crime_sf), \"\\n\")\n\n\nTotal Selected Crimes (Count): 24184 \n\n\n\n\n1.2.2 Census data (tidycensus):\nTo isolate the true impact of transit ridership on crime, we must first account for the socio-economic context of the neighborhood. A bus stop in a distressed area faces different risks than one in a wealthy suburb.\nBy integrating 2023 ACS Census data, we control for structural disadvantages‚Äîsuch as poverty rates, unemployment, and housing vacancy.\n\n\nCode\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\", overwrite = TRUE, install = TRUE)\n\n\n\n\nCode\n# Load Census data for Philadelphia tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    poverty_pop = \"B17001_002\",\n    med_income = \"B19013_001\",\n    ba_degree = \"B15003_022\",\n    total_edu = \"B15003_001\",\n    labor_force = \"B23025_003\",\n    unemployed = \"B23025_005\",\n    total_housing = \"B25002_001\",\n    vacant_housing = \"B25002_003\"\n  ),\n  year = 2023, \n  state = \"PA\",\n  county = \"Philadelphia\",\n  geometry = TRUE,\n  output = \"wide\"\n) %&gt;%\n  st_transform(2272) %&gt;%\n  mutate(\n    Poverty_Rate = poverty_popE / total_popE,\n    Med_Income = med_incomeE,\n    ba_rate = 100 * ba_degreeE / total_eduE,\n    unemployment_rate = 100 * unemployedE / labor_forceE,\n    vacancy_rate = 100 * vacant_housingE / total_housingE\n  ) %&gt;%\n  dplyr::select(GEOID, Poverty_Rate, Med_Income, ba_rate, unemployment_rate, vacancy_rate)\n\n\n\n\n1.2.3 Spatial amenities\nWe ingest data from OpenDataPhilly to capture the built environment‚Äôs impact on safety. This includes Alcohol Outlets (potential crime generators), Street Lights (natural surveillance/visibility) and Police Stations (formal guardianship/deterrence).\n\nAlcohol Outlets\nStreet Poles\nPolice Stations\n\n\n\nCode\n# A. Alcohol Outlets (Crime Generators)\nalcohol_sf &lt;- read_csv(\"data/PHL_PLCB_geocoded.csv\") %&gt;%\n  filter(!is.na(lon) & !is.na(lat)) %&gt;%\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(2272)\n\n\n\n\nCode\n# B. Street Lights (Guardianship)\nlights_sf &lt;- read_csv(\"data/Street_Poles.csv\") %&gt;%\n  filter(!is.na(X) & !is.na(Y)) %&gt;%\n  st_as_sf(coords = c(\"X\", \"Y\"), crs = 3857) %&gt;%\n  st_transform(2272)\n\n\nProximity to police stations acts as a proxy for formal guardianship. We load these coordinates to later calculate the distance-based deterrence effect.\n\n\nCode\n# C. Police Stations (Guardianship) \npolice_sf &lt;- st_read(\"data/Police_Stations.geojson\", quiet = TRUE) %&gt;%\n  st_transform(2272) \n\ncat(\"Total Police Stations:\", nrow(police_sf))\n\n\nTotal Police Stations: 20"
  },
  {
    "objectID": "Final/Final_appendix.html#buffer-creation",
    "href": "Final/Final_appendix.html#buffer-creation",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "2.1 Buffer creation:",
    "text": "2.1 Buffer creation:\n\n2.1.1 400m buffer\n\n\nCode\n# Create Buffer(400m)\nbus_buffer &lt;- st_buffer(bus_long, 1312)\n\n\n\n\n2.1.2 Crime numbers\n\n\nCode\n# 1. Calculate total count\n# Calculate crime_sf data set covers how many weekdays and weekends\nday_counts &lt;- crime_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(is_crime_weekend) %&gt;%\n  summarise(n_days = n_distinct(crime_date)) \n\nprint(day_counts)\n\n\n# A tibble: 2 √ó 2\n  is_crime_weekend n_days\n             &lt;dbl&gt;  &lt;int&gt;\n1                0     65\n2                1     26\n\n\nCode\n# 2. Join and calculate daily crime Rate\ncrime_agg &lt;- st_join(bus_buffer, crime_sf, join = st_intersects) %&gt;%\n  filter(is_weekend == is_crime_weekend) %&gt;%\n  group_by(Stop_Code, is_weekend) %&gt;%\n  summarise(\n    Crime_Total_Count = n() \n  ) %&gt;%\n  st_drop_geometry() %&gt;%\n  \n  left_join(day_counts, by = c(\"is_weekend\" = \"is_crime_weekend\")) %&gt;%\n  mutate(\n    Crime_Daily_Rate = Crime_Total_Count / n_days,\n    \n    # Keep n_daysÔºåas negative binomial regresssion offset\n    Exposure_Days = n_days\n  )\n\nhead(crime_agg)\n\n\n# A tibble: 6 √ó 6\n# Groups:   Stop_Code [3]\n  Stop_Code is_weekend Crime_Total_Count n_days Crime_Daily_Rate Exposure_Days\n      &lt;dbl&gt;      &lt;dbl&gt;             &lt;int&gt;  &lt;int&gt;            &lt;dbl&gt;         &lt;int&gt;\n1         2          0                11     65           0.169             65\n2         2          1                 2     26           0.0769            26\n3         4          0                70     65           1.08              65\n4         4          1                26     26           1                 26\n5         5          0               102     65           1.57              65\n6         5          1                34     26           1.31              26\n\n\n\n\n2.1.3 Spatial Feature\nWe know that a dangerous corner doesn‚Äôt become safe overnight. To make our model fair, we must acknowledge that some stops have a history of trouble.\nWe calculated the crime counts for each stop from the previous season (Q1). We try to explain: ‚ÄòGiven how dangerous this spot usually is, does adding more bus riders make it safer or worse?‚Äô\n\n\nCode\nday_counts_q1 &lt;- crime_q1 %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(is_crime_weekend) %&gt;%\n  summarise(n_days_lag = n_distinct(crime_date)) \n\nprint(day_counts_q1)\n\n\n# A tibble: 2 √ó 2\n  is_crime_weekend n_days_lag\n             &lt;dbl&gt;      &lt;int&gt;\n1                0         65\n2                1         26\n\n\nCode\ncrime_lag_q1 &lt;- st_join(bus_buffer, crime_q1, join = st_intersects) %&gt;%\n  filter(is_weekend == is_crime_weekend) %&gt;%\n  group_by(Stop_Code, is_weekend) %&gt;%\n  summarise(\n    Crime_Total_Count_lag = n() \n  ) %&gt;%\n  st_drop_geometry() %&gt;%\n  left_join(day_counts_q1, by = c(\"is_weekend\" = \"is_crime_weekend\")) %&gt;%\n  mutate(\n    Crime_Daily_Rate_lag = Crime_Total_Count_lag / n_days_lag,\n    Exposure_Days_lag    = n_days_lag\n  )\n\nhead(crime_lag_q1)\n\n\n# A tibble: 6 √ó 6\n# Groups:   Stop_Code [3]\n  Stop_Code is_weekend Crime_Total_Count_lag n_days_lag Crime_Daily_Rate_lag\n      &lt;dbl&gt;      &lt;dbl&gt;                 &lt;int&gt;      &lt;int&gt;                &lt;dbl&gt;\n1         2          0                     5         65               0.0769\n2         2          1                     2         26               0.0769\n3         4          0                    63         65               0.969 \n4         4          1                    26         26               1     \n5         5          0                    52         65               0.8   \n6         5          1                    27         26               1.04  \n# ‚Ñπ 1 more variable: Exposure_Days_lag &lt;int&gt;\n\n\n\n\n2.1.3 Alcohol Numbers\nAlcohol outlets are established ‚Äòcrime generators.‚Äô We load their locations to model areas with higher potential for intoxication-related conflicts.\n\n\nCode\nalcohol_agg &lt;- st_join(bus_buffer, alcohol_sf, join = st_intersects) %&gt;%\n  group_by(Stop_Code, is_weekend) %&gt;%\n  summarise(Alcohol_Count = n() - 1) %&gt;% # Subtract 1 because st_join is left join (self-intersection NA check)\n  st_drop_geometry()\n\n\n\n\n2.1.4 Infrastructure Numbers\nStreet lighting is a key component of CPTED (Crime Prevention Through Environmental Design). We process the location of street poles to estimate visibility and natural surveillance levels.\n\n\nCode\nlight_agg &lt;- st_join(bus_buffer, lights_sf, join = st_intersects) %&gt;%\n  group_by(Stop_Code, is_weekend) %&gt;%\n  summarise(Light_Count = n() - 1) %&gt;%\n  st_drop_geometry()\n\n\n\n\n2.1.5 Census Demographics\nTo control for neighborhood context, we calculated the average:\n- Avg_Poverty: Rate of economic deprivation.\n- Avg_Income: Proxy for local wealth and resources.\n- Avg_BA: Educational attainment.\n- Avg_Unemployment: Measure of labor market instability.\n- Avg_Vacancy: Indicator of physical disorder.\n\n\nCode\n# Average Census Demographics\ncensus_agg &lt;- st_join(bus_buffer, philly_census, join = st_intersects) %&gt;%\n  group_by(Stop_Code, is_weekend) %&gt;%\n  summarise(\n    Avg_Poverty = mean(Poverty_Rate, na.rm = TRUE),\n    Avg_Income = mean(Med_Income, na.rm = TRUE),\n    Avg_BA = mean(ba_rate, na.rm = TRUE),\n    Avg_Unemployment = mean(unemployment_rate, na.rm = TRUE),\n    Avg_Vacancy = mean(vacancy_rate, na.rm = TRUE)\n  ) %&gt;%\n  st_drop_geometry()"
  },
  {
    "objectID": "Final/Final_appendix.html#k-nearest-neighbor-features",
    "href": "Final/Final_appendix.html#k-nearest-neighbor-features",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "2.2 k-Nearest Neighbor features:",
    "text": "2.2 k-Nearest Neighbor features:\n\n2.2.1 Police station (KNN-1)\n\n\nCode\n# Calculate distance to nearest police station\ndist_matrix &lt;- st_distance(bus_long, police_sf)\n\nbus_long$Dist_Police &lt;- apply(dist_matrix, 1, min) / 5280"
  },
  {
    "objectID": "Final/Final_appendix.html#get-police-service-area-psa-id-for-fixed-effects",
    "href": "Final/Final_appendix.html#get-police-service-area-psa-id-for-fixed-effects",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "2.3 Get Police Service Area (PSA) ID for Fixed Effects",
    "text": "2.3 Get Police Service Area (PSA) ID for Fixed Effects\n\n\nCode\npsa_sf &lt;- st_read(\"data/Boundaries_PSA.geojson\", quiet = TRUE) %&gt;%\n  st_transform(2272) %&gt;% \n  dplyr::select(PSA_ID = PSA_NUM) \n\n# Spatial Match: Define each bus stop to PSA\nstop_psa_mapping &lt;- bus_long %&gt;%\n  filter(is_weekend == 0) %&gt;% \n  st_join(psa_sf) %&gt;% # Spatial JoinÔºöPoint to Polygon\n  st_drop_geometry() %&gt;%\n  dplyr::select(Stop_Code, PSA_ID) %&gt;%\n  \n  distinct(Stop_Code, .keep_all = TRUE)\n\ncat(\"PSA mapping created for\", nrow(stop_psa_mapping), \"stops.\\n\")\n\n\nPSA mapping created for 5884 stops.\n\n\nCode\nhead(stop_psa_mapping)\n\n\n# A tibble: 6 √ó 2\n  Stop_Code PSA_ID\n      &lt;dbl&gt; &lt;chr&gt; \n1       759 033   \n2     16397 251   \n3     17921 351   \n4     16054 352   \n5     16102 221   \n6     16104 221"
  },
  {
    "objectID": "Final/Final_appendix.html#merge-features-into-master-dataset",
    "href": "Final/Final_appendix.html#merge-features-into-master-dataset",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "2.4 Merge Features into Master Dataset",
    "text": "2.4 Merge Features into Master Dataset\n\n\nCode\nfinal_data &lt;- bus_long %&gt;%\n  # Join all aggregated tables\n  left_join(crime_agg,     by = c(\"Stop_Code\", \"is_weekend\")) %&gt;%\n  left_join(crime_lag_q1,  by = c(\"Stop_Code\", \"is_weekend\")) %&gt;%\n  left_join(alcohol_agg,   by = c(\"Stop_Code\", \"is_weekend\")) %&gt;%\n  left_join(light_agg,     by = c(\"Stop_Code\", \"is_weekend\")) %&gt;%\n  left_join(census_agg,    by = c(\"Stop_Code\", \"is_weekend\")) %&gt;%\n  \n  left_join(stop_psa_mapping, by = \"Stop_Code\") %&gt;%\n  \n   mutate(\n    # 1. Handle NAs for Counts\n    Crime_Total_Count = replace_na(Crime_Total_Count, 0),\n    Crime_Daily_Rate  = replace_na(Crime_Daily_Rate, 0),\n\n    # Deal with NA in lag crime\n    Crime_Total_Count_lag = replace_na(Crime_Total_Count_lag, 0),\n    Crime_Daily_Rate_lag  = replace_na(Crime_Daily_Rate_lag, 0),\n\n    Alcohol_Count = replace_na(Alcohol_Count, 0),\n    Light_Count   = replace_na(Light_Count, 0),\n    \n    # 2. Log transform Ridership\n    Log_Ridership = log(Ridership + 1),\n    \n    # 3. Create Factor for Interaction Term\n    is_weekend_factor = factor(is_weekend, levels = c(0, 1), \n                               labels = c(\"Weekday\", \"Weekend\")),\n\n  ) %&gt;%\n  \n  # 4. Clean up\n  filter(!is.na(PSA_ID)) %&gt;%\n  na.omit() \n\ncat(\"Final Panel Dataset Rows:\", nrow(final_data))\n\n\nFinal Panel Dataset Rows: 11066"
  },
  {
    "objectID": "Final/Final_appendix.html#check-the-density-of-psa-data",
    "href": "Final/Final_appendix.html#check-the-density-of-psa-data",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "2.5 Check the density of PSA data",
    "text": "2.5 Check the density of PSA data\n\n\nCode\n# 1. Panel Data Rows\npsa_counts &lt;- final_data %&gt;%\n  group_by(PSA_ID) %&gt;%\n  summarise(\n    n_observations = n(),              \n    n_stops = n_distinct(Stop_Code)    \n  ) %&gt;%\n  arrange(n_observations)\n\nprint(head(psa_counts, 10))\n\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 2669882 ymin: 215531.6 xmax: 2715062 ymax: 279357.4\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n# A tibble: 10 √ó 4\n   PSA_ID n_observations n_stops                                        geometry\n   &lt;chr&gt;           &lt;int&gt;   &lt;int&gt;                   &lt;MULTIPOINT [US_survey_foot]&gt;\n 1 122                32      16 ((2670537 223892.4), (2670585 223932.8), (2670‚Ä¶\n 2 124                58      29 ((2671643 232878.2), (2672084 233106.1), (2672‚Ä¶\n 3 053                68      42 ((2670063 279330.9), (2670112 279357.4), (2670‚Ä¶\n 4 012                83      44 ((2685082 218564.9), (2685568 218997.3), (2686‚Ä¶\n 5 224                86      43 ((2686092 246848.8), (2686176 246409.7), (2686‚Ä¶\n 6 393                90      45 ((2688934 252812), (2688968 253070.2), (268922‚Ä¶\n 7 181                96      48 ((2669882 237256), (2669902 237214.7), (266998‚Ä¶\n 8 243                96      49 ((2706035 247307.4), (2706350 248302.6), (2706‚Ä¶\n 9 242                98      49 ((2702265 250424.1), (2702297 250520.3), (2702‚Ä¶\n10 052                99      50 ((2671211 266842), (2671303 266812.6), (267140‚Ä¶"
  },
  {
    "objectID": "Final/Final_appendix.html#distribution-of-crime-and-bus-stop-ridership-histogram",
    "href": "Final/Final_appendix.html#distribution-of-crime-and-bus-stop-ridership-histogram",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "3.1 Distribution of Crime and Bus Stop Ridership (histogram)",
    "text": "3.1 Distribution of Crime and Bus Stop Ridership (histogram)\nDoes ridership follow a normal distribution? No.¬†That‚Äôs why we need Negative Binomial.\n\n\nCode\nlibrary(gridExtra)\np1 &lt;- ggplot(final_data, aes(x = Ridership)) +\n  geom_histogram(fill = \"#3182bd\", bins = 50) +\n  labs(title = \"Distribution of Ridership\", x = \"Daily Boardings\") + plotTheme\n\np2 &lt;- ggplot(final_data, aes(x = Crime_Total_Count)) +\n  geom_histogram(fill = \"#de2d26\", bins = 50) +\n  labs(title = \"Distribution of Crime\", x = \"Crime Count (400m)\") + plotTheme\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\nThe histograms reveal that both Ridership (Left) and Crime Counts (Right) follow a highly right-skewed, ‚Äòlong-tail‚Äô distribution, rather than a normal bell curve. This extreme skewness mathematically confirm that a standard OLS Linear Regression would be biased.\nThis visual evidence creates a compelling mandate for using Negative Binomial Regression, which is specifically designed to handle such skewed count data.\n\n\nCode\nmean_crime &lt;- mean(final_data$Crime_Total_Count, na.rm = TRUE)\nvar_crime &lt;- var(final_data$Crime_Total_Count, na.rm = TRUE)\n\ncat(\"Mean:\", mean_crime, \"\\n\")\n\n\nMean: 30.91478 \n\n\nCode\ncat(\"Variance:\", var_crime, \"\\n\")\n\n\nVariance: 1190.485 \n\n\nCode\ncat(\"Ratio (Var/Mean):\", var_crime / mean_crime, \"\\n\")\n\n\nRatio (Var/Mean): 38.50861 \n\n\nWhile many bus stops have zero incidents, a few ‚Äòhotspots‚Äô have very high counts. This causes the variance to be much larger than the mean. To address this overdispersionÔºåwhich violates the core assumption of Poisson regressionÔºå we employed a Negative Binomial model. This approach allows us to model the data more accurately without inflating the significance of our findings."
  },
  {
    "objectID": "Final/Final_appendix.html#spatial-distribution-of-crime-and-bus-stop-ridershipmap",
    "href": "Final/Final_appendix.html#spatial-distribution-of-crime-and-bus-stop-ridershipmap",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "3.2 Spatial distribution of crime and Bus Stop Ridership(map)",
    "text": "3.2 Spatial distribution of crime and Bus Stop Ridership(map)\n\n\nCode\n# 1. Extract Coordinates\n\n# A. Bus Stop data\nbus_plot_data &lt;- bus_long %&gt;%\n  filter(is_weekend == 0) %&gt;%   \n  mutate(\n    X = st_coordinates(geometry)[,1],\n    Y = st_coordinates(geometry)[,2]\n  ) %&gt;%\n  st_drop_geometry() \n\n# B. Crime data\ncrime_plot_data &lt;- crime_sf %&gt;%\n  mutate(\n    X = st_coordinates(geometry)[,1],\n    Y = st_coordinates(geometry)[,2]\n  ) %&gt;%\n  st_drop_geometry()\n\n# 2. Create Maps\n\n# Left: Ridership Density\np_ridership &lt;- ggplot() +\n  geom_sf(data = philly_boundary, fill = \"#f5f5f5\", color = \"grey80\") +\n  \n  stat_density_2d(\n    data = bus_plot_data, \n    aes(x = X, y = Y, fill = ..level.., weight = Ridership), \n    geom = \"polygon\", \n    alpha = 0.75\n  ) +\n  \n  scale_fill_distiller(palette = \"Blues\", direction = 1, guide = \"none\") +\n  labs(\n    title = \"Weekday Ridership Hotspots\", \n    subtitle = \"High Transit Activity Zones\"\n  ) +\n  mapTheme\n\n# RightÔºöCrime Density\np_crime_map &lt;- ggplot() +\n  geom_sf(data = philly_boundary, fill = \"#f5f5f5\", color = \"grey80\") +\n  \n  stat_density_2d(\n    data = crime_plot_data, \n    aes(x = X, y = Y, fill = ..level..), \n    geom = \"polygon\", \n    alpha = 0.4,       \n    bins = 30,         \n    adjust = 0.5       \n  ) +\n  \n  scale_fill_distiller(palette = \"Reds\", direction = 1, guide = \"none\") +\n  labs(\n    title = \"Crime Hotspots\", \n    subtitle = \"High Incident Zones\"\n  ) +\n  mapTheme\n\n# 3. Combine Side-by-Side\n\ncombined_map &lt;- p_ridership + p_crime_map +\n  plot_annotation(\n    title = \"Spatial Mismatch Analysis: Eyes on the Street vs. Targets?\",\n    subtitle = \"Left: Where people are (Ridership) | Right: Where crimes happen\",\n    theme = theme(\n      plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = 12, color = \"grey40\", hjust = 0.5)\n    )\n  )\n\ncombined_map\n\n\n\n\n\n\n\n\n\nTransit activity is anchored in Center City and radiates outward along the major arterials (Broad St.and Market St.). However, crime exhibits a polycentric distribution. Beyond the city center, we observe significant high-crime clusters in North and West Philadelphia (Kensington/Allegheny).\nThis spatial mismatch suggests that ridership volume alone cannot explain crime risk. While the downtown core attracts crime due to sheer foot traffic (‚ÄòTargets‚Äô), the peripheral hotspots are likely driven by other environmental factors‚Äîsuch as socioeconomic disadvantage or the presence of crime generators (e.g., alcohol outlets)‚Äîrather than transit volume alone."
  },
  {
    "objectID": "Final/Final_appendix.html#crime-vs.-bus-stop-ridership-scatter-plots",
    "href": "Final/Final_appendix.html#crime-vs.-bus-stop-ridership-scatter-plots",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "3.3 Crime vs.¬†Bus Stop Ridership (scatter plots)",
    "text": "3.3 Crime vs.¬†Bus Stop Ridership (scatter plots)\n\n\nCode\nbar_plot_data &lt;- final_data %&gt;%\n  st_drop_geometry() %&gt;% \n  group_by(is_weekend_factor) %&gt;% \n  summarise(\n    Avg_Ridership = mean(Ridership, na.rm = TRUE), \n    Avg_Crime_count = mean(Crime_Daily_Rate, na.rm = TRUE)\n  ) %&gt;%\n  # Long\n  pivot_longer(\n    cols = c(Avg_Ridership, Avg_Crime_count),\n    names_to = \"Metric\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(\n    Metric_Label = case_when(\n      Metric == \"Avg_Ridership\" ~ \"Average Ridership\",\n      Metric == \"Avg_Crime_count\" ~ \"Average Crime Count\"\n    )\n  )\n\nggplot(bar_plot_data, aes(x = is_weekend_factor, y = Value, fill = is_weekend_factor)) +\n  geom_col(width = 0.6, alpha = 0.9) +\n  \n  facet_wrap(~Metric_Label, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#3182bd\", \"Weekend\" = \"#de2d26\")) +\n  \n  labs(\n    title = \"Volume vs. Risk: Weekday vs. Weekend\",\n    subtitle = \"Comparison of Average  Ridership and Crime per Stop\",\n    x = \"\", \n    y = \"Average Count\",\n    fill = \"Time Period\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 10, color = \"grey40\", hjust = 0.5),\n    strip.text = element_text(size = 12, face = \"bold\"), \n    axis.text.x = element_text(size = 11, face = \"bold\"),\n    legend.position = \"none\" # \n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Crime vs. Ridership (Interaction Plot)\n\nggplot(final_data, aes(x = Log_Ridership, y = Crime_Daily_Rate, color = is_weekend_factor)) +\n  geom_point(alpha = 0.1, size = 1) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"quasipoisson\"), se = TRUE) +\n  scale_color_manual(values = c(\"Weekday\" = \"#3182bd\", \"Weekend\" = \"#de2d26\")) +\n  labs(\n    title = \"Does Ridership impact Crime differently on Weekends?\",\n    subtitle = \"Interaction Effect (Normalized by Number of Days)\",\n    x = \"Log(Daily Ridership)\",\n    y = \"Average Daily Crime Count (per 400m)\", \n    color = \"Time Period\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\n\nCrime Follows the Crowd (Bar Chart):\nThe left bar charts show that crime volume drops significantly on weekends, mirroring the drop in ridership shown in the right chart. This confirms that crime is likely driven by opportunity‚Äîfewer people on the street simply means fewer targets and fewer conflicts.\nThe Risk ‚ÄúConversion Rate‚Äù is Constant (Scatter Plot):\nThe interaction plot reveals a consistent positive correlation between ridership and crime counts across both weekdays and weekends. While the slopes exhibit only minor differences, the methodological value of this split is significant. By comparing the same bus stops during different time periods (Weekday vs.¬†Weekend), we inherently control for static environmental factors, such as poverty rates, street lighting, and proximity to alcohol outlets, which remain constant regardless of the day.\nConsequently, this acts as a quasi-experimental control: since the physical environment is fixed, the persistent upward trend suggests that ridership itself is a direct driver of crime risk. This supports the ‚ÄòTargets‚Äô hypothesis over the ‚ÄòEyes on the Street‚Äô theory, indicating that higher passenger volumes attract opportunistic crime regardless of the temporal context."
  },
  {
    "objectID": "Final/Final_appendix.html#crime-vs.-spatial-social-features-scatter-plots",
    "href": "Final/Final_appendix.html#crime-vs.-spatial-social-features-scatter-plots",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "3.4 Crime vs.¬†Spatial & Social features (scatter plots)",
    "text": "3.4 Crime vs.¬†Spatial & Social features (scatter plots)\n\n\nCode\nlibrary(patchwork)\nlibrary(scales)\n\nplot_data &lt;- final_data %&gt;%\n  mutate(\n    log_crime = log(Crime_Daily_Rate + 0.01), # +0.01 to avoid log(0)\n    log_income = log(Avg_Income + 1)\n  )\n\n# --- 1. POI & Infrastructure (Built Environment) ---\n\n# A. Alcohol Outlets\np1 &lt;- ggplot(plot_data, aes(x = Alcohol_Count, y = log_crime)) +\n  geom_point(alpha = 0.1, color = \"#6A1B9A\") +\n  geom_smooth(method = \"loess\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Crime) vs. Alcohol Outlets\",\n       subtitle = \"Check for diminishing returns\",\n       x = \"Count of Alcohol Outlets\", y = \"Log(Crime Count)\") +\n  plotTheme\n\n# B. Street Lights\np2 &lt;- ggplot(plot_data, aes(x = Light_Count, y = log_crime)) +\n  geom_point(alpha = 0.1, color = \"#6A1B9A\") +\n  geom_smooth(method = \"loess\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Crime) vs. Street Lights\",\n       subtitle = \"Is the relationship linear?\",\n       x = \"Count of Street Lights\", y = \"\") +\n  plotTheme\n\n# C. Distance to Police\np3 &lt;- ggplot(plot_data, aes(x = Dist_Police, y = log_crime)) +\n  geom_point(alpha = 0.1, color = \"#6A1B9A\") +\n  geom_smooth(method = \"loess\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Crime) vs. Dist to Police\",\n       subtitle = \"Check for U-shape or threshold\",\n       x = \"Distance to Station (Miles)\", y = \"\") +\n  plotTheme\n\n# --- 2. Demographics (Census) ---\n\n# D. Poverty Rate \np4 &lt;- ggplot(plot_data, aes(x = Avg_Poverty, y = log_crime)) +\n  geom_point(alpha = 0.1, color = \"#3182bd\") +\n  geom_smooth(method = \"loess\", color = \"orange\", se = FALSE) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(title = \"Log(Crime) vs. Poverty Rate\",\n       x = \"Poverty Rate\", y = \"Log(Crime Count)\") +\n  plotTheme\n\n# E. Median Income\np5 &lt;- ggplot(plot_data, aes(x = Avg_Income, y = log_crime)) +\n  geom_point(alpha = 0.1, color = \"#3182bd\") +\n  geom_smooth(method = \"loess\", color = \"orange\", se = FALSE) +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Log(Crime) vs. Median Income\",\n       subtitle = \"Does wealth shield against crime?\",\n       x = \"Median Household Income\", y = \"\") +\n  plotTheme\n\n# F. Vacancy Rate\np6 &lt;- ggplot(plot_data, aes(x = Avg_Vacancy, y = log_crime)) +\n  geom_point(alpha = 0.1, color = \"#3182bd\") +\n  geom_smooth(method = \"loess\", color = \"orange\", se = FALSE) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(title = \"Log(Crime) vs. Vacancy Rate\",\n       x = \"Housing Vacancy Rate\", y = \"\") +\n  plotTheme\n\n# --- Combine Plots ---\n(p1 | p2 | p3) / (p4 | p5 | p6) +\n  plot_annotation(\n    title = \"Exploratory Analysis: Variable Functional Forms\",\n    subtitle = \"Red Line = Loess Smoother (Non-linear trend)\",\n    theme = theme(plot.title = element_text(size = 16, face = \"bold\"))\n  )\n\n\n\n\n\n\n\n\n\nBefore finalizing our model, we conducted an extensive exploratory analysis, testing relationships across multiple potential independent variables and their non-linear transformations. While we only present the variables with the most significant statistical relationships and policy implications here, this rigorous process of trial and error was essential to ensure the robustness of our final model. The charts above represent the distilled ‚Äòrisk signals‚Äô identified from this comprehensive screening.\n\nAlcohol Outlets: The plot shows that crime goes up quickly with the first few alcohol stores. However, as the number of stores gets very high, the line flattens out (Diminishing marginal returns). We will try using a log transformation for this feature. This tells the model that the first few stores have a bigger impact than the later ones.\nStreet Lights: The red trend line is not straight. It goes up as lights increase (usually in busy areas), but then it curves. In later this study, we used a polynomial (2nd degree) term. This allows the model to draw a curved line instead of a straight one to fit the data better.\nDistance to Police: The line looks mostly straight and goes down slightly, so we kept this variable linear.\nPoverty Rate: As poverty goes up, crime generally goes up. The line is mostly straight. We also kept it as a standard linear variable.\nMedian Income: The line looks a bit like a ‚ÄúU‚Äù shape. Crime is high in low-income areas, drops in middle-income areas, and rises slightly or stays flat in higher-income areas. We added a squared term (^2). This helps the model understand that crime can be high at both low and high income levels, but low in the middle.\nVacancy Rate: The line looks like an upside-down ‚ÄúU‚Äù. Crime goes up as vacancy increases, but when vacancy gets very high (empty neighborhoods), crime actually drops. We also added a squared term (^2) for this."
  },
  {
    "objectID": "Final/Final_appendix.html#correlation-matrix-of-features-for-all-features",
    "href": "Final/Final_appendix.html#correlation-matrix-of-features-for-all-features",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "3.5 Correlation Matrix of Features for all Features",
    "text": "3.5 Correlation Matrix of Features for all Features\n\n\nCode\nlibrary(ggcorrplot)\n\nnumeric_vars &lt;- final_data %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    Crime_Daily_Rate, \n    Ridership, \n    Alcohol_Count, \n    Light_Count, \n    Dist_Police, \n    Avg_Poverty, \n    Avg_Income, \n    Avg_Unemployment,\n    Avg_Vacancy\n  )\n\ncorr_matrix &lt;- cor(numeric_vars, use = \"complete.obs\")\n\nggcorrplot(\n  corr_matrix, \n  method = \"square\", \n  type = \"lower\", \n  lab = TRUE, \n  lab_size = 3, \n  colors = c(\"#6D9EC1\", \"white\", \"#E46726\"), \n  title = \"Correlation Matrix of Features\",\n  ggtheme = theme_minimal()\n)\n\n\n\n\n\n\n\n\n\n\nThe correlation matrix reveals a critical issue of multicollinearity among the socioeconomic predictors. Specifically, Average Poverty Rate (Avg_Poverty) and Median Household Income (Avg_Income) exhibit a strong negative correlation of -0.81. Additionally, Unemployment Rate (Avg_Unemployment) shows a strong negative correlation with Income (-0.70) and a moderately high positive correlation with Poverty (0.66).\nThese high coefficients suggest that Poverty, Income, and Unemployment are capturing overlapping dimensions of neighborhood socioeconomic status. While this flags a risk of multicollinearity, correlation coefficients alone are insufficient for removing variables. Therefore, we will proceed to calculate the Variance Inflation Factor (VIF) in the next step to rigorously quantify the severity of multicollinearity and determine the appropriate strategy for feature selection or transformation."
  },
  {
    "objectID": "Final/Final_appendix.html#crime-distribution-in-weekdays-and-weekends",
    "href": "Final/Final_appendix.html#crime-distribution-in-weekdays-and-weekends",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "3.6 Crime distribution in weekdays and weekends",
    "text": "3.6 Crime distribution in weekdays and weekends\n\n\nCode\nggplot(final_data, aes(x = is_weekend_factor, y = Crime_Daily_Rate, fill = is_weekend_factor)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) + \n  \n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3, fill = \"white\") +\n  \n  coord_cartesian(ylim = c(0, quantile(final_data$Crime_Daily_Rate, 0.95))) +\n  \n  scale_fill_manual(values = c(\"Weekday\" = \"#3182bd\", \"Weekend\" = \"#de2d26\")) +\n  \n  labs(\n    title = \"Daily Crime Risk: Weekday vs. Weekend\",\n    subtitle = \"Comparison of Average Daily Crime Counts per Stop\",\n    x = \"Time Period\",\n    y = \"Average Daily Crime Count (per 400m)\", \n    caption = \"Note: Values represent daily averages to account for fewer weekend days per year.\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\n\nStability of Crime Volume: The means (indicated by white diamonds) and medians (solid black lines) for both weekdays and weekends are nearly identical, appearing at approximately the same daily average level (~0.6 incidents). This pattern suggests that the aggregate demand for public safety resources at these bus stops does not fluctuate significantly throughout the week.\nJustification for Pseudo-Panel ModelingÔºö While this chart shows crime counts are constant, we know that generally ridership volume typically drops on weekends. If the outcome (crime) remains the same while the input (ridership) decreases, it implies that the risk intensity per rider is likely higher on weekends. Constructing a pseudo-panel allows us to mathematically capture this changing elasticity, rather than averaging it out.\n\nBy disaggregating the data into weekday and weekend observations for each stop, we create a pseudo-panel structure that acts as a natural control. Since the physical environment (lighting, poverty, location) remains fixed for a specific stop, splitting the data allows our model to isolate ridership as the primary changing variable, thereby enabling a more robust causal inference about how passenger flows specifically impact crime rates under different density conditions."
  },
  {
    "objectID": "Final/Final_appendix.html#ridership-only",
    "href": "Final/Final_appendix.html#ridership-only",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "4.1 Ridership only:",
    "text": "4.1 Ridership only:\n\n\nCode\nmodel_1 &lt;- glm.nb(Crime_Total_Count ~ \n                    Log_Ridership + \n                    offset(log(Exposure_Days)), \n                  data = final_data)\nsummary(model_1)\n\n\n\nCall:\nglm.nb(formula = Crime_Total_Count ~ Log_Ridership + offset(log(Exposure_Days)), \n    data = final_data, init.theta = 1.962937689, link = log)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -1.328484   0.018925  -70.20   &lt;2e-16 ***\nLog_Ridership  0.243197   0.004944   49.19   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.9629) family taken to be 1)\n\n    Null deviance: 14335  on 11065  degrees of freedom\nResidual deviance: 11852  on 11064  degrees of freedom\nAIC: 92538\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.9629 \n          Std. Err.:  0.0274 \n\n 2 x log-likelihood:  -92531.6570 \n\n\n\nThis baseline model establishes the fundamental relationship between transit ridership and crime counts without any confounding factors. It serves as a benchmark to test the raw association. specifically, whether higher passenger volume acts as a ‚Äúcrime generator‚Äù (providing more potential targets) or a deterrent (providing ‚Äúeyes on the street‚Äù). The offset term ensures we are modeling the rate of crime per day, accounting for differences in exposure periods."
  },
  {
    "objectID": "Final/Final_appendix.html#the-interaction",
    "href": "Final/Final_appendix.html#the-interaction",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "4.2 The Interaction",
    "text": "4.2 The Interaction\n\n\nCode\nmodel_2 &lt;- glm.nb(Crime_Total_Count ~ \n                    Log_Ridership * is_weekend_factor + \n                    offset(log(Exposure_Days)), \n                  data = final_data) \nsummary(model_2)\n\n\n\nCall:\nglm.nb(formula = Crime_Total_Count ~ Log_Ridership * is_weekend_factor + \n    offset(log(Exposure_Days)), data = final_data, init.theta = 1.968491375, \n    link = log)\n\nCoefficients:\n                                        Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                            -1.434242   0.027417 -52.311  &lt; 2e-16\nLog_Ridership                           0.263192   0.006798  38.716  &lt; 2e-16\nis_weekend_factorWeekend                0.186359   0.038085   4.893 9.92e-07\nLog_Ridership:is_weekend_factorWeekend -0.034408   0.010040  -3.427  0.00061\n                                          \n(Intercept)                            ***\nLog_Ridership                          ***\nis_weekend_factorWeekend               ***\nLog_Ridership:is_weekend_factorWeekend ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.9685) family taken to be 1)\n\n    Null deviance: 14371  on 11065  degrees of freedom\nResidual deviance: 11849  on 11062  degrees of freedom\nAIC: 92509\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.9685 \n          Std. Err.:  0.0275 \n\n 2 x log-likelihood:  -92498.9340 \n\n\n\nIn this step, we introduce the interaction term (Log_Ridership * is_weekend_factor) to test our core hypothesis: does the impact of ridership on crime change fundamentally during weekends? This specification allows the model to differentiate the ‚Äúcommuter effect‚Äù from potentially riskier weekend dynamics. It helps us determine if a specific bus stop becomes more dangerous per rider on weekends, even if the total volume of passengers decreases."
  },
  {
    "objectID": "Final/Final_appendix.html#built-environment-and-demographics-features",
    "href": "Final/Final_appendix.html#built-environment-and-demographics-features",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "4.3 Built Environment and Demographics features:",
    "text": "4.3 Built Environment and Demographics features:\n\n\nCode\nmodel_3 &lt;- glm.nb(Crime_Total_Count ~ \n                    Log_Ridership * is_weekend_factor + \n                    log(Alcohol_Count + 1) +        \n                    poly(Light_Count, 2) +          \n                    Avg_Poverty +         \n                    Avg_Vacancy + I(Avg_Vacancy^2) +       \n                    Avg_Income + I(Avg_Income^2)  +\n                    Avg_Unemployment +\n                    Dist_Police +\n                    offset(log(Exposure_Days)), \n                  data = final_data)\nsummary(model_3)\n\n\n\nCall:\nglm.nb(formula = Crime_Total_Count ~ Log_Ridership * is_weekend_factor + \n    log(Alcohol_Count + 1) + poly(Light_Count, 2) + Avg_Poverty + \n    Avg_Vacancy + I(Avg_Vacancy^2) + Avg_Income + I(Avg_Income^2) + \n    Avg_Unemployment + Dist_Police + offset(log(Exposure_Days)), \n    data = final_data, init.theta = 3.809131636, link = log)\n\nCoefficients:\n                                         Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                            -8.603e-01  8.710e-02  -9.877  &lt; 2e-16\nLog_Ridership                           1.102e-01  5.485e-03  20.086  &lt; 2e-16\nis_weekend_factorWeekend                4.330e-02  2.965e-02   1.460  0.14416\nlog(Alcohol_Count + 1)                  2.836e-01  7.490e-03  37.869  &lt; 2e-16\npoly(Light_Count, 2)1                   2.074e+01  9.830e-01  21.094  &lt; 2e-16\npoly(Light_Count, 2)2                  -2.401e+00  6.208e-01  -3.867  0.00011\nAvg_Poverty                             1.310e+00  1.157e-01  11.325  &lt; 2e-16\nAvg_Vacancy                             5.836e-03  3.003e-03   1.943  0.05197\nI(Avg_Vacancy^2)                       -1.034e-04  8.434e-05  -1.226  0.22030\nAvg_Income                             -1.162e-05  1.435e-06  -8.097 5.64e-16\nI(Avg_Income^2)                         5.343e-11  7.797e-12   6.852 7.27e-12\nAvg_Unemployment                       -1.235e-02  1.879e-03  -6.575 4.87e-11\nDist_Police                            -1.986e-01  1.149e-02 -17.278  &lt; 2e-16\nLog_Ridership:is_weekend_factorWeekend -2.319e-02  7.718e-03  -3.004  0.00266\n                                          \n(Intercept)                            ***\nLog_Ridership                          ***\nis_weekend_factorWeekend                  \nlog(Alcohol_Count + 1)                 ***\npoly(Light_Count, 2)1                  ***\npoly(Light_Count, 2)2                  ***\nAvg_Poverty                            ***\nAvg_Vacancy                            .  \nI(Avg_Vacancy^2)                          \nAvg_Income                             ***\nI(Avg_Income^2)                        ***\nAvg_Unemployment                       ***\nDist_Police                            ***\nLog_Ridership:is_weekend_factorWeekend ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(3.8091) family taken to be 1)\n\n    Null deviance: 25397  on 11065  degrees of freedom\nResidual deviance: 11556  on 11052  degrees of freedom\nAIC: 85737\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  3.8091 \n          Std. Err.:  0.0607 \n\n 2 x log-likelihood:  -85706.8380 \n\n\n\nWe incorporate criminogenic generators (alcohol outlets), potential guardians (street lights), and socioeconomic indicators (income, vacancy, and poverty).\n\nBy adjusting for these factors, this model rigorously tests whether ridership has a unique impact on crime, or if high-ridership stops simply happen to be located in disadvantaged or commercially dense neighborhoods. This step is crucial for isolating the specific effect of the transit network from broader environmental conditions."
  },
  {
    "objectID": "Final/Final_appendix.html#spatial-fixed-effect",
    "href": "Final/Final_appendix.html#spatial-fixed-effect",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "4.4 Spatial Fixed Effect",
    "text": "4.4 Spatial Fixed Effect\n\n\nCode\nmodel_4 &lt;- glm.nb(Crime_Total_Count ~ \n                    Log_Ridership * is_weekend_factor + \n                    log(Alcohol_Count + 1) +       \n                    poly(Light_Count, 2) +        \n                    Avg_Poverty +        \n                    Avg_Vacancy + I(Avg_Vacancy^2) +      \n                    Avg_Income + I(Avg_Income^2)  +\n                    Avg_Unemployment +\n                    Dist_Police +         \n                    factor(PSA_ID) +    \n                    offset(log(Exposure_Days)), \n                  data = final_data,\n                  control = glm.control(maxit = 100))\nsummary(model_4)\n\n\n\nCall:\nglm.nb(formula = Crime_Total_Count ~ Log_Ridership * is_weekend_factor + \n    log(Alcohol_Count + 1) + poly(Light_Count, 2) + Avg_Poverty + \n    Avg_Vacancy + I(Avg_Vacancy^2) + Avg_Income + I(Avg_Income^2) + \n    Avg_Unemployment + Dist_Police + factor(PSA_ID) + offset(log(Exposure_Days)), \n    data = final_data, control = glm.control(maxit = 100), init.theta = 4.730969316, \n    link = log)\n\nCoefficients:\n                                         Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                            -4.058e-01  1.071e-01  -3.789 0.000151\nLog_Ridership                           9.525e-02  5.201e-03  18.314  &lt; 2e-16\nis_weekend_factorWeekend                2.432e-05  2.758e-02   0.001 0.999296\nlog(Alcohol_Count + 1)                  2.423e-01  8.067e-03  30.036  &lt; 2e-16\npoly(Light_Count, 2)1                   2.468e+01  1.203e+00  20.510  &lt; 2e-16\npoly(Light_Count, 2)2                  -5.178e+00  7.333e-01  -7.062 1.64e-12\nAvg_Poverty                             8.036e-01  1.364e-01   5.891 3.83e-09\nAvg_Vacancy                             9.710e-03  4.093e-03   2.372 0.017672\nI(Avg_Vacancy^2)                       -1.480e-04  9.976e-05  -1.484 0.137899\nAvg_Income                             -1.750e-05  1.684e-06 -10.396  &lt; 2e-16\nI(Avg_Income^2)                         8.873e-11  8.819e-12  10.061  &lt; 2e-16\nAvg_Unemployment                       -1.928e-02  2.160e-03  -8.924  &lt; 2e-16\nDist_Police                            -2.755e-01  1.534e-02 -17.954  &lt; 2e-16\nfactor(PSA_ID)012                       3.774e-02  7.062e-02   0.534 0.593118\nfactor(PSA_ID)021                       5.770e-01  5.888e-02   9.800  &lt; 2e-16\nfactor(PSA_ID)022                       4.204e-01  6.138e-02   6.848 7.49e-12\nfactor(PSA_ID)023                       2.167e-01  5.230e-02   4.144 3.42e-05\nfactor(PSA_ID)031                      -2.512e-01  5.587e-02  -4.496 6.92e-06\nfactor(PSA_ID)032                       1.861e-01  5.190e-02   3.586 0.000336\nfactor(PSA_ID)033                      -1.804e-01  4.656e-02  -3.874 0.000107\nfactor(PSA_ID)051                      -1.158e-01  6.476e-02  -1.788 0.073805\nfactor(PSA_ID)052                      -3.001e-01  7.173e-02  -4.184 2.87e-05\nfactor(PSA_ID)053                      -3.824e-01  8.832e-02  -4.330 1.49e-05\nfactor(PSA_ID)071                       3.128e-01  6.760e-02   4.627 3.71e-06\nfactor(PSA_ID)072                      -2.708e-01  6.293e-02  -4.303 1.69e-05\nfactor(PSA_ID)073                      -3.697e-01  6.517e-02  -5.672 1.41e-08\nfactor(PSA_ID)081                       4.798e-01  6.006e-02   7.989 1.36e-15\nfactor(PSA_ID)082                      -1.388e-01  5.780e-02  -2.402 0.016302\nfactor(PSA_ID)083                      -4.606e-03  6.210e-02  -0.074 0.940871\nfactor(PSA_ID)091                       1.973e-01  5.179e-02   3.811 0.000139\nfactor(PSA_ID)092                       1.249e-01  5.629e-02   2.219 0.026498\nfactor(PSA_ID)093                      -2.243e-02  5.251e-02  -0.427 0.669180\nfactor(PSA_ID)094                       4.995e-01  5.413e-02   9.229  &lt; 2e-16\nfactor(PSA_ID)095                      -1.978e-01  5.810e-02  -3.404 0.000663\nfactor(PSA_ID)121                       4.666e-01  6.270e-02   7.443 9.85e-14\nfactor(PSA_ID)122                      -1.427e-01  1.038e-01  -1.375 0.169163\nfactor(PSA_ID)123                       8.914e-02  6.188e-02   1.441 0.149665\nfactor(PSA_ID)124                       2.368e-01  7.720e-02   3.067 0.002159\nfactor(PSA_ID)141                      -3.889e-02  5.571e-02  -0.698 0.485112\nfactor(PSA_ID)142                      -1.625e-01  6.301e-02  -2.579 0.009916\nfactor(PSA_ID)143                       1.960e-01  5.740e-02   3.415 0.000637\nfactor(PSA_ID)144                      -1.241e-02  6.515e-02  -0.190 0.848975\nfactor(PSA_ID)151                       1.524e-01  5.272e-02   2.891 0.003845\nfactor(PSA_ID)152                       4.628e-01  5.253e-02   8.810  &lt; 2e-16\nfactor(PSA_ID)153                       5.232e-01  5.387e-02   9.713  &lt; 2e-16\nfactor(PSA_ID)161                      -3.663e-02  5.725e-02  -0.640 0.522209\nfactor(PSA_ID)162                      -5.668e-02  5.983e-02  -0.947 0.343485\nfactor(PSA_ID)171                      -1.610e-01  5.428e-02  -2.965 0.003022\nfactor(PSA_ID)172                      -5.680e-01  6.898e-02  -8.234  &lt; 2e-16\nfactor(PSA_ID)173                      -3.975e-01  5.909e-02  -6.727 1.73e-11\nfactor(PSA_ID)181                      -1.636e-01  6.676e-02  -2.451 0.014265\nfactor(PSA_ID)182                       3.990e-02  5.880e-02   0.679 0.497423\nfactor(PSA_ID)183                      -1.393e-01  5.350e-02  -2.603 0.009229\nfactor(PSA_ID)191                       1.965e-01  5.347e-02   3.674 0.000238\nfactor(PSA_ID)192                       1.749e-02  5.921e-02   0.295 0.767637\nfactor(PSA_ID)193                       2.983e-01  5.720e-02   5.215 1.84e-07\nfactor(PSA_ID)221                       2.063e-01  5.666e-02   3.642 0.000271\nfactor(PSA_ID)222                      -1.035e-02  6.510e-02  -0.159 0.873735\nfactor(PSA_ID)223                       2.647e-01  5.970e-02   4.434 9.23e-06\nfactor(PSA_ID)224                       1.685e-01  6.918e-02   2.435 0.014876\nfactor(PSA_ID)241                       1.121e-01  6.299e-02   1.780 0.075073\nfactor(PSA_ID)242                       3.243e-01  6.764e-02   4.795 1.63e-06\nfactor(PSA_ID)243                       8.122e-02  6.635e-02   1.224 0.220954\nfactor(PSA_ID)251                       1.819e-01  6.727e-02   2.704 0.006846\nfactor(PSA_ID)252                      -2.377e-01  6.457e-02  -3.682 0.000232\nfactor(PSA_ID)253                      -3.656e-01  6.789e-02  -5.385 7.25e-08\nfactor(PSA_ID)254                      -5.605e-02  6.495e-02  -0.863 0.388107\nfactor(PSA_ID)261                      -1.929e-01  6.498e-02  -2.969 0.002990\nfactor(PSA_ID)262                       1.561e-01  5.853e-02   2.667 0.007655\nfactor(PSA_ID)263                      -9.630e-02  5.027e-02  -1.916 0.055399\nfactor(PSA_ID)351                      -3.793e-02  5.096e-02  -0.744 0.456679\nfactor(PSA_ID)352                       6.887e-02  5.383e-02   1.279 0.200771\nfactor(PSA_ID)353                      -2.201e-02  5.796e-02  -0.380 0.704186\nfactor(PSA_ID)391                       1.119e-02  5.277e-02   0.212 0.832001\nfactor(PSA_ID)392                      -1.532e-01  5.705e-02  -2.686 0.007233\nfactor(PSA_ID)393                       1.888e-01  6.928e-02   2.725 0.006431\nLog_Ridership:is_weekend_factorWeekend -1.720e-02  7.148e-03  -2.406 0.016114\n                                          \n(Intercept)                            ***\nLog_Ridership                          ***\nis_weekend_factorWeekend                  \nlog(Alcohol_Count + 1)                 ***\npoly(Light_Count, 2)1                  ***\npoly(Light_Count, 2)2                  ***\nAvg_Poverty                            ***\nAvg_Vacancy                            *  \nI(Avg_Vacancy^2)                          \nAvg_Income                             ***\nI(Avg_Income^2)                        ***\nAvg_Unemployment                       ***\nDist_Police                            ***\nfactor(PSA_ID)012                         \nfactor(PSA_ID)021                      ***\nfactor(PSA_ID)022                      ***\nfactor(PSA_ID)023                      ***\nfactor(PSA_ID)031                      ***\nfactor(PSA_ID)032                      ***\nfactor(PSA_ID)033                      ***\nfactor(PSA_ID)051                      .  \nfactor(PSA_ID)052                      ***\nfactor(PSA_ID)053                      ***\nfactor(PSA_ID)071                      ***\nfactor(PSA_ID)072                      ***\nfactor(PSA_ID)073                      ***\nfactor(PSA_ID)081                      ***\nfactor(PSA_ID)082                      *  \nfactor(PSA_ID)083                         \nfactor(PSA_ID)091                      ***\nfactor(PSA_ID)092                      *  \nfactor(PSA_ID)093                         \nfactor(PSA_ID)094                      ***\nfactor(PSA_ID)095                      ***\nfactor(PSA_ID)121                      ***\nfactor(PSA_ID)122                         \nfactor(PSA_ID)123                         \nfactor(PSA_ID)124                      ** \nfactor(PSA_ID)141                         \nfactor(PSA_ID)142                      ** \nfactor(PSA_ID)143                      ***\nfactor(PSA_ID)144                         \nfactor(PSA_ID)151                      ** \nfactor(PSA_ID)152                      ***\nfactor(PSA_ID)153                      ***\nfactor(PSA_ID)161                         \nfactor(PSA_ID)162                         \nfactor(PSA_ID)171                      ** \nfactor(PSA_ID)172                      ***\nfactor(PSA_ID)173                      ***\nfactor(PSA_ID)181                      *  \nfactor(PSA_ID)182                         \nfactor(PSA_ID)183                      ** \nfactor(PSA_ID)191                      ***\nfactor(PSA_ID)192                         \nfactor(PSA_ID)193                      ***\nfactor(PSA_ID)221                      ***\nfactor(PSA_ID)222                         \nfactor(PSA_ID)223                      ***\nfactor(PSA_ID)224                      *  \nfactor(PSA_ID)241                      .  \nfactor(PSA_ID)242                      ***\nfactor(PSA_ID)243                         \nfactor(PSA_ID)251                      ** \nfactor(PSA_ID)252                      ***\nfactor(PSA_ID)253                      ***\nfactor(PSA_ID)254                         \nfactor(PSA_ID)261                      ** \nfactor(PSA_ID)262                      ** \nfactor(PSA_ID)263                      .  \nfactor(PSA_ID)351                         \nfactor(PSA_ID)352                         \nfactor(PSA_ID)353                         \nfactor(PSA_ID)391                         \nfactor(PSA_ID)392                      ** \nfactor(PSA_ID)393                      ** \nLog_Ridership:is_weekend_factorWeekend *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(4.731) family taken to be 1)\n\n    Null deviance: 30304  on 11065  degrees of freedom\nResidual deviance: 11532  on 10989  degrees of freedom\nAIC: 83893\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  4.7310 \n          Std. Err.:  0.0792 \n\n 2 x log-likelihood:  -83736.6850 \n\n\n\nIn this robust specification, we introduce Police Service Area (PSA) fixed effects to control for unobserved spatial heterogeneity. Unlike census tracts, PSAs are directly relevant to law enforcement strategy, allowing the model to account for differences in patrolling intensity and reporting practices across different police districts. This approach not only improves model fit but also resolves potential overfitting issues associated with smaller spatial units."
  },
  {
    "objectID": "Final/Final_appendix.html#crime-temporal-lag",
    "href": "Final/Final_appendix.html#crime-temporal-lag",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "4.5 Crime Temporal Lag",
    "text": "4.5 Crime Temporal Lag\n\n\nCode\nmodel_5 &lt;- glm.nb(Crime_Total_Count ~ \n                    Log_Ridership * is_weekend_factor + \n                    log(Alcohol_Count + 1) +       \n                    poly(Light_Count, 2) +        \n                    Avg_Poverty +        \n                    Avg_Vacancy + I(Avg_Vacancy^2) +        \n                    Avg_Income + I(Avg_Income^2)  +\n                    Avg_Unemployment +\n                    Dist_Police +       \n                    factor(PSA_ID) + \n                    log(Crime_Daily_Rate_lag + 0.001) +\n                    offset(log(Exposure_Days)), \n                  data = final_data,\n                  control = glm.control(maxit = 100))\nsummary(model_5)\n\n\n\nCall:\nglm.nb(formula = Crime_Total_Count ~ Log_Ridership * is_weekend_factor + \n    log(Alcohol_Count + 1) + poly(Light_Count, 2) + Avg_Poverty + \n    Avg_Vacancy + I(Avg_Vacancy^2) + Avg_Income + I(Avg_Income^2) + \n    Avg_Unemployment + Dist_Police + factor(PSA_ID) + log(Crime_Daily_Rate_lag + \n    0.001) + offset(log(Exposure_Days)), data = final_data, control = glm.control(maxit = 100), \n    init.theta = 13.48398402, link = log)\n\nCoefficients:\n                                         Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                             2.557e-01  7.508e-02   3.405 0.000661\nLog_Ridership                           3.097e-02  3.559e-03   8.701  &lt; 2e-16\nis_weekend_factorWeekend                7.513e-03  1.983e-02   0.379 0.704750\nlog(Alcohol_Count + 1)                  6.111e-02  5.773e-03  10.587  &lt; 2e-16\npoly(Light_Count, 2)1                   3.775e+00  8.603e-01   4.388 1.14e-05\npoly(Light_Count, 2)2                  -8.197e-01  4.967e-01  -1.650 0.098911\nAvg_Poverty                             1.727e-01  9.536e-02   1.811 0.070160\nAvg_Vacancy                            -8.960e-03  2.840e-03  -3.155 0.001604\nI(Avg_Vacancy^2)                        2.615e-04  6.921e-05   3.779 0.000158\nAvg_Income                             -8.633e-06  1.166e-06  -7.405 1.31e-13\nI(Avg_Income^2)                         4.994e-11  6.027e-12   8.286  &lt; 2e-16\nAvg_Unemployment                       -1.039e-02  1.539e-03  -6.750 1.48e-11\nDist_Police                            -9.557e-02  1.108e-02  -8.629  &lt; 2e-16\nfactor(PSA_ID)012                      -1.441e-01  5.067e-02  -2.844 0.004459\nfactor(PSA_ID)021                       1.421e-01  3.993e-02   3.558 0.000373\nfactor(PSA_ID)022                       6.487e-02  4.244e-02   1.529 0.126353\nfactor(PSA_ID)023                       5.092e-02  3.636e-02   1.400 0.161366\nfactor(PSA_ID)031                      -2.420e-02  3.758e-02  -0.644 0.519537\nfactor(PSA_ID)032                       3.348e-02  3.519e-02   0.951 0.341369\nfactor(PSA_ID)033                      -7.734e-02  3.133e-02  -2.469 0.013559\nfactor(PSA_ID)051                       5.406e-02  4.560e-02   1.186 0.235803\nfactor(PSA_ID)052                      -1.807e-01  5.194e-02  -3.479 0.000504\nfactor(PSA_ID)053                      -3.275e-01  6.822e-02  -4.801 1.58e-06\nfactor(PSA_ID)071                       2.336e-01  4.844e-02   4.822 1.42e-06\nfactor(PSA_ID)072                      -2.252e-01  4.649e-02  -4.845 1.27e-06\nfactor(PSA_ID)073                      -1.690e-01  4.871e-02  -3.469 0.000521\nfactor(PSA_ID)081                       1.977e-01  4.246e-02   4.655 3.23e-06\nfactor(PSA_ID)082                      -2.421e-01  4.216e-02  -5.742 9.34e-09\nfactor(PSA_ID)083                      -9.608e-02  4.610e-02  -2.084 0.037172\nfactor(PSA_ID)091                      -1.083e-01  3.486e-02  -3.108 0.001886\nfactor(PSA_ID)092                       8.268e-02  3.771e-02   2.193 0.028343\nfactor(PSA_ID)093                       3.994e-02  3.498e-02   1.142 0.253427\nfactor(PSA_ID)094                       2.249e-01  3.584e-02   6.276 3.47e-10\nfactor(PSA_ID)095                      -1.073e-01  3.890e-02  -2.758 0.005810\nfactor(PSA_ID)121                       4.142e-01  4.334e-02   9.557  &lt; 2e-16\nfactor(PSA_ID)122                      -1.703e-01  7.205e-02  -2.363 0.018121\nfactor(PSA_ID)123                      -1.704e-01  4.189e-02  -4.067 4.75e-05\nfactor(PSA_ID)124                       2.026e-01  5.139e-02   3.943 8.05e-05\nfactor(PSA_ID)141                      -1.294e-01  3.887e-02  -3.330 0.000868\nfactor(PSA_ID)142                      -1.265e-01  4.355e-02  -2.905 0.003670\nfactor(PSA_ID)143                      -1.477e-03  4.006e-02  -0.037 0.970583\nfactor(PSA_ID)144                      -8.455e-02  4.711e-02  -1.795 0.072659\nfactor(PSA_ID)151                       1.626e-02  3.617e-02   0.450 0.652963\nfactor(PSA_ID)152                       1.268e-01  3.589e-02   3.532 0.000412\nfactor(PSA_ID)153                       2.305e-01  3.677e-02   6.269 3.62e-10\nfactor(PSA_ID)161                       1.339e-01  3.881e-02   3.449 0.000562\nfactor(PSA_ID)162                       1.178e-01  4.045e-02   2.913 0.003581\nfactor(PSA_ID)171                      -6.720e-02  3.659e-02  -1.836 0.066302\nfactor(PSA_ID)172                      -4.248e-01  4.869e-02  -8.724  &lt; 2e-16\nfactor(PSA_ID)173                      -2.044e-01  3.989e-02  -5.124 2.99e-07\nfactor(PSA_ID)181                      -1.544e-01  4.483e-02  -3.445 0.000572\nfactor(PSA_ID)182                       1.001e-01  3.898e-02   2.569 0.010197\nfactor(PSA_ID)183                      -8.624e-02  3.617e-02  -2.384 0.017118\nfactor(PSA_ID)191                       2.030e-01  3.696e-02   5.493 3.94e-08\nfactor(PSA_ID)192                       6.114e-02  3.944e-02   1.550 0.121118\nfactor(PSA_ID)193                       2.517e-01  3.986e-02   6.315 2.71e-10\nfactor(PSA_ID)221                       6.874e-02  3.797e-02   1.811 0.070214\nfactor(PSA_ID)222                       7.326e-03  4.428e-02   0.165 0.868600\nfactor(PSA_ID)223                      -6.076e-02  3.978e-02  -1.528 0.126626\nfactor(PSA_ID)224                      -1.448e-01  4.634e-02  -3.124 0.001781\nfactor(PSA_ID)241                      -4.814e-02  4.310e-02  -1.117 0.264063\nfactor(PSA_ID)242                      -7.662e-03  4.520e-02  -0.169 0.865409\nfactor(PSA_ID)243                       1.327e-01  4.562e-02   2.908 0.003640\nfactor(PSA_ID)251                       1.919e-01  4.508e-02   4.256 2.08e-05\nfactor(PSA_ID)252                      -4.972e-02  4.469e-02  -1.113 0.265880\nfactor(PSA_ID)253                      -2.549e-01  4.660e-02  -5.469 4.52e-08\nfactor(PSA_ID)254                       4.112e-02  4.372e-02   0.941 0.346951\nfactor(PSA_ID)261                      -7.614e-02  4.393e-02  -1.733 0.083059\nfactor(PSA_ID)262                       5.008e-02  3.908e-02   1.282 0.199990\nfactor(PSA_ID)263                      -6.978e-02  3.389e-02  -2.059 0.039467\nfactor(PSA_ID)351                      -9.185e-02  3.527e-02  -2.604 0.009201\nfactor(PSA_ID)352                       7.575e-02  3.668e-02   2.065 0.038890\nfactor(PSA_ID)353                      -1.149e-01  3.954e-02  -2.905 0.003667\nfactor(PSA_ID)391                      -8.094e-02  3.676e-02  -2.202 0.027682\nfactor(PSA_ID)392                      -7.300e-02  3.878e-02  -1.883 0.059766\nfactor(PSA_ID)393                       4.386e-02  4.647e-02   0.944 0.345290\nlog(Crime_Daily_Rate_lag + 0.001)       7.084e-01  6.903e-03 102.622  &lt; 2e-16\nLog_Ridership:is_weekend_factorWeekend -8.622e-03  5.060e-03  -1.704 0.088411\n                                          \n(Intercept)                            ***\nLog_Ridership                          ***\nis_weekend_factorWeekend                  \nlog(Alcohol_Count + 1)                 ***\npoly(Light_Count, 2)1                  ***\npoly(Light_Count, 2)2                  .  \nAvg_Poverty                            .  \nAvg_Vacancy                            ** \nI(Avg_Vacancy^2)                       ***\nAvg_Income                             ***\nI(Avg_Income^2)                        ***\nAvg_Unemployment                       ***\nDist_Police                            ***\nfactor(PSA_ID)012                      ** \nfactor(PSA_ID)021                      ***\nfactor(PSA_ID)022                         \nfactor(PSA_ID)023                         \nfactor(PSA_ID)031                         \nfactor(PSA_ID)032                         \nfactor(PSA_ID)033                      *  \nfactor(PSA_ID)051                         \nfactor(PSA_ID)052                      ***\nfactor(PSA_ID)053                      ***\nfactor(PSA_ID)071                      ***\nfactor(PSA_ID)072                      ***\nfactor(PSA_ID)073                      ***\nfactor(PSA_ID)081                      ***\nfactor(PSA_ID)082                      ***\nfactor(PSA_ID)083                      *  \nfactor(PSA_ID)091                      ** \nfactor(PSA_ID)092                      *  \nfactor(PSA_ID)093                         \nfactor(PSA_ID)094                      ***\nfactor(PSA_ID)095                      ** \nfactor(PSA_ID)121                      ***\nfactor(PSA_ID)122                      *  \nfactor(PSA_ID)123                      ***\nfactor(PSA_ID)124                      ***\nfactor(PSA_ID)141                      ***\nfactor(PSA_ID)142                      ** \nfactor(PSA_ID)143                         \nfactor(PSA_ID)144                      .  \nfactor(PSA_ID)151                         \nfactor(PSA_ID)152                      ***\nfactor(PSA_ID)153                      ***\nfactor(PSA_ID)161                      ***\nfactor(PSA_ID)162                      ** \nfactor(PSA_ID)171                      .  \nfactor(PSA_ID)172                      ***\nfactor(PSA_ID)173                      ***\nfactor(PSA_ID)181                      ***\nfactor(PSA_ID)182                      *  \nfactor(PSA_ID)183                      *  \nfactor(PSA_ID)191                      ***\nfactor(PSA_ID)192                         \nfactor(PSA_ID)193                      ***\nfactor(PSA_ID)221                      .  \nfactor(PSA_ID)222                         \nfactor(PSA_ID)223                         \nfactor(PSA_ID)224                      ** \nfactor(PSA_ID)241                         \nfactor(PSA_ID)242                         \nfactor(PSA_ID)243                      ** \nfactor(PSA_ID)251                      ***\nfactor(PSA_ID)252                         \nfactor(PSA_ID)253                      ***\nfactor(PSA_ID)254                         \nfactor(PSA_ID)261                      .  \nfactor(PSA_ID)262                         \nfactor(PSA_ID)263                      *  \nfactor(PSA_ID)351                      ** \nfactor(PSA_ID)352                      *  \nfactor(PSA_ID)353                      ** \nfactor(PSA_ID)391                      *  \nfactor(PSA_ID)392                      .  \nfactor(PSA_ID)393                         \nlog(Crime_Daily_Rate_lag + 0.001)      ***\nLog_Ridership:is_weekend_factorWeekend .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(13.484) family taken to be 1)\n\n    Null deviance: 64740  on 11065  degrees of freedom\nResidual deviance: 11871  on 10988  degrees of freedom\nAIC: 76243\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  13.484 \n          Std. Err.:  0.307 \n\n 2 x log-likelihood:  -76085.254 \n\n\n\nThe last model incorporates a temporal lag (Log_Crime_Rate_lag), accounting for the historical ‚Äústickiness‚Äù of crime hotspots. By controlling for past crime alongside spatial fixed effects, this model offers the most rigorous test, correcting for both spatial dependence and serial correlation.\n\n\n\nCode\nvif(model_5)\n\n\n                                         GVIF Df GVIF^(1/(2*Df))\nLog_Ridership                        2.140750  1        1.463130\nis_weekend_factor                    8.319162  1        2.884296\nlog(Alcohol_Count + 1)               3.845278  1        1.960938\npoly(Light_Count, 2)                11.981538  2        1.860493\nAvg_Poverty                          9.159654  1        3.026492\nAvg_Vacancy                         23.286389  1        4.825597\nI(Avg_Vacancy^2)                    14.466404  1        3.803473\nAvg_Income                          98.553277  1        9.927400\nI(Avg_Income^2)                     63.899807  1        7.993735\nAvg_Unemployment                     3.897398  1        1.974183\nDist_Police                          2.828785  1        1.681899\nfactor(PSA_ID)                    2368.120093 63        1.063606\nlog(Crime_Daily_Rate_lag + 0.001)    2.737960  1        1.654678\nLog_Ridership:is_weekend_factor      8.153167  1        2.855375\n\n\n\nA diagnostic check for multicollinearity in Model 5 revealed significant redundancy among the socioeconomic variables. Specifically, Median Income (Avg_Income) exhibited an extremely high Generalized VIF score of 9.92, indicating strong collinearity with Poverty Rate (Avg_Poverty). Including both variables introduces noise and destabilizes coefficient estimates, as they essentially measure the same underlying concept of economic disadvantage.\n\nConsequently, in the transition to Model 6 (Refined Model), we removed Avg_Income and its quadratic term, retaining Avg_Poverty as the primary proxy for socioeconomic status due to its stronger theoretical link to crime risk in urban literature. Additionally, we simplified the Street Lights variable from a polynomial to a linear term (Light_Count). Given that the model includes robust spatial fixed effects (PSA_ID), the complex non-linear variation is already largely absorbed by the spatial controls, making a linear specification for infrastructure more parsimonious and interpretable."
  },
  {
    "objectID": "Final/Final_appendix.html#refine-model-5-and-create-final-model",
    "href": "Final/Final_appendix.html#refine-model-5-and-create-final-model",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "4.6 Refine Model 5 and Create Final Model",
    "text": "4.6 Refine Model 5 and Create Final Model\n\n\nCode\nmodel_6 &lt;- glm.nb(Crime_Total_Count ~ \n                             Log_Ridership * is_weekend_factor + \n                             log(Alcohol_Count + 1) +\n                             Light_Count +\n                             Avg_Poverty +   \n                             Avg_Vacancy + I(Avg_Vacancy^2) + \n                             Avg_Unemployment +\n                             Dist_Police +\n                             factor(PSA_ID) + \n                             log(Crime_Daily_Rate_lag + 0.001) +\n                             offset(log(Exposure_Days)), \n                           data = final_data,\n                           control = glm.control(maxit = 100))\n\nsummary(model_6)\n\n\n\nCall:\nglm.nb(formula = Crime_Total_Count ~ Log_Ridership * is_weekend_factor + \n    log(Alcohol_Count + 1) + Light_Count + Avg_Poverty + Avg_Vacancy + \n    I(Avg_Vacancy^2) + Avg_Unemployment + Dist_Police + factor(PSA_ID) + \n    log(Crime_Daily_Rate_lag + 0.001) + offset(log(Exposure_Days)), \n    data = final_data, control = glm.control(maxit = 100), init.theta = 13.34075513, \n    link = log)\n\nCoefficients:\n                                         Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                            -1.939e-01  4.323e-02  -4.485 7.30e-06\nLog_Ridership                           3.134e-02  3.564e-03   8.793  &lt; 2e-16\nis_weekend_factorWeekend                8.872e-03  1.987e-02   0.447 0.655203\nlog(Alcohol_Count + 1)                  6.276e-02  5.758e-03  10.899  &lt; 2e-16\nLight_Count                             1.154e-04  3.655e-05   3.158 0.001590\nAvg_Poverty                             3.399e-01  7.377e-02   4.607 4.08e-06\nAvg_Vacancy                            -6.985e-03  2.840e-03  -2.460 0.013899\nI(Avg_Vacancy^2)                        2.459e-04  6.941e-05   3.543 0.000396\nAvg_Unemployment                       -9.685e-03  1.477e-03  -6.555 5.55e-11\nDist_Police                            -9.825e-02  1.095e-02  -8.971  &lt; 2e-16\nfactor(PSA_ID)012                      -1.422e-01  5.058e-02  -2.811 0.004946\nfactor(PSA_ID)021                       1.509e-01  3.998e-02   3.773 0.000161\nfactor(PSA_ID)022                       1.014e-01  4.228e-02   2.399 0.016438\nfactor(PSA_ID)023                       6.396e-02  3.636e-02   1.759 0.078605\nfactor(PSA_ID)031                       2.073e-02  3.584e-02   0.578 0.563028\nfactor(PSA_ID)032                       5.317e-02  3.469e-02   1.533 0.125309\nfactor(PSA_ID)033                      -8.258e-02  3.112e-02  -2.654 0.007952\nfactor(PSA_ID)051                       6.217e-02  4.519e-02   1.376 0.168953\nfactor(PSA_ID)052                      -1.710e-01  5.178e-02  -3.302 0.000961\nfactor(PSA_ID)053                      -3.309e-01  6.830e-02  -4.845 1.27e-06\nfactor(PSA_ID)071                       2.547e-01  4.842e-02   5.259 1.45e-07\nfactor(PSA_ID)072                      -2.214e-01  4.646e-02  -4.764 1.89e-06\nfactor(PSA_ID)073                      -1.700e-01  4.882e-02  -3.482 0.000498\nfactor(PSA_ID)081                       2.072e-01  4.248e-02   4.877 1.08e-06\nfactor(PSA_ID)082                      -2.493e-01  4.219e-02  -5.909 3.45e-09\nfactor(PSA_ID)083                      -1.196e-01  4.575e-02  -2.615 0.008924\nfactor(PSA_ID)091                      -9.835e-02  3.360e-02  -2.927 0.003423\nfactor(PSA_ID)092                       1.325e-01  3.576e-02   3.705 0.000212\nfactor(PSA_ID)093                       3.605e-02  3.468e-02   1.040 0.298572\nfactor(PSA_ID)094                       2.110e-01  3.357e-02   6.287 3.25e-10\nfactor(PSA_ID)095                      -3.208e-02  3.647e-02  -0.880 0.379119\nfactor(PSA_ID)121                       4.456e-01  4.336e-02  10.275  &lt; 2e-16\nfactor(PSA_ID)122                      -1.271e-01  7.201e-02  -1.765 0.077630\nfactor(PSA_ID)123                      -1.146e-01  4.141e-02  -2.767 0.005659\nfactor(PSA_ID)124                       2.308e-01  5.113e-02   4.515 6.34e-06\nfactor(PSA_ID)141                      -1.059e-01  3.856e-02  -2.747 0.006022\nfactor(PSA_ID)142                      -8.819e-02  4.343e-02  -2.030 0.042325\nfactor(PSA_ID)143                       2.082e-02  4.008e-02   0.519 0.603501\nfactor(PSA_ID)144                      -5.119e-02  4.695e-02  -1.090 0.275505\nfactor(PSA_ID)151                       4.567e-02  3.592e-02   1.271 0.203590\nfactor(PSA_ID)152                       1.576e-01  3.578e-02   4.404 1.06e-05\nfactor(PSA_ID)153                       2.389e-01  3.681e-02   6.490 8.58e-11\nfactor(PSA_ID)161                       1.985e-01  3.806e-02   5.215 1.84e-07\nfactor(PSA_ID)162                       1.781e-01  3.975e-02   4.480 7.48e-06\nfactor(PSA_ID)171                      -4.983e-02  3.509e-02  -1.420 0.155534\nfactor(PSA_ID)172                      -4.390e-01  4.839e-02  -9.073  &lt; 2e-16\nfactor(PSA_ID)173                      -2.201e-01  3.978e-02  -5.533 3.15e-08\nfactor(PSA_ID)181                      -9.756e-02  4.423e-02  -2.206 0.027403\nfactor(PSA_ID)182                       1.344e-01  3.864e-02   3.479 0.000503\nfactor(PSA_ID)183                      -4.389e-02  3.495e-02  -1.256 0.209178\nfactor(PSA_ID)191                       1.931e-01  3.687e-02   5.237 1.63e-07\nfactor(PSA_ID)192                       1.307e-01  3.848e-02   3.396 0.000684\nfactor(PSA_ID)193                       2.633e-01  3.985e-02   6.608 3.88e-11\nfactor(PSA_ID)221                       1.218e-01  3.758e-02   3.242 0.001189\nfactor(PSA_ID)222                       7.118e-02  4.376e-02   1.627 0.103789\nfactor(PSA_ID)223                      -1.989e-02  3.955e-02  -0.503 0.615088\nfactor(PSA_ID)224                      -1.497e-01  4.627e-02  -3.235 0.001215\nfactor(PSA_ID)241                       1.896e-03  4.282e-02   0.044 0.964684\nfactor(PSA_ID)242                       4.314e-02  4.490e-02   0.961 0.336712\nfactor(PSA_ID)243                       1.449e-01  4.519e-02   3.206 0.001345\nfactor(PSA_ID)251                       2.562e-01  4.454e-02   5.752 8.84e-09\nfactor(PSA_ID)252                       8.936e-03  4.432e-02   0.202 0.840212\nfactor(PSA_ID)253                      -1.611e-01  4.549e-02  -3.540 0.000399\nfactor(PSA_ID)254                       1.288e-01  4.258e-02   3.024 0.002497\nfactor(PSA_ID)261                      -2.606e-02  4.353e-02  -0.599 0.549301\nfactor(PSA_ID)262                       6.579e-02  3.819e-02   1.723 0.084969\nfactor(PSA_ID)263                      -4.964e-02  3.330e-02  -1.491 0.135995\nfactor(PSA_ID)351                      -7.299e-02  3.527e-02  -2.070 0.038487\nfactor(PSA_ID)352                       1.256e-01  3.624e-02   3.466 0.000528\nfactor(PSA_ID)353                      -5.349e-02  3.886e-02  -1.377 0.168638\nfactor(PSA_ID)391                      -4.033e-02  3.657e-02  -1.103 0.270083\nfactor(PSA_ID)392                      -1.015e-02  3.821e-02  -0.266 0.790478\nfactor(PSA_ID)393                       1.247e-01  4.552e-02   2.739 0.006163\nlog(Crime_Daily_Rate_lag + 0.001)       7.118e-01  6.898e-03 103.194  &lt; 2e-16\nLog_Ridership:is_weekend_factorWeekend -8.741e-03  5.073e-03  -1.723 0.084891\n                                          \n(Intercept)                            ***\nLog_Ridership                          ***\nis_weekend_factorWeekend                  \nlog(Alcohol_Count + 1)                 ***\nLight_Count                            ** \nAvg_Poverty                            ***\nAvg_Vacancy                            *  \nI(Avg_Vacancy^2)                       ***\nAvg_Unemployment                       ***\nDist_Police                            ***\nfactor(PSA_ID)012                      ** \nfactor(PSA_ID)021                      ***\nfactor(PSA_ID)022                      *  \nfactor(PSA_ID)023                      .  \nfactor(PSA_ID)031                         \nfactor(PSA_ID)032                         \nfactor(PSA_ID)033                      ** \nfactor(PSA_ID)051                         \nfactor(PSA_ID)052                      ***\nfactor(PSA_ID)053                      ***\nfactor(PSA_ID)071                      ***\nfactor(PSA_ID)072                      ***\nfactor(PSA_ID)073                      ***\nfactor(PSA_ID)081                      ***\nfactor(PSA_ID)082                      ***\nfactor(PSA_ID)083                      ** \nfactor(PSA_ID)091                      ** \nfactor(PSA_ID)092                      ***\nfactor(PSA_ID)093                         \nfactor(PSA_ID)094                      ***\nfactor(PSA_ID)095                         \nfactor(PSA_ID)121                      ***\nfactor(PSA_ID)122                      .  \nfactor(PSA_ID)123                      ** \nfactor(PSA_ID)124                      ***\nfactor(PSA_ID)141                      ** \nfactor(PSA_ID)142                      *  \nfactor(PSA_ID)143                         \nfactor(PSA_ID)144                         \nfactor(PSA_ID)151                         \nfactor(PSA_ID)152                      ***\nfactor(PSA_ID)153                      ***\nfactor(PSA_ID)161                      ***\nfactor(PSA_ID)162                      ***\nfactor(PSA_ID)171                         \nfactor(PSA_ID)172                      ***\nfactor(PSA_ID)173                      ***\nfactor(PSA_ID)181                      *  \nfactor(PSA_ID)182                      ***\nfactor(PSA_ID)183                         \nfactor(PSA_ID)191                      ***\nfactor(PSA_ID)192                      ***\nfactor(PSA_ID)193                      ***\nfactor(PSA_ID)221                      ** \nfactor(PSA_ID)222                         \nfactor(PSA_ID)223                         \nfactor(PSA_ID)224                      ** \nfactor(PSA_ID)241                         \nfactor(PSA_ID)242                         \nfactor(PSA_ID)243                      ** \nfactor(PSA_ID)251                      ***\nfactor(PSA_ID)252                         \nfactor(PSA_ID)253                      ***\nfactor(PSA_ID)254                      ** \nfactor(PSA_ID)261                         \nfactor(PSA_ID)262                      .  \nfactor(PSA_ID)263                         \nfactor(PSA_ID)351                      *  \nfactor(PSA_ID)352                      ***\nfactor(PSA_ID)353                         \nfactor(PSA_ID)391                         \nfactor(PSA_ID)392                         \nfactor(PSA_ID)393                      ** \nlog(Crime_Daily_Rate_lag + 0.001)      ***\nLog_Ridership:is_weekend_factorWeekend .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(13.3408) family taken to be 1)\n\n    Null deviance: 64299  on 11065  degrees of freedom\nResidual deviance: 11874  on 10991  degrees of freedom\nAIC: 76310\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  13.341 \n          Std. Err.:  0.303 \n\n 2 x log-likelihood:  -76157.840 \n\n\nCode\nvif(model_6)\n\n\n                                        GVIF Df GVIF^(1/(2*Df))\nLog_Ridership                       2.133246  1        1.460563\nis_weekend_factor                   8.301509  1        2.881234\nlog(Alcohol_Count + 1)              3.798142  1        1.948882\nLight_Count                         5.541211  1        2.353978\nAvg_Poverty                         5.428573  1        2.329930\nAvg_Vacancy                        23.126903  1        4.809044\nI(Avg_Vacancy^2)                   14.459269  1        3.802535\nAvg_Unemployment                    3.561355  1        1.887155\nDist_Police                         2.751623  1        1.658802\nfactor(PSA_ID)                    467.133380 63        1.049992\nlog(Crime_Daily_Rate_lag + 0.001)   2.717994  1        1.648634\nLog_Ridership:is_weekend_factor     8.139565  1        2.852992\n\n\n\nAfter removing the conflicting income variables, Model 6 is now statistically healthy. Most importantly, our core variable, Log_Ridership, has a very low GVIF score of 2.13. This is well below the concern threshold (usually 5 or 10), which proves that our main finding that ridership drives crime, is reliable and not distorted by other factors.\n\nHigh scores for Vacancy and the Interaction term are mathematically expected when we use squared terms (like Vacancy^2) or interactions. They do not negatively affect the model‚Äôs ability to predict crime."
  },
  {
    "objectID": "Final/Final_appendix.html#model-comparison-plot",
    "href": "Final/Final_appendix.html#model-comparison-plot",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "4.7 Model Comparison Plot",
    "text": "4.7 Model Comparison Plot\n\n\nCode\nlibrary(modelsummary)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# 1. Modify Coef Map\ncoef_map_refined &lt;- c(\n  \"Log_Ridership\" = \"Ridership (Log)\",\n  \"is_weekend_factorWeekend\" = \"Weekend Effect\",\n  \n  \"Log_Ridership:is_weekend_factorWeekend\" = \"Interaction: Ridership √ó Weekend\",\n  \"is_weekend_factorWeekend:Log_Ridership\" = \"Interaction: Ridership √ó Weekend\", \n  \n  \"log(Alcohol_Count + 1)\" = \"Alcohol Outlets\", \n  \"Light_Count\" = \"Street Lights (Linear)\",   \n  \"Avg_Poverty\" = \"Poverty Rate\",             \n  \"Avg_Vacancy\" = \"Vacancy Rate\",\n  \"I(Avg_Vacancy^2)\" = \"Vacancy Rate (Sq)\",\n  \"Avg_Unemployment\" = \"Unemployment Rate\",\n  \"Dist_Police\" = \"Dist to Police Station\",\n  \"log(Crime_Daily_Rate_lag + 0.001)\" = \"Temporal Lag (Prev. Q)\"\n)\n\n# 2. Mapping\np_comparison &lt;- modelplot(\n  list(\n    \"M1: Ridership\" = model_1, \n    \"M2: +Interact\" = model_2,\n    \"M3: +Env/Demo\" = model_3,\n    \"M4: +PSA Fixed\" = model_4, \n    \"M5: +Lag\" = model_5,\n    \"M6: Final\" = model_6 \n  ),\n  coef_map = coef_map_refined, \n\n  coef_omit = \"Intercept\", \n  \n  conf_level = 0.95,\n  size = 0.8 \n) +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Model Evolution: Stability of Key Drivers\",\n    subtitle = \"Comparing coefficients across model iterations (M1-M6)\",\n    x = \"Effect Size (Coefficient Estimate)\",\n    y = \"\",\n    caption = \"Note: To facilitate direct comparison, this plot displays only the variables selected for the Final Refined Model (M6).\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Dark2\") + \n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    axis.text.y = element_text(size = 10, face = \"bold\", color = \"black\"),\n    panel.grid.minor = element_blank()\n  )\n\np_comparison\n\n\n\n\n\n\n\n\n\n\nThis plot visualizes our journey from a naive baseline to a robust final model. When reading this chart:\n\nDistance from Red Line: The further the dot is to the right, the stronger the positive impact on crime.\n\nLength of the Line: This represents the Confidence Interval. A longer line means higher uncertainty, often caused by multicollinearity.\n\nModel 1(Ridership Only): Without controlling for any other factors, ridership appears to be a massive driver of crime. However, this estimate is likely inflated (biased) because high-ridership stops are often located in dense, low-income areas. The model is mistakenly attributing the neighborhood effect solely to ridership.\nModel 2(+Interaction): We introduce the Weekend Effect and the interaction term. The main Ridership coefficient increases slightly. This confirms that the relationship isn‚Äôt identical across the week. By separating weekends, we begin to see that the ‚Äúbaseline risk‚Äù shifts, validating the need for a pseudo-panel approach, though the core finding (ridership = risk) remains consistent due to its positive coefficient.\nModel 3(+Env & Demo): A dramatic shift occurs. The Poverty Rate coefficient appears and is extremely high (purple dot far to the right), while the Ridership coefficient drops significantly (moving left). This is the ‚ÄúReality Check.‚Äù Once we account for poverty, we realize that socioeconomic status is the primary driver of crime, not just the bus stop itself. Ridership is still a significant risk factor (the coefficient is still positive), but its impact is much smaller than Model 1 suggested.\nModel 4(+PSA Fixed Effects): The Poverty Rate coefficient shrinks (moves left, pink dot) compared to Model 3. By adding PSA (Police Service Area) fixed effects, we control for unobserved neighborhood characteristics (like local culture or police presence). The model no longer relies solely on ‚ÄúPoverty‚Äù to explain crime clusters, making the estimates for specific variables like Alcohol Outlets and Ridership more precise and trustworthy.\nModel 5(+Lag): The Temporal Lag variable appears at the top with a very high positive coefficient. The strongest predictor of future crime is past crime. Adding this variable absorbs a huge amount of variance, creating a very strict test for our other variables.\nModel 6(Final Refined): Ridership remains positive and significant (approx 0.1), proving that even after controlling for everything (poverty, history, location), more passengers still equal more targets. Alcohol shows a stable positive link. Dist to Police shows a negative coefficient (meaning crime is higher closer to stations), but this is probably because there are bias in original dataset where crimes nearby the police stations are more likely to be recorded."
  },
  {
    "objectID": "Final/Final_appendix.html#compare-all-6-models",
    "href": "Final/Final_appendix.html#compare-all-6-models",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "5.1 Compare all 6 models:",
    "text": "5.1 Compare all 6 models:\n\n5.1.1 Create predicted vs.¬†actual plot\n\n\nCode\nf1 &lt;- Crime_Total_Count ~ Log_Ridership + offset(log(Exposure_Days))\nf2 &lt;- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + offset(log(Exposure_Days))\nf3 &lt;- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + log(Alcohol_Count + 1) + poly(Light_Count, 2) + Avg_Poverty + Avg_Vacancy + I(Avg_Vacancy^2) + Avg_Income + I(Avg_Income^2) + Avg_Unemployment + Dist_Police + offset(log(Exposure_Days))\n\nf4 &lt;- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + log(Alcohol_Count + 1) + poly(Light_Count, 2) + Avg_Poverty + Avg_Vacancy + I(Avg_Vacancy^2) + Avg_Income + I(Avg_Income^2) + Avg_Unemployment + Dist_Police + factor(PSA_ID) + offset(log(Exposure_Days)) \n\nf5 &lt;- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + \n      log(Alcohol_Count + 1) + poly(Light_Count, 2) + \n      Avg_Poverty + Avg_Vacancy + I(Avg_Vacancy^2) + \n      Avg_Income + I(Avg_Income^2) + Avg_Unemployment + \n      Dist_Police + factor(PSA_ID) + \n      log(Crime_Daily_Rate_lag + 0.001) + \n      offset(log(Exposure_Days))\n\nf6 &lt;- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + \n      log(Alcohol_Count + 1) + \n      Light_Count +              \n      Avg_Poverty +              \n      Avg_Vacancy + I(Avg_Vacancy^2) + \n      Avg_Unemployment + \n      Dist_Police + \n      factor(PSA_ID) +         \n      log(Crime_Daily_Rate_lag + 0.001) + \n      offset(log(Exposure_Days))\n\n# 2. Train 6 models\nm1 &lt;- glm.nb(f1, data = final_data)\nm2 &lt;- glm.nb(f2, data = final_data)\nm3 &lt;- glm.nb(f3, data = final_data)\nm4 &lt;- glm.nb(f4, data = final_data)\nm5 &lt;- glm.nb(f5, data = final_data)\nm6 &lt;- glm.nb(f6, data = final_data)\n\n# 3. Gather Predictions\nplot_data &lt;- final_data %&gt;%\n  dplyr::select(Crime_Total_Count) %&gt;%\n  mutate(\n    Model_1_Pred = predict(m1, type = \"response\"),\n    Model_2_Pred = predict(m2, type = \"response\"),\n    Model_3_Pred = predict(m3, type = \"response\"),\n    Model_4_Pred = predict(m4, type = \"response\"),\n    Model_5_Pred = predict(m5, type = \"response\"),\n    Model_6_Pred = predict(m6, type = \"response\"),\n  ) %&gt;%\n  pivot_longer(\n    cols = starts_with(\"Model\"), \n    names_to = \"Model\", \n    values_to = \"Predicted_Count\"\n  )\n\n# 4. MapÔºöFacet Wrap compare 6 models\nggplot(plot_data, aes(x = Predicted_Count, y = Crime_Total_Count)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\", size = 0.8) +\n  \n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  \n  facet_wrap(~Model, ncol = 3) +\n  \n  labs(\n    title = \"Predicted vs. Actual Crime Counts\",\n    subtitle = \"Comparing Model Fit: Final Model (M6) shows robust predictions\",\n    x = \"Predicted Crime Count\",\n    y = \"Actual Crime Count\"\n  ) +\n  theme_bw() +\n  \n  coord_cartesian(xlim = c(0, 50), ylim = c(0, 50))\n\n\n\n\n\n\n\n\n\n\nThis plot also visualizes our journey from a naive baseline to a robust final model.\nOverall, as more contextual and more spatial information are added, the predicted values align more closely with the 45¬∞ reference line, indicating improved model fit.\nModels 1‚Äì2 rely mainly on ridership and weekend interaction, resulting in wide dispersion and systematic under-prediction at higher crime levels.\nModels 3‚Äì4 incorporate environmental (alcohol outlets, lighting) and demographic features, producing a noticeably tighter prediction band and capturing more variation across stops.\nModel 5 adds temporal information (lagged crime rate), which further stabilizes predictions and reduces bias.\nCompared with Model 5, Model 6 trims several weak or redundant predictors while keeping the key effects. This parsimonious specification makes the model more stable and interpretable and helps reduce potential multicollinearity among highly correlated socioeconomic variables.\n\n\n\n5.1.2 Report and Compare RMSE, MAE, R¬≤\n\n\nCode\nlibrary(caret)\n\n# 1. Set 5-Fold Cross Validation\nset.seed(999)\nfolds &lt;- createFolds(final_data$Crime_Total_Count, k = 5, list = TRUE)\n\nrun_cv &lt;- function(formula, data, folds) {\n  mae_list &lt;- c()\n  rmse_list &lt;- c()\n  \n  for (i in 1:length(folds)) {\n    # Split Data\n    test_idx &lt;- folds[[i]]\n    train_set &lt;- data[-test_idx, ]\n    test_set  &lt;- data[test_idx, ]\n    \n    # Train Model\n    model &lt;- tryCatch({\n      glm.nb(formula, data = train_set)\n    }, error = function(e) return(NULL))\n    \n    if(!is.null(model)) {\n      preds &lt;- predict(model, newdata = test_set, type = \"response\")\n      \n      # Calculate Errors\n      actuals &lt;- test_set$Crime_Total_Count\n      mae_list &lt;- c(mae_list, mean(abs(actuals - preds)))\n      rmse_list &lt;- c(rmse_list, sqrt(mean((actuals - preds)^2)))\n    }\n  }\n  return(c(mean(mae_list), mean(rmse_list)))\n}\n\n# 2. CV 6 models\nresults_m1 &lt;- run_cv(f1, final_data, folds)\nresults_m2 &lt;- run_cv(f2, final_data, folds)\nresults_m3 &lt;- run_cv(f3, final_data, folds)\nresults_m4 &lt;- run_cv(f4, final_data, folds)\nresults_m5 &lt;- run_cv(f5, final_data, folds)\nresults_m6 &lt;- run_cv(f6, final_data, folds)\n\n# 3. Generate chart\nvalidation_summary &lt;- data.frame(\n  Model = c(\"1. Ridership Only\", \"2. +Interaction\", \"3. +Env & Demo\", \"4. +Spatial Fixed\", \"5. +Temporal lag\", \"6. Refined\"),\n  MAE  = c(results_m1[1], results_m2[1], results_m3[1], results_m4[1], results_m5[1], results_m6[1]),\n  RMSE = c(results_m1[2], results_m2[2], results_m3[2], results_m4[2], results_m5[2], results_m6[2])\n) %&gt;%\n  mutate(\n    Improvement_MAE = (MAE[1] - MAE) / MAE[1]\n  )\n\nkable(validation_summary, digits = 3, caption = \"5-Fold Cross-Validation Metrics\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE) %&gt;%\n  column_spec(4, color = \"green\", bold = TRUE)\n\n\n\n5-Fold Cross-Validation Metrics\n\n\nModel\nMAE\nRMSE\nImprovement_MAE\n\n\n\n\n1. Ridership Only\n17.268\n28.877\n0.000\n\n\n2. +Interaction\n17.209\n28.868\n0.003\n\n\n3. +Env & Demo\n12.777\n21.039\n0.260\n\n\n4. +Spatial Fixed\n11.409\n18.343\n0.339\n\n\n5. +Temporal lag\n7.453\n11.629\n0.568\n\n\n6. Refined\n7.475\n11.662\n0.567\n\n\n\n\n\n\nThese findings are validated through 5-fold cross-validation, which evaluates each model on unseen data rather than relying solely on in-sample fit. The consistent reduction in MAE and RMSE across folds confirms that the improvements are not artifacts of overfitting but reflect real gains in out-of-sample predictive performance.\nModel 1(Ridership Only)\nThe model using only ridership intensity establishes a basic relationship between human activity levels and crime, but its predictive capacity is limited. Ridership alone captures general exposure but cannot explain spatial heterogeneity across stops.\nModel 2(+ Interaction)\nAdding the weekday‚Äìweekend interaction improves the model slightly.\nModel 3(+ Environment&Demographic)\nAlcohol outlet density, Lighting conditions poverty, income, vacancy, and unemployment: these variables substantially reduce prediction error, indicating that crime near transit stops is strongly shaped by both environmental risk factors and neighborhood disadvantage.\nModel 4(+Spatial Fixed)\nIntroducing police station distance and especially PSA fixed effects further improves performance.\nThis suggests that:Crime patterns are partly structured by local policing jurisdictions and there are unobserved spatial factors (culture, enforcement style, land use patterns) that are constant within each PSA.PSA fixed effects absorb these stable spatial characteristics, making the model more robust.\nModel 5(Temporal Lag)\nIncluding lagged crime rate (previous quarter) yields the largest single improvement. Lagged crime captures temporal persistence‚Äîareas that experienced more crime in the past tend to remain active in the present.This is a very strong and stable predictor, dramatically improving accuracy.\nModel 6(Refined)\nModel 6 streamlines the specification by removing weaker or collinear variables.Despite using fewer predictors, its accuracy remains nearly identical to Model 5.\nThis indicates that a refined, parsimonious model with reduced multicollinearity can maintain strong predictive performance while improving interpretability and stability."
  },
  {
    "objectID": "Final/Final_appendix.html#spatial-autocorrelation-of-residuals-morans-i",
    "href": "Final/Final_appendix.html#spatial-autocorrelation-of-residuals-morans-i",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "6.1 Spatial Autocorrelation of Residuals (Moran‚Äôs I)",
    "text": "6.1 Spatial Autocorrelation of Residuals (Moran‚Äôs I)\n\n\nCode\nbest_model &lt;- model_6\n\nfinal_data$resid_pearson  &lt;- residuals(best_model, type = \"pearson\")\nfinal_data$resid_deviance &lt;- residuals(best_model, type = \"deviance\")\n\n# Spatial Weights Matrix\ncoords &lt;- st_coordinates(final_data)\nneighbor_nb &lt;- knn2nb(knearneigh(coords, k = 8))\nspatial_weights &lt;- nb2listw(neighbor_nb, style = \"W\")\n\n# Moran's I test\nmoran_result &lt;- moran.test(final_data$resid_pearson, spatial_weights)\n\nprint(moran_result)\n\n\n\n    Moran I test under randomisation\n\ndata:  final_data$resid_pearson  \nweights: spatial_weights    \n\nMoran I statistic standard deviate = 100.73, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     4.518144e-01     -9.037506e-05      2.012726e-05 \n\n\n\nAlthough we added spatial fixed effect(which is also a way to reduce spatial autocorrelation), the Moran‚Äôs I test on the model residuals shows a strong and highly significant level of spatial autocorrelation (Moran‚Äôs I = 0.452, p &lt; 2.2 √ó 10‚Åª¬π‚Å∂).\nThis indicates that the residuals are not randomly distributed across space and that the model likely fails to capture important spatial dependence structures in the data.\nIn practical terms, nearby transit stops tend to have systematically similar over or under predictions, suggesting the presence of spatially clustered unobserved factors such as local policing practices, neighborhood context, or land-use patterns that are not fully absorbed even after including PSA fixed effects and temporal lag terms.\nGiven the strength of the residual spatial autocorrelation, the results imply that the model could benefit from incorporating explicit spatial components, such as:\n\nspatially lagged variables (SAR, CAR, or SLX terms)\n\nspatial random effects\n\ngeographically weighted or spatially varying coefficient structures\n\n\nOverall, the Moran‚Äôs I result shows that while the model fits well in non-spatial dimensions, unmodeled spatial processes remain significant and should be considered in future extensions."
  },
  {
    "objectID": "Final/Final_appendix.html#spatial-distribution-of-residuals",
    "href": "Final/Final_appendix.html#spatial-distribution-of-residuals",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "6.2 Spatial Distribution of Residuals",
    "text": "6.2 Spatial Distribution of Residuals\n\n\nCode\n# Spatial Distribution of Residuals\nfinal_data$spatial_resid &lt;- residuals(best_model, type = \"deviance\")\n\nggplot() +\n  geom_sf(data = philly_boundary, fill = \"grey95\", color = NA) +\n  \n  geom_sf(data = final_data, \n          aes(color = spatial_resid), \n          size = 0.8, alpha = 0.7) +\n  \n  scale_color_gradient2(\n    low = \"blue\", mid = \"grey90\", high = \"red\",\n    midpoint = 0,\n    name = \"Deviance\\nResidual\",\n    limits = c(-3, 3), \n    oob = scales::squish \n  ) +\n  \n  labs(\n    title = \"Map of Model Residuals\",\n    subtitle = \"Red = Unexpectedly High Crime (Under-predicted)\\nBlue = Unexpectedly Low Crime (Over-predicted)\"\n  ) +\n  mapTheme\n\n\n\n\n\n\n\n\n\n\nRed spots (positive deviance residuals):These locations experienced more crime than the model expected.\nBlue spots (negative deviance residuals):These locations show less crime than predicted.\nAlthough the residuals exhibit noticeable spatial clustering, which was confirmed by the significant Moran‚Äôs I test, the pattern does not form a clear, interpretable spatial structure.\nThe clusters of over and under prediction appear scattered across different parts of the city without aligning with obvious geographic boundaries, transit corridors, or neighborhood divisions.\nThis suggests that while some localized spatial dependence remains in the model, these patterns do not translate into a single coherent spatial process.\nIn other words, the remaining spatial signal is weakly structured and heterogeneous, making it difficult to summarize into a simple spatial rule or trend."
  },
  {
    "objectID": "Final/Final_appendix.html#residual-plot",
    "href": "Final/Final_appendix.html#residual-plot",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "6.3 Residual plot",
    "text": "6.3 Residual plot\n\n\nCode\nbest_model &lt;- model_6\n\nmodel_data &lt;- data.frame(\n  Fitted = fitted(best_model),\n  Residuals = residuals(best_model, type = \"pearson\")\n)\n\np_resid_fitted &lt;- ggplot(model_data, aes(x = log(Fitted), y = Residuals)) +\n  geom_point(alpha = 0.3, color = \"#6A1B9A\", size = 1.5) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"loess\", color = \"black\", se = FALSE, linewidth = 0.8) +\n  labs(\n    title = \"Residuals vs Fitted Values\",\n    subtitle = \"Checking for systematic bias in the Count Model\",\n    x = \"Log(Fitted Values) - Predicted Crime Count\",\n    y = \"Pearson Residuals\"\n  ) +\n  plotTheme\n\np_resid_fitted\n\n\n\n\n\n\n\n\n\n\nThis plot shows the residuals against the predicted crime counts from the final negative binomial model. Overall, the residuals are centered around zero without clear curvature or funnel shapes, suggesting that the model does not suffer from major misspecification or heteroskedasticity.\nThe ‚Äústriped‚Äù patterns come from the integer nature of crime counts which is an effect commonly called the discreteness pattern in count models. This is not a model problem; it simply reflects that many stations share the same small crime counts , leading to stacked fitted values. The smooth LOESS line stays close to zero across the range, indicating no strong systematic bias.\nIn short, the residual plot shows that the negative binomial model is behaving as expected for count data, and there is no major evidence of nonlinearity or missing predictors."
  },
  {
    "objectID": "Final/Final_appendix.html#q-q-plot",
    "href": "Final/Final_appendix.html#q-q-plot",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "6.4 Q-Q plot:",
    "text": "6.4 Q-Q plot:\n\n\nCode\np_qq &lt;- ggplot(model_data, aes(sample = Residuals)) +\n  stat_qq(color = \"#6A1B9A\", size = 1.5, alpha = 0.5) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Q-Q Plot of Pearson Residuals\",\n    subtitle = \"Checking for extreme outliers in count data\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  plotTheme\n\np_qq\n\n\n\n\n\n\n\n\n\n\nThe Q‚ÄìQ plot shows that the Pearson residuals follow the theoretical quantiles closely in the middle range, meaning the model captures the main distribution of crime counts well. The deviation at the upper tail indicates a few unusually high-crime stops that the model cannot fully explain‚Äîthis is common in count data, especially when rare but extreme events occur.\nNegative Binomial models are designed to handle overdispersion, and the relatively small tail deviation suggests that any remaining overdispersion is limited to a small number of extreme observations rather than a systematic model failure.\nOverall, the Q‚ÄìQ plot supports that the model fits the bulk of the data well, with only mild deviations at the extremes. Binomial assumption might still be struggling with extreme overdispersion."
  },
  {
    "objectID": "Final/Final_appendix.html#the-verdict-targets-outweigh-eyes-on-the-street",
    "href": "Final/Final_appendix.html#the-verdict-targets-outweigh-eyes-on-the-street",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "7.1 The Verdict: ‚ÄúTargets‚Äù Outweigh ‚ÄúEyes on the Street‚Äù",
    "text": "7.1 The Verdict: ‚ÄúTargets‚Äù Outweigh ‚ÄúEyes on the Street‚Äù\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(kableExtra)\n\nmodel_final &lt;- model_6 \n\ncoef_name_base &lt;- \"Log_Ridership\"\ncoef_name_interact &lt;- \"Log_Ridership:is_weekend_factorWeekend\"\n\nbeta_base &lt;- coef(model_final)[coef_name_base]        \nbeta_interact &lt;- coef(model_final)[coef_name_interact]\nvcov_matrix &lt;- vcov(model_final)                     \n\n# Calculate real slope in weekends and weekdays\n\n# A. Weekday: Base\nslope_weekday &lt;- beta_base\nse_weekday &lt;- sqrt(vcov_matrix[coef_name_base, coef_name_base])\np_weekday &lt;- 2 * (1 - pnorm(abs(slope_weekday / se_weekday)))\n\n# B. Weekend: Base + Interaction\nslope_weekend &lt;- beta_base + beta_interact\n# Calculate Var: Var(A+B) = Var(A) + Var(B) + 2*Cov(A,B)\nvar_weekend &lt;- vcov_matrix[coef_name_base, coef_name_base] + \n               vcov_matrix[coef_name_interact, coef_name_interact] + \n               2 * vcov_matrix[coef_name_base, coef_name_interact]\nse_weekend &lt;- sqrt(var_weekend)\np_weekend &lt;- 2 * (1 - pnorm(abs(slope_weekend / se_weekend)))\n\nhypothesis_test &lt;- data.frame(\n  Scenario = c(\"Weekday (Commuters)\", \"Weekend (Non-Routine)\"),\n  Impact_Slope = c(slope_weekday, slope_weekend),\n  Std_Error = c(se_weekday, se_weekend),\n  P_Value = c(p_weekday, p_weekend)\n) %&gt;%\n  mutate(\n    Interpretation = case_when(\n      Impact_Slope &gt; 0 & P_Value &lt; 0.05 ~ \"Positive & Significant (Target Hypothesis Supported)\",\n      Impact_Slope &lt; 0 & P_Value &lt; 0.05 ~ \"Negative & Significant (Eyes on Street Supported)\",\n      TRUE ~ \"Not Significant\"\n    ),\n    \n    Impact_Slope = round(Impact_Slope, 3),\n    Std_Error = round(Std_Error, 3),\n    P_Value = ifelse(P_Value &lt; 0.001, \"&lt; 0.001\", round(P_Value, 3))\n  )\n\nkbl(hypothesis_test, caption = \"The Verdict: Does Ridership Drive Crime in Both Contexts?\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = F) %&gt;%\n  row_spec(1:2, bold = TRUE, color = \"black\", background = \"#e6f5ff\") %&gt;%\n  footnote(general = \"Slopes &gt; 0 indicate that higher ridership is associated with higher crime counts.\")\n\n\n\nThe Verdict: Does Ridership Drive Crime in Both Contexts?\n\n\nScenario\nImpact_Slope\nStd_Error\nP_Value\nInterpretation\n\n\n\n\nWeekday (Commuters)\n0.031\n0.004\n&lt; 0.001\nPositive & Significant (Target Hypothesis Supported)\n\n\nWeekend (Non-Routine)\n0.023\n0.004\n&lt; 0.001\nPositive & Significant (Target Hypothesis Supported)\n\n\n\nNote: \n\n\n\n\n\n\n Slopes &gt; 0 indicate that higher ridership is associated with higher crime counts.\n\n\n\n\n\n\n\n\n\n\n\nOur analysis provides a definitive answer to the ‚ÄúEyes on the Street‚Äù versus ‚ÄúTargets‚Äù debate within the context of SEPTA bus stops. As illustrated in our final verdict table, ridership exhibits a consistent, positive, and significant correlation with crime counts (\\(p &lt; 0.001\\)). Specifically, we observed an impact slope of 0.031 for Weekdays (routine commuting) and 0.023 for Weekends (discretionary travel). Crucially, this relationship holds true even after controlling for neighborhood socioeconomic status. This suggests that in Philadelphia, high passenger volume acts primarily as an attractor for opportunistic crime (‚ÄúTargets‚Äù) rather than a deterrent. Therefore, reliance on passive surveillance by crowds is insufficient; active security measures are required as ridership grows."
  },
  {
    "objectID": "Final/Final_appendix.html#operational-strategy-precision-policing-based-on-risk-anomalies",
    "href": "Final/Final_appendix.html#operational-strategy-precision-policing-based-on-risk-anomalies",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "7.2 Operational Strategy: Precision Policing based on ‚ÄúRisk Anomalies‚Äù",
    "text": "7.2 Operational Strategy: Precision Policing based on ‚ÄúRisk Anomalies‚Äù\n\n\nCode\n## 7.2.1 Top 50 Under-Policed stops\nfinal_data &lt;- final_data %&gt;%\n  mutate(\n    Predicted_Crime = predict(best_model, type = \"response\"),\n    Resid_Raw = Crime_Total_Count - Predicted_Crime\n  )\n\ntop_50_anomalies &lt;- final_data %&gt;%\n  arrange(desc(Resid_Raw)) %&gt;%\n  slice(1:50)\n\nggplot() +\n  geom_sf(data = philly_boundary, fill = \"grey95\", color = \"white\") +\n  geom_sf(data = final_data, color = \"grey80\", size = 0.5, alpha = 0.3) +\n  geom_sf(data = top_50_anomalies, \n          aes(size = Resid_Raw), \n          color = \"steelblue\", \n          alpha = 0.8) +\n\n  scale_size_continuous(name = \"Excess Crimes\\n(Actual - Predicted)\") +\n  \n  labs(\n    title = \"Top 50 Under-policed Stops\",\n    subtitle = \"Locations where actual crime significantly exceeds model predictions.\",\n    caption = \"Blue dots represent stops performing worse than their environment, suggesting there are more police in need.\"\n  ) +\n  mapTheme +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n## 7.2.2 Top 50 Over-Policed stops\nfinal_data &lt;- final_data %&gt;%\n  mutate(\n    Predicted_Crime = predict(best_model, type = \"response\"),\n    Resid_Raw =  Predicted_Crime - Crime_Total_Count\n  )\n\ntop_50_anomalies &lt;- final_data %&gt;%\n  arrange(desc(Resid_Raw)) %&gt;%\n  slice(1:50)\n\nggplot() +\n  geom_sf(data = philly_boundary, fill = \"grey95\", color = \"white\") +\n  geom_sf(data = final_data, color = \"grey80\", size = 0.5, alpha = 0.3) +\n  geom_sf(data = top_50_anomalies, \n          aes(size = Resid_Raw), \n          color = \"#d73027\", \n          alpha = 0.8) +\n\n  scale_size_continuous(name = \"Excess Crimes\\n(Predicted - Actual)\") +\n  \n  labs(\n    title = \"Top 50 'Over-policed' Stops\",\n    subtitle = \"Locations where actual crime significantly under model predictions.\",\n    caption = \"Red dots represent stations which are statistically risky, but the actual crime counts are actually small. \"\n  ) +\n  mapTheme +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\nInstead of simply deploying police to the busiest stations (which is inefficient) or the poorest neighborhoods (which reinforces bias), our model offers a ‚ÄúPrecision Policing‚Äù approach based on spatial anomalies. By calculating the residuals‚Äîthe difference between actual and predicted crime‚Äîwe spatially distinguished areas of resource saturation from areas of critical need.\nAs shown in our anomaly maps, the ‚ÄúOver-policed‚Äù stops (Red Dots) are heavily clustered in Center City, indicating that current security measures there are suppressing crime below predicted levels. In sharp contrast, we identified the ‚ÄúTop 50 Under-policed Stops‚Äù (Blue Dots). These blue anomalies represent locations where actual crime exceeds our model‚Äôs predictions by up to 90 incidents‚Äîa gap that local environmental factors like poverty or lighting cannot explain. We recommend SEPTA Transit Police utilize this ‚ÄúHit List‚Äù to reallocate patrol units from the saturated downtown core to these specific outlying hotspots. Focusing resources on these specific anomalies yields the highest marginal return on public safety investment.\n\n\n\nCode\n## 7.2.3 Top 10 High-Risk Anomaly Table\n\nlibrary(kableExtra)\n\ntop_10_table &lt;- final_data %&gt;%\n  mutate(\n    Predicted = predict(best_model, type = \"response\"),\n    Residual = Crime_Total_Count - Predicted\n  ) %&gt;%\n  arrange(desc(Residual)) %&gt;%\n  slice(1:10) %&gt;%\n  \n  dplyr::select(\n    \"Bus Stop Name\" = Stop,\n    \"Police District (PSA)\" = PSA_ID,\n    \"Actual Crime\" = Crime_Total_Count,\n    \"Model Predicted\" = Predicted,\n    \"Unexplained Excess\" = Residual\n  )\n\nkbl(top_10_table, digits = 1, caption = \"The 'Hit List': Top 10 Stops with Highest Unexplained Crime Risk\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), full_width = F) %&gt;%\n  column_spec(5, bold = TRUE, color = \"white\", background = \"#d73027\") %&gt;%\n \n  footnote(general = \" 'Unexplained Excess' = Actual Crime minus Predicted Crime based on local environment. Positive values indicate specific local security failures.\")\n\n\n\nThe 'Hit List': Top 10 Stops with Highest Unexplained Crime Risk\n\n\nBus Stop Name\nPolice District (PSA)\nActual Crime\nModel Predicted\nUnexplained Excess\ngeometry\n\n\n\n\nLocust St & 17th St\n093\n295\n198.2\n96.8\nPOINT (2691904 234766)\n\n\nWhitby Av & 53rd St\n124\n138\n52.3\n85.7\nPOINT (2675321 233206.7)\n\n\n17th St & Locust St\n093\n276\n190.9\n85.1\nPOINT (2691903 234716.1)\n\n\n54th St & Willows Av\n124\n142\n58.2\n83.8\nPOINT (2675499 232607.7)\n\n\n52nd St & Jefferson St\n193\n176\n94.5\n81.5\nPOINT (2675925 245535.5)\n\n\n54th St & Whitby Av - FS\n124\n136\n55.4\n80.6\nPOINT (2675117 232954.9)\n\n\n54th St & Whitby Av\n124\n137\n57.2\n79.8\nPOINT (2675141 233000)\n\n\nWhitby Av & 53rd St - FS\n124\n133\n53.5\n79.5\nPOINT (2675326 233279)\n\n\nJefferson St & 52nd St - FS\n193\n176\n98.9\n77.1\nPOINT (2675866 245528.4)\n\n\n52nd St & Heston St\n193\n176\n98.9\n77.1\nPOINT (2675998 245602.9)\n\n\n\nNote: \n\n\n\n\n\n\n\n 'Unexplained Excess' = Actual Crime minus Predicted Crime based on local environment. Positive values indicate specific local security failures.\n\n\n\n\n\n\n\n\n\n\n\n\nBeyond general heatmaps, our model generates a granular ‚ÄúHit List‚Äù for immediate operational use. As detailed in the Top 10 Stops with Highest Unexplained Crime Risk table, the intersection of Locust St & 17th St stands out as the single most critical anomaly, registering 96.8 excess crimes‚Äîincidents that simply cannot be attributed to the surrounding built environment. Furthermore, we observe a disturbing systemic pattern in Police Service Area (PSA) 124: it accounts for 5 of the top 10 riskiest locations, particularly along the Whitby Ave and 54th St corridors. This concentration suggests a localized failure in current patrol strategies within PSA 124. We recommend an immediate ‚Äúsecurity audit‚Äù for these ten specific coordinates to identify micro-level drivers of risk‚Äîsuch as broken cameras, blind spots, or unmonitored alcoves‚Äîthat macro-level data might miss."
  },
  {
    "objectID": "Final/Final_appendix.html#beyond-policing-cpted-interventions-for-long-term-safety",
    "href": "Final/Final_appendix.html#beyond-policing-cpted-interventions-for-long-term-safety",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "7.3 Beyond Policing: CPTED Interventions for Long-term Safety",
    "text": "7.3 Beyond Policing: CPTED Interventions for Long-term Safety\n\nOur model highlights that policing is not the only solution. Built environment features, specifically Street Light Density and Housing Vacancy, were significant predictors of crime risk. The significance of vacancy rates supports the ‚ÄúBroken Windows Theory‚Äù‚Äîphysical disorder invites criminal activity. Therefore, we propose a cross-departmental collaboration between SEPTA and the Philadelphia Department of Streets to prioritize lighting upgrades and blight remediation around high-risk bus stops. These ‚ÄúCrime Prevention Through Environmental Design‚Äù (CPTED) interventions offer a sustainable, non-punitive strategy to reduce crime risk without increasing arrest rates."
  },
  {
    "objectID": "Final/Final_appendix.html#equity-concern-and-limitations",
    "href": "Final/Final_appendix.html#equity-concern-and-limitations",
    "title": "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops",
    "section": "7.4 Equity Concern and Limitations",
    "text": "7.4 Equity Concern and Limitations\n\nA major concern in crime modeling is the potential to stigmatize disadvantaged communities. We addressed this by incorporating Census Tract Fixed Effects (Phase 4) and controlling for poverty rates. This ensures our model compares bus stops within the same neighborhood context, rather than unfairly comparing a stop in North Philadelphia to one in Center City. However, we acknowledge that our final model still exhibits some spatial autocorrelation (Moran‚Äôs I \\(\\approx\\) 0.29), indicating that unobserved spatial clusters remain.\nWhile the Negative Binomial model effectively handles the over-dispersed count data, future iterations should explore Bayesian Spatial Models to fully resolve the complex spatial dependencies inherent in urban crime data."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Load necessary libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE, tigris_class = \"sf\")\nlibrary(MASS)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(nngeo)\nlibrary(car)\nlibrary(knitr)\nlibrary(readr)\nlibrary(patchwork)\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  \n\n\n1.1 Load and clean Philadelphia sales data:\n\n1.1.1 Load data\n\n\n\nCode\nlibrary(tidyverse)\nopa &lt;- read_csv(\"opa_properties_public1.csv\")\n\n\n\n1.1.2 Filter to residential properties, 2023-2024 sales\n\n\n\nCode\n# data in 2022 will be used as predictor, so keep them as well.\nopa_clean &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# record the amount of data we will focus on\nopa_clean2 &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# Select relevant variables \nopa_var &lt;- opa_clean %&gt;%\n  dplyr::select(\n    sale_date, sale_price, market_value, building_code_description,\n    total_livable_area, number_of_bedrooms, number_of_bathrooms,\n    number_stories, garage_spaces, central_air, quality_grade,\n    interior_condition, exterior_condition, year_built, \n    off_street_open, zip_code, census_tract, zoning, owner_1,\n    category_code_description, shape, fireplaces\n  )\n\n\n\n1.1.3 Remove obvious errors & Handle missing values\n\n\n\nCode\n# ! check before remove NA value\n\ncat(\"&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\")\n\n\n&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\n\n\nCode\n#remove errors and drop rows with small NA counts in specific columns\nopa_var &lt;- opa_var %&gt;% \n  distinct() %&gt;%  #Remove duplicate lines\n  filter(\n    !is.na(total_livable_area) & total_livable_area &gt; 0,\n    !is.na(year_built) & year_built &gt; 0 & year_built &lt; 2025,\n    !is.na(number_of_bathrooms),\n    !is.na(fireplaces),\n    !is.na(interior_condition),\n    garage_spaces&lt;30\n  )   \n\n\n\n1.1.4 Other cleaning decisions\n\n\n\nCode\n#numeric quality_grade\nvalid_grades &lt;- c(\"A+\", \"A\", \"A-\", \n                  \"B+\", \"B\", \"B-\", \n                  \"C+\", \"C\", \"C-\", \n                  \"D+\", \"D\", \"D-\", \n                  \"E+\", \"E\", \"E-\")\n\nopa_var &lt;- opa_var %&gt;%\n  filter(quality_grade %in% valid_grades) %&gt;%\n  mutate(\n    quality_grade = factor(quality_grade, levels = valid_grades, ordered = TRUE),\n    quality_grade_num = rev(seq_along(valid_grades))[as.numeric(quality_grade)]\n  )\n\n#central_air (keep and transform the large NA values)\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    central_air_dummy = case_when(\n      central_air %in% c(1, \"Y\", \"y\") ~ 1,\n      central_air %in% c(0, \"N\", \"n\") ~ 0,\n      TRUE ~ NA_real_\n    ),\n    central_air_missing = if_else(is.na(central_air), 1, 0),\n    central_air_dummy = if_else(is.na(central_air_dummy), 0, central_air_dummy)\n  )\n\n#house age\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    house_age = 2025 - year_built\n  )\n\n\n\n1.1.5 Explore structural data biases and identify non-market transactions\n\n\n\nCode\n# check the relation of Sale Price ~ Market Value\noptions(scipen = 999)\nplot(opa_var$sale_price, opa_var$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value (Predicted)  vs  Sale Price (Actual)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# create a new column to record the identified non-market transactions\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    non_market = \n      (((sale_price &lt; 0.10*market_value) | sale_price &lt; 2000) | (sale_price&gt; 5*market_value))\n  )\n\n\nInterpretation: - The goal of this model is to predict typical, market-driven sale prices. The provided scatter plot of Market Value (Predicted) vs.¬†Sale Price (Actual) is an essential diagnostic tool for assessing data quality. - The red line on the plot represents a perfect 1-to-1 relationship (Y=X), where the property‚Äôs actual sale price is exactly equal to its predicted market value. While most data points cluster tightly around this line‚Äîindicating the ‚ÄúMarket Value‚Äù is a strong predictor‚Äîthe plot clearly reveals two distinct types of extreme outliers that do not represent typical market transactions. 1. Removing Non-Market Sales (The Sale Price &lt; 0.05 * Market Value Rule) - There is a dense cluster of points stacked vertically along the y-axis, where Sale Price (X) is near zero, but Market Value (Y) is high (e.g., $1M, $2M, even $4M). These points represent transactions where a property was ‚Äúsold‚Äù for a price that is a tiny fraction of its assessed worth (e.g., a $2,000,000 house sold for $50,000). - These are non-arm‚Äôs-length transactions, not true market sales. Examples include sales between family members, inheritance transfers, or other legal transactions where the price does not reflect the market. 2. Removing Anomalous High Sales (The Sale Price &gt; 5 * Market Value Rule) - There are several data points scattered far to the right and bottom of the plot, far below the red Y=X line. These points represent properties where the Sale Price (X) was massively higher than its Market Value (Y). For example, a property with an assessed value of $500,000 (Y) selling for $4,000,000 (X). - These are also not typical sales. They could represent (a) data entry errors, (b) sales where the ‚ÄúMarket Value‚Äù figure is obsolete (e.g., a run-down property sold for land value/development potential), or (c) properties that underwent major renovations not yet captured in the assessed value. - Retaining these two groups of outliers would severely skew the model‚Äôs coefficients and reduce its predictive accuracy for the vast majority of normal, market-based transactions. This filtering rule is a necessary step to clean the data, ensure the model learns from valid transactions, and improve its overall reliability.\n\n1.1.6 enhance the sales data\n\nWe have 8000+ non market transactions, that is 1/4 of the total sales data! That‚Äôs too big to let go, The model that we want to generate will become much more stable if we can make use of them.\n\n\nCode\n# filter the REAL MARKET data in the time period we need\nopa_selected &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n  )   \n\n#filter the NON MARKET data in the time period we need\nopa_non_market &lt;- opa_var %&gt;%\n  filter(\n    non_market ==1,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n    )\n\nopa_selected2 &lt;- opa_var\nopa_bonus &lt;- opa_var \n\n\n\n\nCode\n#try to find the relationship between market_value and sale_price\noptions(scipen = 999)\nplot(opa_selected$sale_price, opa_selected$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value vs  Sale Price (cleaned)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# That's quite linear, let's try to build a simple OLS model\nopa_mdata &lt;- opa_bonus %&gt;%\n  filter(\n    non_market == 0\n    )\n\nmodel_non &lt;- lm(sale_price ~ market_value, data = opa_mdata)\nsummary(model_non)\n\n\n\nCall:\nlm(formula = sale_price ~ market_value, data = opa_mdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1432832   -35207    -1669    30014  1975714 \n\nCoefficients:\n                 Estimate   Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  -9189.286424   663.771782  -13.84 &lt;0.0000000000000002 ***\nmarket_value     1.027924     0.001773  579.62 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 86310 on 40927 degrees of freedom\nMultiple R-squared:  0.8914,    Adjusted R-squared:  0.8914 \nF-statistic: 3.36e+05 on 1 and 40927 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\npred &lt;- predict(model_non, newdata = opa_mdata)\nresid &lt;- opa_mdata$sale_price - pred\n\n# RMSE\nrmse_value &lt;- sqrt(mean(resid^2, na.rm = TRUE))\nrmse_value\n\n\n[1] 86311.29\n\n\nThe results demonstrate a very strong linear relationship between sale_price and market_value. The market_value variable in the dataset effectively predicts the sale_price. Therefore, we can leverage this relationship to estimate the normal market prices for non-market transactions. We record these estimated values as sale_price_predicted. By doing this, we enhance our data!\n\n\nCode\n#bring data back\nopa_non_market$sale_price_predicted &lt;- predict(model_non, newdata = opa_non_market)\n\n#join back to the main data\nopa_selected &lt;- opa_selected %&gt;%\n  mutate(sale_price_predicted= sale_price)\n\nset.seed(123)\nopa_bind &lt;- bind_rows(opa_selected, opa_non_market) %&gt;%\n  slice_sample(prop = 1)\n\n\n1.2 Load and clean secondary dataset:\n\n1.2.1 Census data (tidycensus):\n\n\n\nCode\n# Transform to sf object \nopa_bind &lt;- st_as_sf(opa_bind, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(4326)\nopa_sf&lt;- opa_bind\n\n\n\n\nCode\n# Load Census data for Philadelphia tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    \n    ba_degree = \"B15003_022\",\n    total_edu = \"B15003_001\",\n    \n    median_income = \"B19013_001\",\n   \n    labor_force = \"B23025_003\",\n    unemployed = \"B23025_005\",\n    \n    total_housing = \"B25002_001\",\n    vacant_housing = \"B25002_003\"\n  ),\n  year = 2023,\n  state = \"PA\",\n  county = \"Philadelphia\",\n  geometry = TRUE\n) %&gt;%\n  dplyr::select(GEOID, variable, estimate, geometry) %&gt;%   \n  tidyr::pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  dplyr::mutate(\n    ba_rate = 100 * ba_degree / total_edu,\n    unemployment_rate = 100 * unemployed / labor_force,\n    vacancy_rate = 100 * vacant_housing / total_housing\n  ) %&gt;%\n  st_transform(st_crs(opa_sf))\n\n# Spatial join of OPA data with Census data\nopa_census &lt;- st_join(opa_sf, philly_census, join = st_within) %&gt;%\n  filter(!is.na(median_income))\n\n\n\n1.2.2 Spatial amenities (OpenDataPhilly)\n\n\n\nCode\n#load crime,poi,transit,hospital\nopa_census &lt;- st_transform(opa_census, 3857)\n\ncrime &lt;- read_csv(\"crime_sel.csv\") %&gt;% \n  filter(!is.na(lat) & !is.na(lng)) \ncrime_sf &lt;- st_as_sf(crime, coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census)) \n\npoi_sf &lt;- st_read(\"data/gis_osm_pois_a_free_1.shp\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census)) \n\nTransit &lt;- read_csv(\"Transit.csv\")\ntransit_sf &lt;- st_as_sf(Transit, coords = c(\"Lon\", \"Lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census))\n\nhospital_sf &lt;- st_read(\"hospitals.geojson\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census))\n\n\n1.3 Summary tables showing before/after dimensions\n\n\nCode\n# Original data dimensions\nopa_dims &lt;- tibble(\n  dataset = \"raw CSV\",\n  rows = nrow(opa),\n  columns = ncol(opa)\n)\n\n# cleaned data dimensions\nopa_filter_dims &lt;- tibble(\n  dataset = \"after fixed criteria\",\n  rows = nrow(opa_clean2),\n  columns = ncol(opa_clean2)\n)\n\nopa_selected_dims &lt;- tibble(\n  dataset = \"after cleaned\",\n  rows = nrow(opa_bind),\n  columns = ncol(opa_bind)\n)\n# data dimensions (within census tracts)\nopa_census_dims &lt;- tibble(\n  dataset = \"after census joined\",\n  rows = nrow(opa_census),\n  columns = ncol(opa_census)\n)\n\n# create summary table\nsummary_table &lt;- bind_rows(opa_dims, opa_filter_dims,opa_selected_dims, opa_census_dims)\nlibrary(knitr)\nsummary_table %&gt;%\n  kable(caption = \"Summary of OPA data before and after cleaning\")\n\n\n\nSummary of OPA data before and after cleaning\n\n\ndataset\nrows\ncolumns\n\n\n\n\nraw CSV\n153267\n79\n\n\nafter fixed criteria\n34559\n79\n\n\nafter cleaned\n31968\n28\n\n\nafter census joined\n31613\n40\n\n\n\n\n\nPrinciples of Data Processing\n\nopa Data\n\nFilter sale_date between 2023-01-01 and 2024-12-31.\n\nKeep only residential properties (category_code == 1).\n\nRemove records with missing values in total_livable_area, sale_price, or number_of_bathrooms.\n\nFilter records year_built &gt; 0 .\n\nCensus data\n\nLoad data including total_pop, ba_degree, total_edu, median_income, labor_force, unemployed, total_housing, vacant_housing.\nTransform to spatial format and remove records with missing values.\n\nSpatial amenities\n\nLoad datasets Transit, crime, POIs, Hospitals.\nTransform to spatial format and remove records with missing values.\n\n\nInterpretation: The selected variables can be grouped into several meaningful categories that are theoretically and empirically linked to housing prices:\n- Neighborhood Safety: - Crime data: Areas with lower crime rates are generally more desirable, leading to higher property values. Including crime incidents helps capture the effect of public safety on housing prices. - Accessibility and Transportation: - Transit points: Proximity to public transportation (e.g., bus stops, subway stations) increases accessibility and convenience, which is often capitalized into higher home values. - Points of Interest (POIs): Nearby amenities such as shops, restaurants, and parks improve quality of life and can positively influence housing prices. - Healthcare Access: - Hospitals: Easy access to healthcare facilities is a valued neighborhood characteristic, especially for families and older residents, and can contribute to higher property values. - Socioeconomic and Demographic Context (from Census data): - Total population: Indicates population density, which can affect demand for housing. - Educational attainment (e.g., % with BA degree): Higher education levels are often correlated with higher income and neighborhood desirability. - Median income: Directly influences purchasing power and demand for housing in an area. - Employment status (labor force and unemployment): Reflects economic stability and local job market health, which affect housing demand. - Housing market conditions (total and vacant housing): Vacancy rates can signal neighborhood decline or oversupply, both of which impact prices. - Together, these variables provide a multidimensional view of a neighborhood‚Äôs appeal, safety, convenience, and economic health‚Äîall key determinants of housing prices."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-1-data-preparation",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-1-data-preparation",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Load necessary libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE, tigris_class = \"sf\")\nlibrary(MASS)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(nngeo)\nlibrary(car)\nlibrary(knitr)\nlibrary(readr)\nlibrary(patchwork)\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  \n\n\n1.1 Load and clean Philadelphia sales data:\n\n1.1.1 Load data\n\n\n\nCode\nlibrary(tidyverse)\nopa &lt;- read_csv(\"opa_properties_public1.csv\")\n\n\n\n1.1.2 Filter to residential properties, 2023-2024 sales\n\n\n\nCode\n# data in 2022 will be used as predictor, so keep them as well.\nopa_clean &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# record the amount of data we will focus on\nopa_clean2 &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# Select relevant variables \nopa_var &lt;- opa_clean %&gt;%\n  dplyr::select(\n    sale_date, sale_price, market_value, building_code_description,\n    total_livable_area, number_of_bedrooms, number_of_bathrooms,\n    number_stories, garage_spaces, central_air, quality_grade,\n    interior_condition, exterior_condition, year_built, \n    off_street_open, zip_code, census_tract, zoning, owner_1,\n    category_code_description, shape, fireplaces\n  )\n\n\n\n1.1.3 Remove obvious errors & Handle missing values\n\n\n\nCode\n# ! check before remove NA value\n\ncat(\"&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\")\n\n\n&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\n\n\nCode\n#remove errors and drop rows with small NA counts in specific columns\nopa_var &lt;- opa_var %&gt;% \n  distinct() %&gt;%  #Remove duplicate lines\n  filter(\n    !is.na(total_livable_area) & total_livable_area &gt; 0,\n    !is.na(year_built) & year_built &gt; 0 & year_built &lt; 2025,\n    !is.na(number_of_bathrooms),\n    !is.na(fireplaces),\n    !is.na(interior_condition),\n    garage_spaces&lt;30\n  )   \n\n\n\n1.1.4 Other cleaning decisions\n\n\n\nCode\n#numeric quality_grade\nvalid_grades &lt;- c(\"A+\", \"A\", \"A-\", \n                  \"B+\", \"B\", \"B-\", \n                  \"C+\", \"C\", \"C-\", \n                  \"D+\", \"D\", \"D-\", \n                  \"E+\", \"E\", \"E-\")\n\nopa_var &lt;- opa_var %&gt;%\n  filter(quality_grade %in% valid_grades) %&gt;%\n  mutate(\n    quality_grade = factor(quality_grade, levels = valid_grades, ordered = TRUE),\n    quality_grade_num = rev(seq_along(valid_grades))[as.numeric(quality_grade)]\n  )\n\n#central_air (keep and transform the large NA values)\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    central_air_dummy = case_when(\n      central_air %in% c(1, \"Y\", \"y\") ~ 1,\n      central_air %in% c(0, \"N\", \"n\") ~ 0,\n      TRUE ~ NA_real_\n    ),\n    central_air_missing = if_else(is.na(central_air), 1, 0),\n    central_air_dummy = if_else(is.na(central_air_dummy), 0, central_air_dummy)\n  )\n\n#house age\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    house_age = 2025 - year_built\n  )\n\n\n\n1.1.5 Explore structural data biases and identify non-market transactions\n\n\n\nCode\n# check the relation of Sale Price ~ Market Value\noptions(scipen = 999)\nplot(opa_var$sale_price, opa_var$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value (Predicted)  vs  Sale Price (Actual)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# create a new column to record the identified non-market transactions\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    non_market = \n      (((sale_price &lt; 0.10*market_value) | sale_price &lt; 2000) | (sale_price&gt; 5*market_value))\n  )\n\n\nInterpretation: - The goal of this model is to predict typical, market-driven sale prices. The provided scatter plot of Market Value (Predicted) vs.¬†Sale Price (Actual) is an essential diagnostic tool for assessing data quality. - The red line on the plot represents a perfect 1-to-1 relationship (Y=X), where the property‚Äôs actual sale price is exactly equal to its predicted market value. While most data points cluster tightly around this line‚Äîindicating the ‚ÄúMarket Value‚Äù is a strong predictor‚Äîthe plot clearly reveals two distinct types of extreme outliers that do not represent typical market transactions. 1. Removing Non-Market Sales (The Sale Price &lt; 0.05 * Market Value Rule) - There is a dense cluster of points stacked vertically along the y-axis, where Sale Price (X) is near zero, but Market Value (Y) is high (e.g., $1M, $2M, even $4M). These points represent transactions where a property was ‚Äúsold‚Äù for a price that is a tiny fraction of its assessed worth (e.g., a $2,000,000 house sold for $50,000). - These are non-arm‚Äôs-length transactions, not true market sales. Examples include sales between family members, inheritance transfers, or other legal transactions where the price does not reflect the market. 2. Removing Anomalous High Sales (The Sale Price &gt; 5 * Market Value Rule) - There are several data points scattered far to the right and bottom of the plot, far below the red Y=X line. These points represent properties where the Sale Price (X) was massively higher than its Market Value (Y). For example, a property with an assessed value of $500,000 (Y) selling for $4,000,000 (X). - These are also not typical sales. They could represent (a) data entry errors, (b) sales where the ‚ÄúMarket Value‚Äù figure is obsolete (e.g., a run-down property sold for land value/development potential), or (c) properties that underwent major renovations not yet captured in the assessed value. - Retaining these two groups of outliers would severely skew the model‚Äôs coefficients and reduce its predictive accuracy for the vast majority of normal, market-based transactions. This filtering rule is a necessary step to clean the data, ensure the model learns from valid transactions, and improve its overall reliability.\n\n1.1.6 enhance the sales data\n\nWe have 8000+ non market transactions, that is 1/4 of the total sales data! That‚Äôs too big to let go, The model that we want to generate will become much more stable if we can make use of them.\n\n\nCode\n# filter the REAL MARKET data in the time period we need\nopa_selected &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n  )   \n\n#filter the NON MARKET data in the time period we need\nopa_non_market &lt;- opa_var %&gt;%\n  filter(\n    non_market ==1,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n    )\n\nopa_selected2 &lt;- opa_var\nopa_bonus &lt;- opa_var \n\n\n\n\nCode\n#try to find the relationship between market_value and sale_price\noptions(scipen = 999)\nplot(opa_selected$sale_price, opa_selected$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value vs  Sale Price (cleaned)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# That's quite linear, let's try to build a simple OLS model\nopa_mdata &lt;- opa_bonus %&gt;%\n  filter(\n    non_market == 0\n    )\n\nmodel_non &lt;- lm(sale_price ~ market_value, data = opa_mdata)\nsummary(model_non)\n\n\n\nCall:\nlm(formula = sale_price ~ market_value, data = opa_mdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1432832   -35207    -1669    30014  1975714 \n\nCoefficients:\n                 Estimate   Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  -9189.286424   663.771782  -13.84 &lt;0.0000000000000002 ***\nmarket_value     1.027924     0.001773  579.62 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 86310 on 40927 degrees of freedom\nMultiple R-squared:  0.8914,    Adjusted R-squared:  0.8914 \nF-statistic: 3.36e+05 on 1 and 40927 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\npred &lt;- predict(model_non, newdata = opa_mdata)\nresid &lt;- opa_mdata$sale_price - pred\n\n# RMSE\nrmse_value &lt;- sqrt(mean(resid^2, na.rm = TRUE))\nrmse_value\n\n\n[1] 86311.29\n\n\nThe results demonstrate a very strong linear relationship between sale_price and market_value. The market_value variable in the dataset effectively predicts the sale_price. Therefore, we can leverage this relationship to estimate the normal market prices for non-market transactions. We record these estimated values as sale_price_predicted. By doing this, we enhance our data!\n\n\nCode\n#bring data back\nopa_non_market$sale_price_predicted &lt;- predict(model_non, newdata = opa_non_market)\n\n#join back to the main data\nopa_selected &lt;- opa_selected %&gt;%\n  mutate(sale_price_predicted= sale_price)\n\nset.seed(123)\nopa_bind &lt;- bind_rows(opa_selected, opa_non_market) %&gt;%\n  slice_sample(prop = 1)\n\n\n1.2 Load and clean secondary dataset:\n\n1.2.1 Census data (tidycensus):\n\n\n\nCode\n# Transform to sf object \nopa_bind &lt;- st_as_sf(opa_bind, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(4326)\nopa_sf&lt;- opa_bind\n\n\n\n\nCode\n# Load Census data for Philadelphia tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    \n    ba_degree = \"B15003_022\",\n    total_edu = \"B15003_001\",\n    \n    median_income = \"B19013_001\",\n   \n    labor_force = \"B23025_003\",\n    unemployed = \"B23025_005\",\n    \n    total_housing = \"B25002_001\",\n    vacant_housing = \"B25002_003\"\n  ),\n  year = 2023,\n  state = \"PA\",\n  county = \"Philadelphia\",\n  geometry = TRUE\n) %&gt;%\n  dplyr::select(GEOID, variable, estimate, geometry) %&gt;%   \n  tidyr::pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  dplyr::mutate(\n    ba_rate = 100 * ba_degree / total_edu,\n    unemployment_rate = 100 * unemployed / labor_force,\n    vacancy_rate = 100 * vacant_housing / total_housing\n  ) %&gt;%\n  st_transform(st_crs(opa_sf))\n\n# Spatial join of OPA data with Census data\nopa_census &lt;- st_join(opa_sf, philly_census, join = st_within) %&gt;%\n  filter(!is.na(median_income))\n\n\n\n1.2.2 Spatial amenities (OpenDataPhilly)\n\n\n\nCode\n#load crime,poi,transit,hospital\nopa_census &lt;- st_transform(opa_census, 3857)\n\ncrime &lt;- read_csv(\"crime_sel.csv\") %&gt;% \n  filter(!is.na(lat) & !is.na(lng)) \ncrime_sf &lt;- st_as_sf(crime, coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census)) \n\npoi_sf &lt;- st_read(\"data/gis_osm_pois_a_free_1.shp\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census)) \n\nTransit &lt;- read_csv(\"Transit.csv\")\ntransit_sf &lt;- st_as_sf(Transit, coords = c(\"Lon\", \"Lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census))\n\nhospital_sf &lt;- st_read(\"hospitals.geojson\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census))\n\n\n1.3 Summary tables showing before/after dimensions\n\n\nCode\n# Original data dimensions\nopa_dims &lt;- tibble(\n  dataset = \"raw CSV\",\n  rows = nrow(opa),\n  columns = ncol(opa)\n)\n\n# cleaned data dimensions\nopa_filter_dims &lt;- tibble(\n  dataset = \"after fixed criteria\",\n  rows = nrow(opa_clean2),\n  columns = ncol(opa_clean2)\n)\n\nopa_selected_dims &lt;- tibble(\n  dataset = \"after cleaned\",\n  rows = nrow(opa_bind),\n  columns = ncol(opa_bind)\n)\n# data dimensions (within census tracts)\nopa_census_dims &lt;- tibble(\n  dataset = \"after census joined\",\n  rows = nrow(opa_census),\n  columns = ncol(opa_census)\n)\n\n# create summary table\nsummary_table &lt;- bind_rows(opa_dims, opa_filter_dims,opa_selected_dims, opa_census_dims)\nlibrary(knitr)\nsummary_table %&gt;%\n  kable(caption = \"Summary of OPA data before and after cleaning\")\n\n\n\nSummary of OPA data before and after cleaning\n\n\ndataset\nrows\ncolumns\n\n\n\n\nraw CSV\n153267\n79\n\n\nafter fixed criteria\n34559\n79\n\n\nafter cleaned\n31968\n28\n\n\nafter census joined\n31613\n40\n\n\n\n\n\nPrinciples of Data Processing\n\nopa Data\n\nFilter sale_date between 2023-01-01 and 2024-12-31.\n\nKeep only residential properties (category_code == 1).\n\nRemove records with missing values in total_livable_area, sale_price, or number_of_bathrooms.\n\nFilter records year_built &gt; 0 .\n\nCensus data\n\nLoad data including total_pop, ba_degree, total_edu, median_income, labor_force, unemployed, total_housing, vacant_housing.\nTransform to spatial format and remove records with missing values.\n\nSpatial amenities\n\nLoad datasets Transit, crime, POIs, Hospitals.\nTransform to spatial format and remove records with missing values.\n\n\nInterpretation: The selected variables can be grouped into several meaningful categories that are theoretically and empirically linked to housing prices:\n- Neighborhood Safety: - Crime data: Areas with lower crime rates are generally more desirable, leading to higher property values. Including crime incidents helps capture the effect of public safety on housing prices. - Accessibility and Transportation: - Transit points: Proximity to public transportation (e.g., bus stops, subway stations) increases accessibility and convenience, which is often capitalized into higher home values. - Points of Interest (POIs): Nearby amenities such as shops, restaurants, and parks improve quality of life and can positively influence housing prices. - Healthcare Access: - Hospitals: Easy access to healthcare facilities is a valued neighborhood characteristic, especially for families and older residents, and can contribute to higher property values. - Socioeconomic and Demographic Context (from Census data): - Total population: Indicates population density, which can affect demand for housing. - Educational attainment (e.g., % with BA degree): Higher education levels are often correlated with higher income and neighborhood desirability. - Median income: Directly influences purchasing power and demand for housing in an area. - Employment status (labor force and unemployment): Reflects economic stability and local job market health, which affect housing demand. - Housing market conditions (total and vacant housing): Vacancy rates can signal neighborhood decline or oversupply, both of which impact prices. - Together, these variables provide a multidimensional view of a neighborhood‚Äôs appeal, safety, convenience, and economic health‚Äîall key determinants of housing prices."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-2-feature-engineering",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-2-feature-engineering",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 2: Feature Engineering",
    "text": "Phase 2: Feature Engineering\n2.1 Buffer-based features:\n\n2.1.1 neighborhood avg sale price in the past year\n\n\n\nCode\n#filter the past sales data\n\nopa_census &lt;- opa_census %&gt;%\n  mutate(sale_id = row_number())\n\n\n\nopa_past &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2022-12-31\"\n  ) \n\nopa_past &lt;- opa_past %&gt;%\n  mutate(sale_id2 = row_number())\n\nopa_past &lt;- st_as_sf(opa_past, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(3857)\n\nopa_census &lt;- st_transform(opa_census, 3857)\nopa_past   &lt;- st_transform(opa_past, 3857)\n\nopa_census_buffer &lt;- st_buffer(opa_census, 300)\ndrop_cols &lt;- names(opa_census_buffer) %in% c(\"sale_price\", \"total_livable_area\",\n                                             \"sale_price.y\", \"total_livable_area.y\")\nopa_census_buffer &lt;- opa_census_buffer[ , !drop_cols, drop = FALSE]\n\njoin_result &lt;- st_join(\n  opa_census_buffer,\n  opa_past,\n  join = st_intersects,\n  left = TRUE\n)\n\n\njoin_result &lt;- st_join(\n  opa_census_buffer,\n  opa_past,\n  join = st_intersects,\n  left = TRUE\n)\njoin_dedup &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  distinct(sale_id, sale_id2, .keep_all = TRUE)\n\nopa_summary &lt;- join_dedup %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(\n    past_count = sum(!is.na(sale_id2)),\n    avg_past_price_density = ifelse(\n      sum(!is.na(total_livable_area)) == 0, NA_real_,\n      sum(sale_price, na.rm = TRUE) / sum(total_livable_area, na.rm = TRUE)\n    ),\n    .groups = \"drop\"\n  )\nopa_census &lt;- opa_census %&gt;%\n  left_join(opa_summary, by = \"sale_id\")\n\n\n\n2.1.2 crime numbers\n\n\n\nCode\nradius_cri &lt;- 250 \n\nopa_census$crime_count &lt;- lengths(st_is_within_distance(opa_census, crime_sf, dist = radius_cri)) \n\n\n\n2.1.3 POI numbers\n\n\n\nCode\nopa_census_m &lt;- st_transform(opa_census, 3857)\npoi_sf_m     &lt;- st_transform(poi_sf, 3857)\n\n\nradius_poi &lt;- 400\nopa_census_buffer &lt;- st_buffer(opa_census_m, radius_poi)\n\njoin_result &lt;- st_join(opa_census_buffer, poi_sf_m, join = st_intersects, left = TRUE)\n\npoi_summary &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(poi_count = sum(!is.na(osm_id)))  \n\nopa_census &lt;- opa_census_m %&gt;%\n  left_join(poi_summary, by = \"sale_id\")\n\n\n\n2.1.4 Transit numbers\n\n\n\nCode\nopa_census_m &lt;- st_transform(opa_census, 3857)\ntransit_sf_m &lt;- st_transform(transit_sf, 3857)\n\nradius_ts &lt;- 400\nopa_census_buffer &lt;- st_buffer(opa_census_m, radius_ts)\n\njoin_result &lt;- st_join(opa_census_buffer, transit_sf_m, join = st_intersects, left = TRUE)\n\ntransit_summary &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(transit_count = sum(!is.na(FID)))  \n\nopa_census &lt;- opa_census_m %&gt;%\n  left_join(transit_summary, by = \"sale_id\")\n\n\n2.2 k-Nearest Neighbor features:\n\n2.2.1 Hospitals (KNN-3)\n\n\n\nCode\nnearest_hospital_index &lt;- st_nn(\n  opa_census,\n  hospital_sf,\n  k = 3,            \n  returnDist = TRUE \n)\n\nopa_census$nearest_hospital_knn3 &lt;- sapply(nearest_hospital_index$dist, mean)\n\n\n2.3 Weights:\n\n\nCode\n# add different weights to actual and non market transactions\n\nopa_census_all &lt;- opa_census\n\nnon_market_share&lt;- mean(opa_census_all$non_market == 1, na.rm = TRUE)\nnon_market_share\n\n\n[1] 0.261791\n\n\nCode\nopa_census_all &lt;- opa_census %&gt;%\n  mutate(weight_mix = ifelse(non_market == 1, non_market_share, 1))\n\n\n2.4 Transformation:\n\n\nCode\n#standardize houseage and median income\nmean_age_all &lt;- mean(opa_census_all$house_age, na.rm = TRUE)\nmean_income_log &lt;- mean(log(opa_census_all$median_income), na.rm = TRUE)\n\n# centralize\nopa_census_all &lt;- opa_census_all %&gt;%\n  mutate(\n    house_age_c  = house_age - mean_age_all,\n    house_age_c2 = house_age_c^2,\n    income_log   = log(median_income),\n    income_scaled = income_log - mean_income_log\n  )\n\nopa_census_2&lt;- opa_census_all %&gt;%\n  filter(\n    non_market==0\n  )"
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-3-exploratory-data-analysis",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-3-exploratory-data-analysis",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 3: Exploratory Data Analysis",
    "text": "Phase 3: Exploratory Data Analysis\n\n3.1 Distribution of sale prices (histogram)\n\n\nCode\nggplot(opa_census_all, aes(x = sale_price_predicted)) +\n  geom_histogram(\n    bins = 50,                 \n    fill = \"#6A1B9A\",          \n    color = \"white\",           \n    alpha = 0.8                \n  ) +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Distribution of  Sale Prices\",\n    x = \"Predicted Sale Price (USD)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(opa_census_all, aes(x = log(sale_price_predicted))) +\n  geom_histogram(\n    bins = 50,                 \n    fill = \"#6A1B9A\",          \n    color = \"white\",           \n    alpha = 0.8               \n  ) +\n  scale_x_continuous() +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Distribution of  Log(Sale Prices)\",\n    x = \"Log(Predicted Sale Price) \",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\nKey Findings: - Highly Right-Skewed: The bulk of the distribution is concentrated on the left side (low price range), while the right tail is very long, extending towards higher prices. This means that the majority of houses have lower prices, and very expensive houses are very few in number.\n- Concentration and Outliers: The count of houses in each bin drops rapidly as the price increases. The number of houses above $2,500,000 is very small, indicating the presence of extreme high-price outliers.\n- Preprocessing Requirement: This distribution strongly suggests that before building a house price prediction model, the Sale Price variable will need a transformation, most commonly a log transformation, to make the distribution more closely resemble a normal distribution.\n\n\n3.2 Spatial distribution of sale prices (map)\n\n\nCode\nopa_census_all &lt;- opa_census_all %&gt;%\n  mutate(price_quartile = ntile(sale_price_predicted, 4))\n\nggplot() +\n  geom_sf(data = philly_census, fill = \"lightgrey\", color = \"white\") +\n  geom_sf(data = opa_census_all, aes(color = factor(price_quartile)), size = 0.5, alpha = 0.7) +\n  scale_color_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    labels = c(\"0%-25%\", \"25%-50%\", \"50%-75%\", \"75%-100%\")\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Housing Sales Price in Philadelphia (2023‚Äì2024)\",\n    color = \"Sale Price Quartile\"\n  )  +\n  guides(color = guide_legend(override.aes = list(size = 3)))\n\n\n\n\n\n\n\n\n\nKey Findings:\n- Spatial Concentration of Housing Prices: Highest Prices are clearly concentrated in the Center City/Downtown area of Philadelphia and its immediate surroundings, which indicates that the most expensive property transactions occur in the high-value areas of and around the city center.\n- Discontinuous Price Transitions: Housing prices do not exhibit smooth gradients across the city; instead, they show abrupt changes between different price quartiles, forming distinct spatial clusters. This suggests that price variations are influenced by fixed boundaries such as neighborhoods, infrastructure, or socio-economic factors, rather than continuous spatial diffusion. - Peripheral Price Patterns: Lower-price quartiles (0%-25% and 25%-50%) are predominantly located in the outer regions of the city, particularly in the northern and southern peripheries, indicating a clear core-periphery divide in housing values.\n\n\n3.3 Price vs.¬†structural features (scatter plots)\n\n\nCode\nopa_census_plot &lt;- opa_census_all %&gt;%\n  mutate(log_sale_price = log(sale_price_predicted))\n\n# 1Ô∏è‚É£ total_livable_area\np1 &lt;- ggplot(opa_census_plot, aes(x = log(total_livable_area), y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Log(Livable Area)\",\n    x = \"Log(Total Livable Area)\",\n    y = \"Log(Sale Price)\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 2Ô∏è‚É£ number_of_bathrooms\np2 &lt;- ggplot(opa_census_plot, aes(x = number_of_bathrooms, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Bathrooms\",\n    x = \"Number of Bathrooms\",  \n    y = \"\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 3Ô∏è‚É£ interior_condition\np3 &lt;- ggplot(opa_census_plot, aes(x = interior_condition, y = log_sale_price)) +\n  geom_jitter(alpha = 0.5, color = \"#6A1B9A\", width = 0.2, height = 0) +  \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Interior Condition\",\n    x = \"Interior Condition\",\n    y = \"Log(Sale Price)\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 4Ô∏è‚É£ house_age\np4 &lt;- ggplot(opa_census_plot, aes(x = house_age^2, y = log_sale_price)) + \n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. House Age¬≤\",\n    x = \"House Age¬≤ (2025 - Year Built)¬≤\",\n    y = \"\"\n  ) +\n  theme_minimal(base_size = 10)\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\nquarto check Key Findings:\n- Strong Positive Correlation with Size and Bathrooms: There is a clear positive linear relationship between the log of sale price and both the log of total livable area and the number of bathrooms. This indicates that larger properties and those with more bathrooms command significantly higher market prices. - Non-Linear Relationship with Interior Condition: While a general positive trend exists, the relationship between log sale price and interior condition rating is not perfectly linear. The data dispersion suggests that the effect of interior condition on price may be subject to diminishing marginal returns or other non-linear dynamics. - Negative Correlation with House Age Squared: A significant negative relationship is observed between log sale price and the squared term of house age. This indicates that property values depreciate as homes get older, and this depreciation effect may accelerate over time, reflecting a non-linear aging effect on housing value.\n\n\n3.4 Price vs.¬†spatial & Social features (scatter plots)\n\n\nCode\nopa_census_plot &lt;- opa_census_all %&gt;%\n  mutate(\n    log_sale_price = log(sale_price_predicted),\n    sqrt_crime_count = sqrt(crime_count)\n  )\n\np1 &lt;- ggplot(opa_census_plot, aes(x = ba_rate, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  labs(title = \"Log(Sale Price) vs. BA Rate\",\n       x = \"Bachelor's Degree Rate\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np2 &lt;- ggplot(opa_census_plot, aes(x = unemployment_rate, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  labs(title = \"Log(Sale Price) vs. Unemployment Rate\",\n       x = \"Unemployment Rate\", y = \"\") +\n  theme_minimal(base_size = 10)\n\np3 &lt;- ggplot(opa_census_plot, aes(x = median_income, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = dollar_format()) +\n  labs(title = \"Log(Sale Price) vs. Median Income\",\n       x = \"Median Household Income (USD)\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np4 &lt;- ggplot(opa_census_plot, aes(x = avg_past_price_density, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. Past Price Density\",\n       x = \"Average Past Price Density\", y = \"\") +\n  theme_minimal(base_size = 10)\n\np5 &lt;- ggplot(opa_census_plot, aes(x = sqrt_crime_count, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. ‚àö(Crime Count)\",\n       x = \"Square Root of Crime Count\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np6 &lt;- ggplot(opa_census_plot, aes(x = transit_count, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. Transit Count\",\n       x = \"Number of Transit Stops Nearby\", y = \"\") +\n  theme_minimal(base_size = 10)\n\n(p1 | p2 | p3) / (p4 | p5 | p6)\n\n\n\n\n\n\n\n\n\nKey Findings:\n- Strong Socioeconomic Influence: Housing prices show clear positive correlations with key socioeconomic indicators. Both bachelor‚Äôs degree rate and median household income exhibit strong positive relationships with sale prices, indicating that neighborhoods with higher educational attainment and income levels command substantially higher property values. - Negative Impact of Crime and Unemployment: There are evident negative relationships between housing prices and both crime levels (measured by square root of crime count) and unemployment rates. This demonstrates that public safety and local economic vitality are significant determinants of property values in Philadelphia. - Positive Effects of Historical Prices and Transit Access: Sale prices maintain a positive relationship with both historical price density and accessibility to public transportation. This suggests that areas with established high-value characteristics and better transit infrastructure maintain their premium in the housing market, reflecting path dependence in neighborhood valuation and the value of transportation accessibility.\n\n\nOther visualization\n\n\nCode\ntract_price &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(avg_past_price_density = mean(avg_past_price_density, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_price, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = avg_past_price_density), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"white\", high = \"#6A1B9A\",\n    name = \"Mean Sale Price Per sqft\",\n    labels = scales::dollar_format(),\n    na.value = \"grey60\"\n  ) +\n  scale_alpha(range = c(0.2, 0.8), name = \"Interior Condition\") +\n  \n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Buffered Area Mean Sold Price of 2022\",\n    subtitle = \"clustered in census tracts\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ntract_condition &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_interior_condition = mean(interior_condition, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_condition, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_interior_condition), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"#6A1B9A\", high = \"white\",      \n    name = \"Avg Interior Condition\",\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Average Interior Condition by Census Tract\",\n    subtitle = \"Darker color = better average condition\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ntract_area &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_total_livable_area = mean(total_livable_area, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_area, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_total_livable_area), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"white\", high = \"#6A1B9A\",   \n    name = \"Avg Livable Area (sqft)\",\n    labels = scales::comma_format(),\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Average Livable Area by Census Tract\",\n    subtitle = \"Darker color indicates larger average livable area\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\nKey Findings:\n- Spatial Variation in Property Values: The average sale price per square foot shows significant geographic clustering across census tracts, with distinct high-value areas concentrated in specific neighborhoods. This indicates strong spatial autocorrelation in housing prices, where adjacent tracts tend to have similar price levels. - Correlation Between Property Condition and Location: Better average interior conditions are systematically concentrated in particular geographic areas, suggesting that housing maintenance and quality are not randomly distributed but follow spatial patterns that may correlate with neighborhood characteristics and property values. - Heterogeneous Distribution of Housing Size: The average livable area varies substantially across census tracts, with larger properties clustered in specific regions. This spatial patterning of housing size complements the price distribution, indicating that both property characteristics and location factors contribute to the overall housing market structure in Philadelphia."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-4-model-building",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-4-model-building",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 4: Model Building",
    "text": "Phase 4: Model Building\n\nBuild models progressively\n4.1 Structural features only:\n\n\nCode\nmodel_1 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +  #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing, \n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_1)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing, data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-3.2930 -0.2144  0.0510  0.2911  2.4585 \n\nCoefficients:\n                            Estimate   Std. Error t value            Pr(&gt;|t|)\n(Intercept)              6.435857858  0.077629460  82.905 &lt;0.0000000000000002\nlog(total_livable_area)  0.744348571  0.010689344  69.635 &lt;0.0000000000000002\nnumber_of_bathrooms      0.051667122  0.005580674   9.258 &lt;0.0000000000000002\nhouse_age_c              0.000002273  0.000118374   0.019               0.985\nhouse_age_c2             0.000046457  0.000001746  26.607 &lt;0.0000000000000002\ninterior_condition      -0.112636239  0.004358818 -25.841 &lt;0.0000000000000002\nquality_grade_num        0.069613279  0.002849720  24.428 &lt;0.0000000000000002\nfireplaces               0.116405937  0.010884143  10.695 &lt;0.0000000000000002\ngarage_spaces            0.140429585  0.006549989  21.440 &lt;0.0000000000000002\ncentral_air_dummy        0.458743112  0.008036173  57.085 &lt;0.0000000000000002\ncentral_air_missing     -0.237637667  0.008809496 -26.975 &lt;0.0000000000000002\n                           \n(Intercept)             ***\nlog(total_livable_area) ***\nnumber_of_bathrooms     ***\nhouse_age_c                \nhouse_age_c2            ***\ninterior_condition      ***\nquality_grade_num       ***\nfireplaces              ***\ngarage_spaces           ***\ncentral_air_dummy       ***\ncentral_air_missing     ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4911 on 31602 degrees of freedom\nMultiple R-squared:  0.5212,    Adjusted R-squared:  0.521 \nF-statistic:  3439 on 10 and 31602 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation:\n- log(total_livable_area) (0.752): An elasticity coefficient. A 1% increase in livable area is associated with a 0.752% increase in price. This is a strong positive driver. - number_of_bathrooms (0.046): Each additional bathroom is associated with a 4.6% increase in price. - house_age_c (-0.00001): The linear term for house age is statistically insignificant (p=0.929). - house_age_c2 (0.000047): The squared term for age is positive and significant. Combined with the insignificant linear term, this suggests a slight U-shaped relationship, where new homes and very old homes (perhaps with historical value) command a premium over middle-aged homes. - interior_condition (-0.114): Assuming a higher value means worse condition, each one-unit worsening in condition is associated with an 11.4% decrease in price. - quality_grade_num (0.070): Each one-unit increase in the quality grade is associated with a 7.0% increase in price. - fireplaces (0.117): Each additional fireplace is associated with a 11.7% increase in price. - garage_spaces (0.143): Each additional garage space is associated with a 14.3% increase in price. - central_air_dummy (0.458): Homes with central air are estimated to be 45.8% more expensive than the baseline (e.g., no AC). This is a very significant amenity premium. - central_air_missing (-0.230): Homes where central air data is missing are 23.0% cheaper than the baseline.\n4.2 Census variables:\n\n\nCode\nmodel_2 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census\n                  ba_rate +\n                  unemployment_rate,\n  \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_2)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate, \n    data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-3.2514 -0.1415  0.0546  0.2237  2.0880 \n\nCoefficients:\n                            Estimate   Std. Error t value             Pr(&gt;|t|)\n(Intercept)              7.279501574  0.064443586 112.959 &lt; 0.0000000000000002\nlog(total_livable_area)  0.700431343  0.008755698  79.997 &lt; 0.0000000000000002\nnumber_of_bathrooms      0.057764788  0.004568488  12.644 &lt; 0.0000000000000002\nhouse_age_c             -0.000064246  0.000097377  -0.660                0.509\nhouse_age_c2             0.000009814  0.000001468   6.686    0.000000000023255\ninterior_condition      -0.126482820  0.003572164 -35.408 &lt; 0.0000000000000002\nquality_grade_num        0.001702051  0.002417186   0.704                0.481\nfireplaces               0.064233267  0.008917462   7.203    0.000000000000602\ngarage_spaces            0.168324520  0.005472052  30.761 &lt; 0.0000000000000002\ncentral_air_dummy        0.219136581  0.006851612  31.983 &lt; 0.0000000000000002\ncentral_air_missing     -0.155840316  0.007252663 -21.487 &lt; 0.0000000000000002\nincome_scaled            0.453407655  0.009052596  50.086 &lt; 0.0000000000000002\nba_rate                  0.012813063  0.000358216  35.769 &lt; 0.0000000000000002\nunemployment_rate       -0.006619614  0.000527486 -12.549 &lt; 0.0000000000000002\n                           \n(Intercept)             ***\nlog(total_livable_area) ***\nnumber_of_bathrooms     ***\nhouse_age_c                \nhouse_age_c2            ***\ninterior_condition      ***\nquality_grade_num          \nfireplaces              ***\ngarage_spaces           ***\ncentral_air_dummy       ***\ncentral_air_missing     ***\nincome_scaled           ***\nba_rate                 ***\nunemployment_rate       ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4017 on 31599 degrees of freedom\nMultiple R-squared:  0.6796,    Adjusted R-squared:  0.6794 \nF-statistic:  5155 on 13 and 31599 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation:\n- Coefficient Evolution (vs.¬†Model 1): - log(total_livable_area) (0.710 vs 0.752): The elasticity of area decreased. This suggests Model 1 overestimated the impact of area. Why? Because larger homes are often located in wealthier neighborhoods. Model 1 incorrectly attributed some of the ‚Äúwealthy neighborhood‚Äù premium to ‚Äúlarge area.‚Äù - quality_grade_num (0.0015 vs 0.070): The coefficient for quality grade became statistically insignificant (p=0.520). This is a key finding: home quality is highly correlated with neighborhood income. Once we directly control for income (income_scaled), the independent effect of quality grade disappears. - central_air_dummy (0.219 vs 0.458): The premium for central air was halved. This also indicates that central air is more common in affluent areas, and Model 1 suffered from significant Omitted Variable Bias (OVB). - New Variable (Census) Interpretation: - income_scaled (0.455): A one-unit increase in standardized census tract income is associated with a 45.5% increase in price. A very strong positive effect. - ba_rate (0.0129): A 1 percentage point increase in the neighborhood‚Äôs bachelor‚Äôs degree attainment rate is associated with a 1.3% price increase. - unemployment_rate (-0.0066): A 1 percentage point increase in the unemployment rate is associated with a 0.66% decrease in price.\n4.3 Spatial features:\n\n\nCode\nmodel_3 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census \n                  ba_rate +\n                  unemployment_rate +\n                \n                  transit_count+\n                  avg_past_price_density+      #Spatial \n                  sqrt(crime_count) +\n                  log(nearest_hospital_knn3),\n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_3)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate + \n    transit_count + avg_past_price_density + sqrt(crime_count) + \n    log(nearest_hospital_knn3), data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-3.03217 -0.09905  0.05666  0.18464  2.17114 \n\nCoefficients:\n                               Estimate   Std. Error t value\n(Intercept)                 6.562309985  0.084718417  77.460\nlog(total_livable_area)     0.742766329  0.007908779  93.917\nnumber_of_bathrooms         0.057510788  0.004057815  14.173\nhouse_age_c                -0.000093444  0.000087876  -1.063\nhouse_age_c2               -0.000005113  0.000001322  -3.866\ninterior_condition         -0.161139310  0.003179231 -50.685\nquality_grade_num          -0.029091664  0.002254432 -12.904\nfireplaces                  0.031025554  0.008023515   3.867\ngarage_spaces               0.096993528  0.005071631  19.125\ncentral_air_dummy           0.099045830  0.006176862  16.035\ncentral_air_missing        -0.164351875  0.006393736 -25.705\nincome_scaled               0.166039882  0.008635180  19.228\nba_rate                     0.003250996  0.000349765   9.295\nunemployment_rate          -0.002568034  0.000466955  -5.500\ntransit_count               0.000311944  0.000171205   1.822\navg_past_price_density      0.003018698  0.000039395  76.626\nsqrt(crime_count)          -0.049042531  0.001408152 -34.828\nlog(nearest_hospital_knn3)  0.081937118  0.006616703  12.383\n                                       Pr(&gt;|t|)    \n(Intercept)                &lt; 0.0000000000000002 ***\nlog(total_livable_area)    &lt; 0.0000000000000002 ***\nnumber_of_bathrooms        &lt; 0.0000000000000002 ***\nhouse_age_c                            0.287626    \nhouse_age_c2                           0.000111 ***\ninterior_condition         &lt; 0.0000000000000002 ***\nquality_grade_num          &lt; 0.0000000000000002 ***\nfireplaces                             0.000110 ***\ngarage_spaces              &lt; 0.0000000000000002 ***\ncentral_air_dummy          &lt; 0.0000000000000002 ***\ncentral_air_missing        &lt; 0.0000000000000002 ***\nincome_scaled              &lt; 0.0000000000000002 ***\nba_rate                    &lt; 0.0000000000000002 ***\nunemployment_rate                  0.0000000384 ***\ntransit_count                          0.068458 .  \navg_past_price_density     &lt; 0.0000000000000002 ***\nsqrt(crime_count)          &lt; 0.0000000000000002 ***\nlog(nearest_hospital_knn3) &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3536 on 31481 degrees of freedom\n  (114 observations deleted due to missingness)\nMultiple R-squared:  0.7505,    Adjusted R-squared:  0.7504 \nF-statistic:  5571 on 17 and 31481 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation:\n- Coefficient Evolution (vs.¬†Model 2): - income_scaled (0.146 vs 0.455): The effect of income dropped sharply (by ~2/3). This again reveals OVB in Model 2. The large ‚Äúincome‚Äù effect in Model 2 was confounded with ‚Äúspatial amenities‚Äù‚Äîhigh-income individuals tend to live in low-crime, accessible areas. - ba_rate (0.0027 vs 0.0129): The education premium also dropped significantly for the same reason. - garage_spaces (0.095 vs 0.170): The garage premium decreased, likely because spatial variables (like density or transit access) have captured related information. - New Variable (Spatial) Interpretation: - transit_count (0.00029): Each additional nearby public transit stop is associated with a 0.029% increase in price. - avg_past_price_density (0.0032): As a proxy for local market heat or locational value, each unit increase is associated with a 0.32% price increase. - sqrt(crime_count) (-0.040): A one-unit increase in the square root of the crime count is associated with a 4.0% decrease in price. - log(nearest_hospital_knn3) (0.087): A 1% increase in the distance from the nearest hospital is associated with a 0.087% increase in price. This suggests people prefer to live further away from hospitals (perhaps to avoid noise, traffic, or sirens), not closer.\n4.4 Interactions and fixed effects:\n\n\nCode\nmodel_4 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census \n                  ba_rate +\n                  unemployment_rate +\n                \n                  transit_count+\n                  avg_past_price_density+      #Spatial \n                  sqrt(crime_count) +\n                  log(nearest_hospital_knn3)+\n                \n                  (interior_condition * income_scaled)+  #FE & Interaction\n                  factor(zip_code),\n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_4)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate + \n    transit_count + avg_past_price_density + sqrt(crime_count) + \n    log(nearest_hospital_knn3) + (interior_condition * income_scaled) + \n    factor(zip_code), data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-2.86007 -0.09067  0.05500  0.16974  2.17553 \n\nCoefficients:\n                                     Estimate   Std. Error t value\n(Intercept)                       7.147174262  0.125631514  56.890\nlog(total_livable_area)           0.762702187  0.007951214  95.923\nnumber_of_bathrooms               0.062545809  0.003951966  15.827\nhouse_age_c                       0.000081673  0.000091303   0.895\nhouse_age_c2                      0.000002396  0.000001333   1.797\ninterior_condition               -0.167693153  0.003180473 -52.726\nquality_grade_num                -0.020614687  0.002369293  -8.701\nfireplaces                        0.040919276  0.008065426   5.073\ngarage_spaces                     0.062755291  0.005224896  12.011\ncentral_air_dummy                 0.088693723  0.006224102  14.250\ncentral_air_missing              -0.145545244  0.006529396 -22.291\nincome_scaled                    -0.308308801  0.022099101 -13.951\nba_rate                           0.004727434  0.000434125  10.890\nunemployment_rate                -0.002009575  0.000512327  -3.922\ntransit_count                     0.000210309  0.000188166   1.118\navg_past_price_density            0.002676947  0.000053851  49.710\nsqrt(crime_count)                -0.040746561  0.001633811 -24.940\nlog(nearest_hospital_knn3)       -0.007368586  0.012879355  -0.572\nfactor(zip_code)19103            -0.191888874  0.038623356  -4.968\nfactor(zip_code)19104             0.043168643  0.044596804   0.968\nfactor(zip_code)19106            -0.072911591  0.039431744  -1.849\nfactor(zip_code)19107             0.002870411  0.041113568   0.070\nfactor(zip_code)19111             0.092033842  0.042730139   2.154\nfactor(zip_code)19114             0.046991631  0.045784241   1.026\nfactor(zip_code)19115             0.036786258  0.045577032   0.807\nfactor(zip_code)19116             0.074693711  0.047046924   1.588\nfactor(zip_code)19118             0.042277408  0.050964443   0.830\nfactor(zip_code)19119            -0.005571634  0.045415173  -0.123\nfactor(zip_code)19120            -0.001979921  0.042451411  -0.047\nfactor(zip_code)19121            -0.079268931  0.042967693  -1.845\nfactor(zip_code)19122            -0.046067555  0.043677075  -1.055\nfactor(zip_code)19123            -0.124474312  0.042021722  -2.962\nfactor(zip_code)19124            -0.042922550  0.042779720  -1.003\nfactor(zip_code)19125            -0.053924641  0.040136398  -1.344\nfactor(zip_code)19126            -0.096134211  0.050099576  -1.919\nfactor(zip_code)19127            -0.054620383  0.046526938  -1.174\nfactor(zip_code)19128            -0.063379705  0.041111172  -1.542\nfactor(zip_code)19129            -0.081840032  0.044983229  -1.819\nfactor(zip_code)19130             0.004465219  0.038447508   0.116\nfactor(zip_code)19131            -0.124963371  0.044224170  -2.826\nfactor(zip_code)19132            -0.353731640  0.042632020  -8.297\nfactor(zip_code)19133            -0.360578685  0.045760207  -7.880\nfactor(zip_code)19134            -0.167019067  0.042150583  -3.962\nfactor(zip_code)19135             0.066237684  0.045406809   1.459\nfactor(zip_code)19136             0.093091097  0.045399194   2.051\nfactor(zip_code)19137             0.003922355  0.050119037   0.078\nfactor(zip_code)19138            -0.061547285  0.045851962  -1.342\nfactor(zip_code)19139            -0.125514020  0.043923079  -2.858\nfactor(zip_code)19140            -0.243638620  0.042070333  -5.791\nfactor(zip_code)19141            -0.104140516  0.045656220  -2.281\nfactor(zip_code)19142            -0.117894811  0.045192100  -2.609\nfactor(zip_code)19143            -0.121037876  0.042647300  -2.838\nfactor(zip_code)19144            -0.119218035  0.044443147  -2.682\nfactor(zip_code)19145             0.000416732  0.040490607   0.010\nfactor(zip_code)19146            -0.065296494  0.038364344  -1.702\nfactor(zip_code)19147            -0.022134932  0.038396424  -0.576\nfactor(zip_code)19148             0.034927326  0.039935049   0.875\nfactor(zip_code)19149             0.163264326  0.043056311   3.792\nfactor(zip_code)19150            -0.044481749  0.048200789  -0.923\nfactor(zip_code)19151            -0.087776350  0.045219855  -1.941\nfactor(zip_code)19152             0.091653850  0.044520396   2.059\nfactor(zip_code)19153            -0.033554531  0.052491824  -0.639\nfactor(zip_code)19154             0.056967215  0.044374083   1.284\ninterior_condition:income_scaled  0.119746969  0.005588581  21.427\n                                             Pr(&gt;|t|)    \n(Intercept)                      &lt; 0.0000000000000002 ***\nlog(total_livable_area)          &lt; 0.0000000000000002 ***\nnumber_of_bathrooms              &lt; 0.0000000000000002 ***\nhouse_age_c                                   0.37105    \nhouse_age_c2                                  0.07241 .  \ninterior_condition               &lt; 0.0000000000000002 ***\nquality_grade_num                &lt; 0.0000000000000002 ***\nfireplaces                        0.00000039295484010 ***\ngarage_spaces                    &lt; 0.0000000000000002 ***\ncentral_air_dummy                &lt; 0.0000000000000002 ***\ncentral_air_missing              &lt; 0.0000000000000002 ***\nincome_scaled                    &lt; 0.0000000000000002 ***\nba_rate                          &lt; 0.0000000000000002 ***\nunemployment_rate                 0.00008784000945812 ***\ntransit_count                                 0.26371    \navg_past_price_density           &lt; 0.0000000000000002 ***\nsqrt(crime_count)                &lt; 0.0000000000000002 ***\nlog(nearest_hospital_knn3)                    0.56724    \nfactor(zip_code)19103             0.00000067928689067 ***\nfactor(zip_code)19104                         0.33306    \nfactor(zip_code)19106                         0.06446 .  \nfactor(zip_code)19107                         0.94434    \nfactor(zip_code)19111                         0.03126 *  \nfactor(zip_code)19114                         0.30472    \nfactor(zip_code)19115                         0.41960    \nfactor(zip_code)19116                         0.11238    \nfactor(zip_code)19118                         0.40680    \nfactor(zip_code)19119                         0.90236    \nfactor(zip_code)19120                         0.96280    \nfactor(zip_code)19121                         0.06507 .  \nfactor(zip_code)19122                         0.29156    \nfactor(zip_code)19123                         0.00306 ** \nfactor(zip_code)19124                         0.31571    \nfactor(zip_code)19125                         0.17911    \nfactor(zip_code)19126                         0.05501 .  \nfactor(zip_code)19127                         0.24042    \nfactor(zip_code)19128                         0.12316    \nfactor(zip_code)19129                         0.06887 .  \nfactor(zip_code)19130                         0.90754    \nfactor(zip_code)19131                         0.00472 ** \nfactor(zip_code)19132            &lt; 0.0000000000000002 ***\nfactor(zip_code)19133             0.00000000000000339 ***\nfactor(zip_code)19134             0.00007435204084637 ***\nfactor(zip_code)19135                         0.14464    \nfactor(zip_code)19136                         0.04032 *  \nfactor(zip_code)19137                         0.93762    \nfactor(zip_code)19138                         0.17951    \nfactor(zip_code)19139                         0.00427 ** \nfactor(zip_code)19140             0.00000000705409283 ***\nfactor(zip_code)19141                         0.02256 *  \nfactor(zip_code)19142                         0.00909 ** \nfactor(zip_code)19143                         0.00454 ** \nfactor(zip_code)19144                         0.00731 ** \nfactor(zip_code)19145                         0.99179    \nfactor(zip_code)19146                         0.08876 .  \nfactor(zip_code)19147                         0.56429    \nfactor(zip_code)19148                         0.38180    \nfactor(zip_code)19149                         0.00015 ***\nfactor(zip_code)19150                         0.35610    \nfactor(zip_code)19151                         0.05225 .  \nfactor(zip_code)19152                         0.03953 *  \nfactor(zip_code)19153                         0.52268    \nfactor(zip_code)19154                         0.19922    \ninterior_condition:income_scaled &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3423 on 31435 degrees of freedom\n  (114 observations deleted due to missingness)\nMultiple R-squared:  0.7665,    Adjusted R-squared:  0.7661 \nF-statistic:  1638 on 63 and 31435 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\nvif(model_4)\n\n\n                                       GVIF Df GVIF^(1/(2*Df))\nlog(total_livable_area)            1.523908  1        1.234467\nnumber_of_bathrooms                1.611921  1        1.269614\nhouse_age_c                        1.580710  1        1.257263\nhouse_age_c2                       1.470007  1        1.212439\ninterior_condition                 1.533402  1        1.238306\nquality_grade_num                  1.779716  1        1.334060\nfireplaces                         1.295084  1        1.138018\ngarage_spaces                      1.586272  1        1.259473\ncentral_air_dummy                  2.084180  1        1.443669\ncentral_air_missing                1.453277  1        1.205519\nincome_scaled                     24.491928  1        4.948932\nba_rate                            6.178507  1        2.485660\nunemployment_rate                  2.016927  1        1.420186\ntransit_count                      1.715169  1        1.309645\navg_past_price_density             7.837830  1        2.799612\nsqrt(crime_count)                  2.815547  1        1.677959\nlog(nearest_hospital_knn3)         8.102261  1        2.846447\nfactor(zip_code)                 565.126609 45        1.072950\ninterior_condition:income_scaled  20.090358  1        4.482227\n\n\nCoefficient Interpretation:\n- Fixed Effects Interpretation: - These coefficients represent the price difference for each zip code relative to the ‚Äúreference zip code‚Äù (which is omitted from the list, e.g., 19102). - Example: factor(zip_code)19106 (-0.107): A home in zip code 19106 is, on average, 10.7% less expensive than a home in the reference zip code, holding all other variables constant. - Example: factor(zip_code)19149 (0.183): A home in zip code 19149 is, on average, 18.3% more expensive. - interior_condition:income_scaled (0.117) (Interaction Term): - This is one of the most interesting findings. It shows that the impact of interior_condition depends on income_scaled. - The total marginal effect of interior_condition is:\\[= -0.1695 + 0.1165 \\times \\text{income\\_scaled}\\] - At the baseline income level (income_scaled = 0), each one-unit worsening in condition is associated with a 17.0% price decrease (-0.1695). However, this penalty is mitigated (lessened) in higher-income areas. For each one-unit increase in income_scaled, the negative penalty of poor condition is reduced by 11.7 percentage points. This may imply that in high-income neighborhoods, ‚Äúfixer-uppers‚Äù (homes in poor condition) are seen as investment opportunities with high renovation potential. Therefore, the market penalty for ‚Äúpoor condition‚Äù is smaller."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-5-model-validation",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-5-model-validation",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 5: Model Validation",
    "text": "Phase 5: Model Validation\n\n10-fold cross-validation\n5.1 Compare all 4 models:"
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-6-model-diagnostics",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-6-model-diagnostics",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 6: Model Diagnostics",
    "text": "Phase 6: Model Diagnostics\n\nCheck assumptions for best model:\n6.1 Residual plot:\n\n\nCode\nmodel_data &lt;- data.frame(\n  Fitted = fitted(model_4),\n  Residuals = resid(model_4)\n)\n\np_resid_fitted &lt;- ggplot(model_data, aes(x = Fitted, y = Residuals)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\", size = 2) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"loess\", color = \"black\", se = FALSE, linewidth = 0.8) +\n  labs(\n    title = \"Residuals vs Fitted Values\",\n    subtitle = \"Checking linearity and homoscedasticity for Model 4\",\n    x = \"Fitted Values (Log(Sale Price))\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_resid_fitted\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\nresid_full &lt;- rep(NA, nrow(opa_census_all))\nresid_full[-as.numeric(model_4$na.action)] &lt;- resid(model_4)\n\nopa_census_all$residuals &lt;- resid_full\n\ntract_resid &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_residual = mean(residuals, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_resid, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_residual), color = \"white\", size = 0.2) +\n  scale_fill_gradient2(\n    low = \"#6A1B9A\", mid = \"white\", high = \"#FFB300\",\n    midpoint = 0,\n    limits = c(-0.5, 0.5),\n    name = \"Mean Log Residual\",\n    breaks = c(-0.3, 0, 0.3),\n    labels = c(\"Overestimated\", \"Accurate\", \"Underestimated\"),\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Hardest to Predict Neighborhoods in Philadelphia\",\n    subtitle = \"Yellow = underestimation | Purple = overestimation\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\nInterpretation:\n- Central Tendency of Residuals: The residuals are generally centered around the zero line, showing no systematic deviation. This indicates that the model captures the overall linear relationship between predictors and the response variable effectively.\n- Homoscedasticity: The residuals show greater variability and dispersion in the lower fitted value range (approximately 10‚Äì12), while they appear more stable at higher fitted values. This suggests that the model performs less effectively for observations with lower predicted values.\n- Model Assumption Assessment: Overall, the assumptions of linearity and homoscedasticity are largely satisfied, indicating a sound model fit, with only slight deviations to monitor at higher fitted values.\n6.2 Q-Q plot:\n\n\nCode\np_qq &lt;- ggplot(model_data, aes(sample = Residuals)) +\n  stat_qq(color = \"#6A1B9A\", size = 2, alpha = 0.6) +\n  stat_qq_line(color = \"red\",linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Normal Q-Q Plot\",\n    subtitle = \"Checking normality of residuals for Model 4\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_qq\n\n\n\n\n\n\n\n\n\nInterpretation:\n- Overall Shape of the Plot: The residual points generally follow the diagonal line, indicating that the overall distribution of residuals is roughly consistent with a normal distribution.\n- Good Fit in the Central Range: In the central range (around -1 to 1 quantiles), the sample quantiles align closely with the theoretical quantiles, suggesting that most residuals conform well to the assumption of normality.\n- Deviation in the Tails: At both tails‚Äîespecially the upper quantiles‚Äîthe points deviate noticeably from the red dashed line, indicating that the residuals have heavier tails than a normal distribution, suggesting slight non-normality.\n- Assessment of Normality Assumption: Although deviations appear in the tails, the overall alignment with the reference line is strong, indicating that the normality assumption largely holds, with only minor deviations for extreme residuals.\n6.3 Cook‚Äôs distance:\n\n\nCode\ncooks_d &lt;- cooks.distance(model_4)\nmodel_data &lt;- data.frame(\n  Index = 1:length(cooks_d),\n  CooksD = cooks_d\n)\nthreshold &lt;- 4 / nrow(model_4$model)\n\np_cook &lt;- ggplot(model_data, aes(x = Index, y = CooksD)) +\n  geom_segment(aes(xend = Index, yend = 0), color = \"#6A1B9A\", alpha = 0.7) +  # vertical lines\n  geom_point(color = \"#6A1B9A\", size = 0.15) +\n  geom_hline(yintercept = threshold, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Cook's Distance\",\n    subtitle = \"Identifying influential observations for Model 4\",\n    x = \"Observation Index\",\n    y = \"Cook's Distance\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_cook\n\n\n\n\n\n\n\n\n\nInterpretation:\n- Overall Distribution Pattern: Most observations have Cook‚Äôs Distance values close to zero, indicating that the dataset‚Äôs overall influence on the model is balanced, with no widespread undue impact.\n- Presence of Influential Points: A few vertical spikes rise noticeably above the rest, indicating the presence of some influential observations that may affect the estimated model coefficients. - Model Robustness Conclusion: Overall, the model appears robust, with no single observation exerting excessive influence."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-7-conclusions-recommendations",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix1.html#phase-7-conclusions-recommendations",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 7: Conclusions & Recommendations",
    "text": "Phase 7: Conclusions & Recommendations\n\nConclusion:\n\nOur final model‚Äôs R¬≤ is 0.84, indicating that the model explains 84% of the variance in log sale prices. The RMSE is , showing that the model‚Äôs predictions are reasonably close to the observed values.\n\nLivable Area of the House matters most for Philadelphia prices.\n\n\n\nRecommendations:\n\nEquity concerns\n\nWhich neighborhoods are hardest to predict?\n\nThe largest prediction errors occur primarily in central Philadelphia, where both overestimation and underestimation coexist within close proximity. This pattern indicates that the model struggles most in areas with high housing heterogeneity- neighborhoods that contain a mix of old row houses, newly renovated apartments, and varying property types within short distances.\nIn contrast, outer neighborhoods such as those in the northwest and northeast tend to have more consistent housing characteristics, leading to smaller residuals. The central tracts‚Äô larger residuals suggest that cultural or historical features have introduced variability that the model‚Äôs current features (mainly physical characteristics) cannot fully explain.\n\nAny data bias?\n\nThe observed spatial pattern of prediction errors reflects inherent data bias. The dataset likely overrepresents mid-range housing conditions and underrepresents both luxury and low-income housing. As shown in the livable area and interior condition maps, data coverage in wealthier areas (especially the northwest) is limited, and these tracts often contain more unique, high-value properties that the model cannot generalize well.\n\nWealthier tracts are more likely to be well-documented, while poorer areas lack records, creating spatial imbalance in model performance.\n\nHigh-priced homes typically have larger negotiation margins, meaning their final sale prices are often lower than the listed prices. In contrast, low-priced homes sell closer to their listing prices. As a result, model tends to overestimate expensive homes and underestimate affordable ones, introducing systematic bias.\n\n\nRecommendations to government\n\n\nImmediate System Calibration: We recommend utilizing our model‚Äôs findings to immediately adjust property assessments in systematically overvalued low-income communities. This action will ensure a fair distribution of the tax burden and address current inequities.\nIntegrate Advanced Spatial Features: We advise the Office of Property Assessment (OPA) to permanently integrate the effective spatial characteristics identified by our study‚Äîspecifically Comparable Sales Proxies (surrounding transaction prices) and Neighborhood Fixed Effects‚Äîinto the next-generation AVM. This will significantly enhance the model‚Äôs responsiveness to rapidly changing market dynamics.\nExtreme high values almost exclusively stem from corporate transactions and require manual review for outliers.\n\n\nLimitations and nest steps\n\nLimitations: Inherent Data Biases\n\nOur predictive accuracy is constrained by inherent data biases which affect equity. We find a dual challenge in data coverage: in affluent areas, high-value, unique properties suffer from data sparsity, making generalization difficult. Conversely, lower-income areas often show data incompleteness, leading to less reliable predictions and higher residual errors. Critically, we observed a price-tier bias: high-priced homes tend to sell below their list price, while low-priced homes transact closer to it. This systemic pattern means the model is prone to over-assessing expensive properties and under-assessing affordable ones, creating a structural risk for vertical inequity in the tax system.\n\nNext Steps: Enhancing Data Quality and Fairness\n\nTo address these limitations, our next steps focus on data enrichment and equitable optimization. The City should partner with us to integrate non-public data, such as detailed appraisal records and permit data, which can account for unobserved renovation quality and close the data gap. Furthermore, to combat the systematic price-tier bias, we recommend integrating fairness metrics directly into the AVM‚Äôs optimization process. This will shift the model‚Äôs objective beyond simple average accuracy (RMSE) to ensure that prediction errors are uniformly low across all price tiers and all Philadelphia neighborhoods, securing both accuracy and equity."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html",
    "href": "Assignments/assignment_5/Assignment_5_coop.html",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "Philadelphia‚Äôs Indego bike share system faces the same operational challenge as every bike share system: rebalancing bikes to meet anticipated demand.\nIn this lab. we build predictive models that forecast bike share demand across space (different stations) and time (different hours) to help solve this operational problem."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#the-rebalancing-challenge-in-philadelphia",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#the-rebalancing-challenge-in-philadelphia",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "Philadelphia‚Äôs Indego bike share system faces the same operational challenge as every bike share system: rebalancing bikes to meet anticipated demand.\nIn this lab. we build predictive models that forecast bike share demand across space (different stations) and time (different hours) to help solve this operational problem."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#load-libraries",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#load-libraries",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n\nCode\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#define-themes",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#define-themes",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Define Themes",
    "text": "Define Themes\n\n\nCode\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 &lt;- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#data-import-preparation",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#data-import-preparation",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Data Import & Preparation",
    "text": "Data Import & Preparation\nIn this lab, we will use the Indego data (Q3 2024), because we want to explore the pattern‚Äôs diffenrence between summer and winter.\n\nDownload data for Q3 2024\n\n\nCode\n# Read Q3 2024 data\nindego_q3 &lt;- read_csv(\"data/indego-trips-2024-q3.csv\")\n\n\n\n\nExamine the Data Structure\n\n\nCode\n# How many trips?\ncat(\"Total trips in Q3 2024:\", nrow(indego_q3), \"\\n\")\n\n\nTotal trips in Q3 2024: 408408 \n\n\nCode\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego_q3$start_time)), \"to\", \n    max(mdy_hm(indego_q3$start_time)), \"\\n\")\n\n\nDate range: 1719792120 to 1727740740 \n\n\nCode\nas.POSIXct(1719792120, origin = \"1970-01-01\", tz = \"America/New_York\")\n\n\n[1] \"2024-06-30 20:02:00 EDT\"\n\n\nCode\nas.POSIXct(1727740740, origin = \"1970-01-01\", tz = \"America/New_York\")\n\n\n[1] \"2024-09-30 19:59:00 EDT\"\n\n\nCode\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego_q3$start_station)), \"\\n\")\n\n\nUnique start stations: 261 \n\n\nCode\n# Trip types\ntable(indego_q3$trip_route_category)\n\n\n\n   One Way Round Trip \n    380939      27469 \n\n\nCode\n# Passholder types\ntable(indego_q3$passholder_type)\n\n\n\n Day Pass  Indego30 Indego365   Walk-up \n    22885    239857    132358     13308 \n\n\nCode\n# Bike types\ntable(indego_q3$bike_type)\n\n\n\nelectric standard \n  236839   171569 \n\n\n\n\nCreate Time Bins\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\nCode\nindego_q3 &lt;- indego_q3 %&gt;%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego_q3 %&gt;% select(start_datetime, interval60, week, dotw, hour, weekend))\n\n\n# A tibble: 6 √ó 6\n  start_datetime      interval60           week dotw   hour weekend\n  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;ord&gt; &lt;int&gt;   &lt;dbl&gt;\n1 2024-07-01 00:02:00 2024-07-01 00:00:00    27 Mon       0       0\n2 2024-07-01 00:03:00 2024-07-01 00:00:00    27 Mon       0       0\n3 2024-07-01 00:04:00 2024-07-01 00:00:00    27 Mon       0       0\n4 2024-07-01 00:05:00 2024-07-01 00:00:00    27 Mon       0       0\n5 2024-07-01 00:06:00 2024-07-01 00:00:00    27 Mon       0       0\n6 2024-07-01 00:06:00 2024-07-01 00:00:00    27 Mon       0       0"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#exploratory-analysis",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#exploratory-analysis",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nWe aggregated individual trip records by date to obtain total daily ridership, then used ggplot2 to plot a time-series line chart with a smoothed trend line to visualize how Indego bike-share demand changed over the third quarter of 2024.\n\nTrips Over Time\n\n\nCode\n# Daily trip counts\ndaily_trips &lt;- indego_q3 %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q3 2024\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nInterpretation:\nThe first pattern is the weekly heartbeat. Those sharp peaks and valleys repeat with metronomic regularity. Indego always dips on the same days each week, typically weekends, because commuting demand relaxes\nThen there‚Äôs the gentle seasonal slope underneath that weekly pulse. Early July starts relatively high, and then the baseline slowly drifts downward into early August (Maybe because of the summer vocation). Mid-August shows a little recovery, followed by a September lift since the fall semester began. Before the season‚Äôs first chilly whisper pushes ridership down again heading into October.\n\n\nHourly Patterns\n\n\nCode\n# Average trips by hour and day type\nhourly_patterns &lt;- indego_q3 %&gt;%\n  group_by(hour, weekend) %&gt;%\n  summarize(avg_trips = n() / n_distinct(date)) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nInterpretation:\nOn weekdays, the peaks are tied to commuting: A morning spike around 8‚Äì9 AM, when the city jolts awake and people pedal to work. A bigger afternoon/evening spike around 5‚Äì6 PM, the classic homebound surge. The slope up to these points is steep, like a heartbeat aligned with office life. On weekends, the rhythm relaxes. There‚Äôs no sharp, early-morning people are not rushing anywhere. Ridership climbs more slowly through the morning toward a broad mid-day plateau, roughly 11 AM to 3 PM, and then drifts downward without the evening spike seen on weekdays.\nSo the pattern suggests that weekday trips are purposeful and synchronized with the workday, while weekend trips stretch out across the middle of the day in a more leisurely wave.\n\n\nTop Stations\n\n\nCode\n# Most popular origin stations\ntop_stations &lt;- indego_q3 %&gt;%\n  count(start_station, start_lat, start_lon, name = \"trips\") %&gt;%\n  arrange(desc(trips)) %&gt;%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 20 Indego Stations by Trip Origins\n\n\nstart_station\nstart_lat\nstart_lon\ntrips\n\n\n\n\n3,010\n39.94711\n-75.16618\n6,654\n\n\n3,032\n39.94527\n-75.17971\n5,436\n\n\n3,244\n39.93865\n-75.16674\n4,421\n\n\n3,054\n39.96250\n-75.17420\n4,305\n\n\n3,359\n39.94888\n-75.16978\n4,305\n\n\n3,296\n39.95134\n-75.16758\n4,252\n\n\n3,101\n39.94295\n-75.15955\n4,226\n\n\n3,020\n39.94855\n-75.19007\n4,154\n\n\n3,295\n39.95028\n-75.16027\n4,144\n\n\n3,066\n39.94561\n-75.17348\n4,114\n\n\n3,059\n39.96244\n-75.16121\n4,022\n\n\n3,022\n39.95472\n-75.18323\n3,954\n\n\n3,362\n39.94816\n-75.16226\n3,948\n\n\n3,163\n39.94974\n-75.18097\n3,942\n\n\n3,028\n39.94061\n-75.14958\n3,921\n\n\n3,208\n39.95048\n-75.19324\n3,919\n\n\n3,061\n39.95425\n-75.17761\n3,796\n\n\n3,046\n39.95012\n-75.14472\n3,734\n\n\n3,063\n39.94633\n-75.16980\n3,726\n\n\n3,185\n39.95169\n-75.15888\n3,678"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#get-philadelphia-spatial-context",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#get-philadelphia-spatial-context",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Get Philadelphia Spatial Context",
    "text": "Get Philadelphia Spatial Context\n\nLoad Philadelphia Census Data\nWe‚Äôll get census tract data to add demographic context to our stations.\n\n\nCode\n# Get Philadelphia census tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\",\n  progress_bar = FALSE\n) %&gt;%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %&gt;%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %&gt;%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n\n# Check the data\nglimpse(philly_census)\n\n\nRows: 408\nColumns: 17\n$ GEOID                  &lt;chr&gt; \"42101001500\", \"42101001800\", \"42101002802\", \"4‚Ä¶\n$ NAME                   &lt;chr&gt; \"Census Tract 15; Philadelphia County; Pennsylv‚Ä¶\n$ Total_Pop              &lt;dbl&gt; 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,‚Ä¶\n$ B01003_001M            &lt;dbl&gt; 677, 369, 796, 437, 853, 210, 480, 734, 763, 11‚Ä¶\n$ Med_Inc                &lt;dbl&gt; 110859, 114063, 78871, 61583, 32347, 48581, 597‚Ä¶\n$ B19013_001M            &lt;dbl&gt; 24975, 30714, 20396, 22293, 4840, 13812, 6278, ‚Ä¶\n$ Total_Commuters        &lt;dbl&gt; 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2‚Ä¶\n$ B08301_001M            &lt;dbl&gt; 387, 308, 478, 383, 456, 189, 380, 281, 456, 68‚Ä¶\n$ Transit_Commuters      &lt;dbl&gt; 429, 123, 685, 506, 534, 192, 658, 218, 438, 51‚Ä¶\n$ B08301_010M            &lt;dbl&gt; 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,‚Ä¶\n$ White_Pop              &lt;dbl&gt; 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35‚Ä¶\n$ B02001_002M            &lt;dbl&gt; 268, 381, 592, 380, 88, 190, 463, 112, 238, 778‚Ä¶\n$ Med_Home_Value         &lt;dbl&gt; 568300, 605000, 350600, 296400, 76600, 289700, ‚Ä¶\n$ B25077_001M            &lt;dbl&gt; 58894, 34876, 12572, 22333, 10843, 118720, 1506‚Ä¶\n$ geometry               &lt;MULTIPOLYGON [¬∞]&gt; MULTIPOLYGON (((-75.16558 3..., MU‚Ä¶\n$ Percent_Taking_Transit &lt;dbl&gt; 20.694645, 5.454545, 22.592348, 21.754084, 26.9‚Ä¶\n$ Percent_White          &lt;dbl&gt; 67.2100892, 75.5757576, 64.5279720, 79.9950360,‚Ä¶\n\n\n\n\nMap Philadelphia Context\nOur code builds a contextual map by shading each Philadelphia census tract according to its median household income. Then it overlays red points representing Indego trip start locations. The figure shows that most bike-share activity clusters in central and lower-income neighborhoods, while higher-income outer areas have fewer stations and fewer trip origins.\n\n\nCode\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego_q3,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n\n\n\n\n\n\n\n\n\n\n\nJoin Census Data to Stations\nWe‚Äôll spatially join census characteristics to each bike station.\n\n\nCode\n# Create sf object for stations\nstations_sf &lt;- indego_q3 %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census &lt;- st_join(stations_sf, philly_census, left = TRUE) %&gt;%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map &lt;- indego_q3 %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_q3_census &lt;- indego_q3 %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map &lt;- indego_q3 %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %&gt;% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %&gt;% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n\n\n\n\n\n\n\n\n\n\n\nDealing with missing data\nIn this step, we are going to remove the non-residential bike share stations.\n\n\nCode\n# Identify which stations to keep\nvalid_stations &lt;- stations_census %&gt;%\n  filter(!is.na(Med_Inc)) %&gt;%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_q3_census &lt;- indego_q3 %&gt;%\n  filter(start_station %in% valid_stations) %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#get-weather-data",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#get-weather-data",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Get Weather Data",
    "text": "Get Weather Data\nWeather significantly affects bike share demand. We get hourly weather for Philadelphia in this part.\n\n\nCode\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q3 2024: July 1 - September 31\nweather_data &lt;- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-07-01\",\n  date_end = \"2024-09-30\"\n)\n\n# Process weather data\nweather_processed &lt;- weather_data %&gt;%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %&gt;%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %&gt;%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete &lt;- weather_processed %&gt;%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %&gt;%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %&gt;% select(Temperature, Precipitation, Wind_Speed))\n\n\n  Temperature    Precipitation        Wind_Speed    \n Min.   :55.00   Min.   :0.000000   Min.   : 0.000  \n 1st Qu.:70.00   1st Qu.:0.000000   1st Qu.: 4.000  \n Median :76.00   Median :0.000000   Median : 7.000  \n Mean   :75.59   Mean   :0.007896   Mean   : 6.893  \n 3rd Qu.:81.00   3rd Qu.:0.000000   3rd Qu.: 9.000  \n Max.   :98.00   Max.   :1.250000   Max.   :44.000  \n\n\n\nVisualize Weather Patterns\n\n\nCode\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q3 2024\",\n    subtitle = \"Summer to early autumn transition\",\n    x = \"Date\",\n    y = \"Temperature (¬∞F)\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nInterpretation:\nThe pattern is a classic seasonal transition: hot, volatile summer days gradually giving way to cooler, steadier early-fall conditions."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#create-space-time-panel",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#create-space-time-panel",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Create Space-Time Panel",
    "text": "Create Space-Time Panel\n\nAggregate Trips to Station-Hour Level\n\n\nCode\n# Count trips by station-hour\ntrips_panel &lt;- indego_q3_census %&gt;%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %&gt;%\n  summarize(Trip_Count = n()) %&gt;%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n\n\n[1] 193072\n\n\nCode\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n\n\n[1] 241\n\n\nCode\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n\n\n[1] 2202\n\n\n\n\nCreate Complete Panel Structure\nNot every station has trips every hour. We need a complete panel where every station-hour combination exists (even if Trip_Count = 0).\n\n\nCode\n# Calculate expected panel size\nn_stations &lt;- length(unique(trips_panel$start_station))\nn_hours &lt;- length(unique(trips_panel$interval60))\nexpected_rows &lt;- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n\n\nExpected panel rows: 530,682 \n\n\nCode\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nCurrent rows: 193,072 \n\n\nCode\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nMissing rows: 337,610 \n\n\nCode\n# Create complete panel\nstudy_panel &lt;- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %&gt;%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %&gt;%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes &lt;- trips_panel %&gt;%\n  group_by(start_station) %&gt;%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n\n\nComplete panel rows: 530,682 \n\n\n\n\nAdd Time Features\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Create day of week factor with treatment (dummy) coding\nstudy_panel &lt;- study_panel %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(study_panel$dotw_simple) &lt;- contr.treatment(7)\n\n\n\n\nJoin Weather Data\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %&gt;% select(Trip_Count, Temperature, Precipitation))\n\n\n   Trip_Count       Temperature    Precipitation   \n Min.   : 0.0000   Min.   :55.00   Min.   :0.0000  \n 1st Qu.: 0.0000   1st Qu.:70.00   1st Qu.:0.0000  \n Median : 0.0000   Median :76.00   Median :0.0000  \n Mean   : 0.7068   Mean   :75.59   Mean   :0.0079  \n 3rd Qu.: 1.0000   3rd Qu.:81.00   3rd Qu.:0.0000  \n Max.   :24.0000   Max.   :98.00   Max.   :1.2500  \n                   NA's   :5784    NA's   :5784"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#create-temporal-lag-variables",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#create-temporal-lag-variables",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Create Temporal Lag Variables",
    "text": "Create Temporal Lag Variables\nThe key innovation for space-time prediction: past demand predicts future demand.\nWhy Lags?\nIf there were 15 bike trips from Station A at 8:00 AM, there will probably be ~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.\n\n\nCode\n# Sort by station and time\nstudy_panel &lt;- study_panel %&gt;%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel &lt;- study_panel %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24),\n    lag1week = lag(Trip_Count, 24*7)\n  ) %&gt;%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete &lt;- study_panel %&gt;%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n\n\nRows after removing NA lags: 646,362 \n\n\n\nVisualize Lag Correlations\n\n\nCode\n# Sample one station to visualize\nexample_station &lt;- study_panel_complete %&gt;%\n  filter(start_station == first(start_station)) %&gt;%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#temporal-traintest-split",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#temporal-traintest-split",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Temporal Train/Test Split",
    "text": "Temporal Train/Test Split\nThe code performs a clean temporal split for forecasting. It first separates the dataset into early weeks and later weeks, identifies stations that appear in both periods, and keeps only those consistent stations. Then it creates a training set using weeks 27‚Äì36 and a testing set using weeks 37‚Äì40, ensuring the model is trained on past data and evaluated on future data.\n\n\nCode\n# Split by week\n# Q3 has weeks 27-40 (Jul-Sep)\n# Train on weeks 27-36 (Jan 1 - early September)\n# Test on weeks 37-40 (rest of September)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 37) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\nlate_stations &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 37) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations &lt;- intersect(early_stations, late_stations)\n\n# Filter panel to only common stations\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 37)\n\ntest &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 37)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n\n\nTraining observations: 470,940 \n\n\nCode\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n\n\nTesting observations: 159,330 \n\n\nCode\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n\n\nTraining date range: 19905 to 19974 \n\n\nCode\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n\n\nTesting date range: 19975 to 19996"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#build-predictive-models",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#build-predictive-models",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Build Predictive Models",
    "text": "Build Predictive Models\nWe‚Äôll build 5 models with increasing complexity to see what improves predictions.\nThis is a model evaluation function that can generate data\n\n\nCode\n# model evaluation function\nmodel_metrics_lm &lt;- function(model, test_data, response_var = \"Trip_Count\") {\n  model_name &lt;- deparse(substitute(model))\n  \n  if (inherits(model, \"glm\") && family(model)$family == \"poisson\") {\n    type       &lt;- \"Poisson\"\n    y_train    &lt;- model$y\n    yhat_train &lt;- predict(model, type = \"response\")\n    \n    sse &lt;- sum((y_train - yhat_train)^2)\n    sst &lt;- sum((y_train - mean(y_train))^2)\n    r2  &lt;- 1 - sse / sst\n    \n    n &lt;- length(y_train)\n    p &lt;- length(coef(model))\n    adjr2 &lt;- 1 - (1 - r2) * (n - 1) / (n - p - 1)\n    \n  } else if (inherits(model, \"lm\")) {\n    type       &lt;- \"OLS\"\n    s          &lt;- summary(model)\n    r2         &lt;- s$r.squared\n    adjr2      &lt;- s$adj.r.squared\n    y_train    &lt;- model$model[[response_var]]\n    yhat_train &lt;- fitted(model)\n    \n  } else {\n    stop(\"Only supports lm and glm(poisson).\")\n  }\n  \n  mae_train &lt;- mean(abs(y_train - yhat_train))\n  \n  if (inherits(model, \"glm\") && family(model)$family == \"poisson\") {\n    yhat_test &lt;- predict(model, newdata = test_data, type = \"response\")\n  } else {\n    yhat_test &lt;- predict(model, newdata = test_data)\n  }\n  y_test &lt;- test_data[[response_var]]\n  cc &lt;- complete.cases(y_test, yhat_test)\n  mae_test &lt;- mean(abs(y_test[cc] - yhat_test[cc]))\n  \n  cat(\"Model:\", model_name, \"\\n\")\n  cat(\"  Type:              \", type, \"\\n\")\n  cat(\"  R-squared (train): \", round(r2, 4), \"\\n\")\n  cat(\"  Adj R-squared:     \", round(adjr2, 4), \"\\n\")\n  cat(\"  MAE (train):       \", round(mae_train, 4), \"\\n\")\n  cat(\"  MAE (test):        \", round(mae_test, 4), \"\\n\\n\")\n}\n\n\n\nModel 1: Baseline (Time + Weather)\nThis model captures daily and weekly cycles, weather effects.\n\n\nCode\n# Now run the model\nmodel1 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model1, test_data = test)\n\n\nModel: model1 \n  Type:               OLS \n  R-squared (train):  0.1085 \n  Adj R-squared:      0.1085 \n  MAE (train):        0.7937 \n  MAE (test):         0.8293 \n\n\nInterpretation:\nThe model shows that trip counts vary strongly by hour of day. Compared to the baseline hour (midnight), coefficients rise sharply in the morning and peak around the late afternoon, matching the commuter pattern observed earlier. Day-of-week effects are smaller but still significant: mid-week days tend to have slightly higher ridership, while weekends show lower counts. Weather matters too‚Äîhigher temperatures slightly reduce trips, and precipitation has a larger negative effect. Although all predictors are statistically significant (given the huge sample size), the model explains only about 11% of the variation, meaning most hour-to-hour fluctuations are driven by other unmeasured factors such as station-specific conditions or random demand swings.\n\n\nModel 2: Add Temporal Lags\n\n\nCode\nmodel2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model2, test_data = test)\n\n\nModel: model2 \n  Type:               OLS \n  R-squared (train):  0.3318 \n  Adj R-squared:      0.3317 \n  MAE (train):        0.6655 \n  MAE (test):         0.6857 \n\n\nInterpretation:\nThe model still captures the usual hourly and day-of-week ridership patterns, but the major change comes from adding the lagged demand terms. The one-hour lag has the strongest effect‚Äîstations that were busy an hour ago are much more likely to be busy now. The three-hour lag and the previous-day same-hour lag also contribute meaningful predictive power. Weather remains significant but plays a smaller role. Overall model fit improves substantially: R¬≤ rises from about 0.11 to about 0.33, and residual error drops, showing that including recent past demand explains much more of the variation. This confirms that short-term station activity is strongly tied to its own immediate history.\n\n\nModel 3: Add Demographics\nThis model captures neighborhood effects on demand\n\n\nCode\nmodel3 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model3, test_data = test)\n\n\nModel: model3 \n  Type:               OLS \n  R-squared (train):  0.3373 \n  Adj R-squared:      0.3373 \n  MAE (train):        0.6673 \n  MAE (test):         0.6868 \n\n\nInterpretation:\nThe R square means the model explains about 34% of hourly variation in bike demand.Adding demographics raises R¬≤ slightly compared to Model 2, they help, but only modestly.Temporal lags dominate the model, while demographics show systematic spatial patterns. All three demographic variables are all significant, and it shows that demographics matter, but compared to temporal lags and hour-of-day, their effect sizes are small.\n\n\nModel 4: Add Station Fixed Effects\n\n\nCode\nmodel4 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model4, test_data = test)\n\n\nModel: model4 \n  Type:               OLS \n  R-squared (train):  0.3627 \n  Adj R-squared:      0.3624 \n  MAE (train):        0.6667 \n  MAE (test):         0.6831 \n\n\nInterpretation:\nModel 4 adds a dummy variable for every station, letting the model control for unobserved station-specific factors. It shows that station-specific factors are strong drivers of bike-share demand. Adding fixed effects significantly boosts the model‚Äôs ability to explain past variation, but the improvement does not reliably boost predictive accuracy. Even though R¬≤ increases, the model‚Äôs MAE does not improve much. Because station fixed effects are great for explaining historical data, but they don‚Äôt generalize as well when predicting new periods.\n\n\nModel 5: Add Rush Hour Interaction\n\n\nCode\nmodel5 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model5, test_data = test)\n\n\nModel: model5 \n  Type:               OLS \n  R-squared (train):  0.3669 \n  Adj R-squared:      0.3665 \n  MAE (train):        0.6653 \n  MAE (test):         0.6833 \n\n\nInterpretation:\nModel 5 introduces an interaction between rush hour and weekends, allowing the model to differentiate the strong weekday commuting peaks from the much softer weekend patterns. It also adds month effects to capture seasonal shifts across July, August, and September. Even though Model 5 has the highest R¬≤, it performs worse on the test set. This indicates that Model 5 is overfit: it explains historical variation better but loses forecasting accuracy. Model 4 remains the best balance of simplicity and predictive accuracy."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#model-evaluation",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#model-evaluation",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nCalculate Predictions and MAE\n\n\nCode\n# Get predictions on test set\ntest &lt;- test %&gt;%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results1 &lt;- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results1, \n      digits = 4,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nMAE (trips)\n\n\n\n\n1. Time + Weather\n0.8293\n\n\n2. + Temporal Lags\n0.6857\n\n\n3. + Demographics\n0.6868\n\n\n4. + Station FE\n0.6831\n\n\n5. + Rush Hour Interaction\n0.6833\n\n\n\n\n\nInterpretation:\n\nTemporal lags are the most important predictive feature.\n\nModel 2 gives the biggest drop in MAE, showing that short-term demand patterns drive accuracy.\n\nDemographics help explain variation but don‚Äôt improve prediction.\nStation fixed effects give a small but meaningful improvement.\n\nThey adjust for inherent differences between stations.\n\nExtra complexity (Model 5) does not yield better predictions.\n\nThe model is already near its predictive ceiling.\n\nBest predictive models are Model 2 (simplest) and Model 4 (slightly better).\n\n\n\nVisualize Model Comparison\n\n\nCode\nggplot(mae_results1, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\",alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 4)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#compare-results-to-q1-2025",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#compare-results-to-q1-2025",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Compare results to Q1 2025:",
    "text": "Compare results to Q1 2025:\n\n\n1. How do the MAE values compare across quarters?\nQ1 2025 MAE values are generally lower than those we reported for Q3 2024.\nSo predictions in Q1 are simply easier.\n\n\n2. Why might they differ?\nWinter(Q1) has smoother, more predictable demand: Riders may behave more consistently in colder months. There are fewer tourists. Weather is cold but stable‚Äîmore consistently low temperatures, fewer big swings.\nSummer(Q3) is noisy and harder to predict: Weather-driven volatility: heat waves, thunderstorms, humidity. More variation in leisure rides, weekend trips, and irregular patterns. Tourism and events add bursts of unpredictable activity.\nSo the model in Q3 faces more randomness, and MAE naturally rises.\n\n\n3. Are temporal patterns different (summer vs.¬†winter)?\nYes. In winter, ridership follows a tighter, more predictable rhythm. The daily totals rise gradually from January into March without the big, volatile spikes that appear in summer. Hourly patterns also become cleaner: weekday mornings and evenings show sharp commuter peaks, while weekends stay much flatter with only a modest mid-day bump. By contrast, summer brings stronger mid-day activity, higher weekend volume, and much more weather-driven noise. Winter demand is steadier and more commute-oriented, whereas summer mixes commuting with a large amount of leisure riding, creating far more variation.\n\n\n4. Which features are most important in your quarter?\nTemporal lags. The results show a large performance jump when lag variables (1 hour, 3 hours, 1 day) are added. Summer ridership changes quickly with events, weather shifts, and bursts of leisure activity."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#space-time-error-analysis",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#space-time-error-analysis",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Space-Time Error Analysis",
    "text": "Space-Time Error Analysis\nObserved vs.¬†Predicted\nLet‚Äôs use our best model (Model 4) for error analysis.\n\n\nCode\ntest &lt;- test %&gt;%\n  mutate(\n    error = Trip_Count - pred4,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"PM Rush\",\n      hour &gt; 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred4)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 4 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nInterpretation:\nWhere the model performs well: It does best in low-to-moderate demand conditions, especially overnight and during mid-day. Those periods have fewer big swings and smaller trip counts, so the model can track the pattern closely. On weekends‚Äîwhen demand is generally smoother‚Äîthe points cluster tightly and the green line stays close to the red line. The model doesn‚Äôt dramatically over- or under-shoot in these calmer periods.\nWhere the model struggles: It struggles during high-demand commuter windows, especially the AM and PM rush hours. In these panels, the cloud of points stretches upward far past the green line, showing that the model systematically under-predicts the busiest moments. Sharp surges in demand are hard to anticipate with simple temporal lags, so the model tends to flatten out the peaks. There is also more scatter on weekday rush hours than on weekends, reflecting the added variability of work-day travel.\nIn conclusion, the model handles routine, low-volume times well, but it misses the sudden spikes that define heavy commuting periods."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#spatial-error-patterns",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#spatial-error-patterns",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Spatial Error Patterns",
    "text": "Spatial Error Patterns\nIn this part, we are exploring whether prediction errors are clustered in certain parts of Philadelphia.\n\n\nCode\n# 1. Calculate MAE by stations\nstation_errors &lt;- test %&gt;%\n  filter(!is.na(pred4)) %&gt;%\n  group_by(start_station, start_lat.y, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.y), !is.na(start_lon.y))\n\n# 2. Map 1ÔºöPrediction Errors\np1 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.y, color = MAE),\n    size = 0.5, alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12, barheight = 1,\n    title.position = \"top\", title.hjust = 0.5\n  ))\n\n# 3. Map 2ÔºöAverage Demand\np2 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.y, color = avg_demand),\n    size = 0.5, alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12, barheight = 1,\n    title.position = \"top\", title.hjust = 0.5\n  ))\n\n# 4. Combine two maps\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\nInterpretation:\nErrors form a dense cluster in and around Center City, extending slightly into University City and parts of South Philly. The neighborhoods in center city have the highest errors.Because these stations often sit near job centers, transit hubs, parks, and retail corridors ‚Äî all places with unpredictable surges. In short, The model struggles where human behavior is least predictable and demand is most dynamic."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#temporal-error-patterns",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#temporal-error-patterns",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Temporal Error Patterns",
    "text": "Temporal Error Patterns\nWhen are we most wrong?\n\n\nCode\n# MAE by time of day and day type\ntemporal_errors &lt;- test %&gt;%\n  group_by(time_of_day, weekend) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nInterpretation:\nWhen are errors highest? Errors peak during the commute periods: PM Rush and AM Rush. These periods have the most unpredictable spikes in demand."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#errors-and-demographics",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#errors-and-demographics",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Errors and Demographics",
    "text": "Errors and Demographics\nAre prediction errors related to neighborhood characteristics?\n\n\nCode\n# Join demographic data to station errors\nstation_errors_demo &lt;- station_errors %&gt;%\n  left_join(\n    station_attributes %&gt;% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %&gt;%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 &lt;- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 &lt;- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 &lt;- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n\n\n\n\n\n\n\n\n\nInterpretation:\nPrediction errors slightly related to neighborhood characteristics?\n\nHigher-income neighborhoods show slightly higher errors. The trend line slopes upward: as median income increases, MAE tends to rise a bit. This likely reflects busier, more mixed-use areas where demand is less predictable.\nNeighborhoods with high transit use show lower errors. Stations located in areas with more transit riders tend to have lower MAE. Transit-heavy areas typically have more stable, commute-driven travel patterns that are easier for the model to predict.\nStations in whiter neighborhoods show slightly higher errors. The relationship is mild, but the upward slope suggests that stations in predominantly white, higher-income areas may see more inconsistent demand‚Äîoften driven by leisure or irregular trip purposes."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#feature-selection-and-engineering",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#feature-selection-and-engineering",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Feature Selection and Engineering",
    "text": "Feature Selection and Engineering\nFrom the error analysis above, we noticed that stations with large prediction errors are concentrated in Center City. From the data structure, we also know that the distribution of Trip_Count is highly skewed. There are many observations with 0 or 1 trip and a few observations with very large counts. The large number of 0 or 1 counts makes the model under-predict for high values. The extreme large counts make the model over-predict for low values. This creates a structural bias that we cannot fully remove.\nTo address this, we want to add variables that can detect and partially adjust for this pattern. Distance to Center City is a natural candidate. It can help the model sense the spatial structure behind these errors. However, the station dummy variables may already capture most of the distance differences. Therefore, the distance variable needs to be transformed or used in interaction terms to avoid collinearity with the station fixed effects.\nWhen we examine the trip-count-over-time plots, we also see spikes that appear irregularly. These spikes may reflect special events that disrupt the usual demand pattern. They do not happen often, but they can create extreme values and break the roughly linear relationship that the model tries to learn. This can have a strong impact on model performance. Federal holidays are one potential source of such events. The current model does not include enough variables to detect these effects.\nBased on these observations, we decide to add two new variables:\n1.Holiday indicator\n\n\nCode\nlibrary(lubridate)\n\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  mutate(\n    # Convert datetime column to Date\n    date = as.Date(interval60),\n\n    # Weekday with Monday = 1, Tuesday = 2, ..., Sunday = 7\n    wday_mon = wday(date, week_start = 1),\n    \n    # US Federal Holidays (rule-based, works for any year)\n    \n    # New Year's Day: January 1st\n    holiday_newyear = month(date) == 1  & mday(date) == 1,\n\n    # Martin Luther King Jr. Day: 3rd Monday in January\n    holiday_mlk = month(date) == 1 &\n                  wday_mon == 1 &        \n                  mday(date) &gt;= 15 & mday(date) &lt;= 21,\n\n    # Presidents‚Äô Day: 3rd Monday in February\n    holiday_pres = month(date) == 2 &\n                   wday_mon == 1 &       \n                   mday(date) &gt;= 15 & mday(date) &lt;= 21,\n\n    # Memorial Day: last Monday in May (Monday on or after the 25th)\n    holiday_memorial = month(date) == 5 &\n                       wday_mon == 1 &     \n                       mday(date) &gt;= 25,\n\n    # Independence Day: July 4th\n    holiday_july4 = month(date) == 7 & mday(date) == 4,\n\n    # Labor Day: 1st Monday in September (Monday between 1st and 7th)\n    holiday_labor = month(date) == 9 &\n                    wday_mon == 1 &        \n                    mday(date) &lt;= 7,\n\n    # Thanksgiving: 4th Thursday in November\n    holiday_thanks = month(date) == 11 &\n                     wday_mon == 4 &              # Thursday\n                     mday(date) &gt;= 22 & mday(date) &lt;= 28,\n\n    # Christmas Day: December 25th\n    holiday_xmas = month(date) == 12 & mday(date) == 25,\n\n    # Combined holiday indicator: 1 if any of the above holidays, 0 otherwise\n    holiday_any = as.integer(\n      holiday_newyear | holiday_mlk | holiday_pres |\n      holiday_memorial | holiday_july4 | holiday_labor |\n      holiday_thanks | holiday_xmas\n    )\n  )\n\n\n2.Distance to Center City\n\n\nCode\n# 1. City Hall point\nphilly_cbd_3857 &lt;- st_sfc(\n  st_point(c(-75.1636, 39.9526)),   # lon, lat\n  crs = 4326\n) %&gt;%\n  st_transform(3857)\n\n# project the dataframe to the same CRS 3857 (to calculate distance in meter)\nstations_dist &lt;- stations_sf %&gt;%\n  st_transform(3857) %&gt;%\n  mutate(\n    dist_center_m = as.numeric(st_distance(geometry, philly_cbd_3857))\n  ) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(start_station, dist_center_m) %&gt;%\n  distinct()\n\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  left_join(stations_dist, by = \"start_station\")\n\n\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  mutate(\n    Rain_any   = as.integer(Precipitation &gt; 0),\n    Heavy_rain = as.integer(Precipitation &gt;= 0.3) \n  )\n\n# let's make a little change in the intersection by using \"weekday*rush_hour\"\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  mutate(\n    weekday = ifelse(dotw %in% c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"), 1, 0)\n  )\n\n#split the train adn test data\ntrain &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 37)\n\ntest &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 37)"
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#model-improvement",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#model-improvement",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model Improvement",
    "text": "Model Improvement\n\nModel 6: Adjust variables in model 4\nIn this model, we refine some existing variables and use the result as the second baseline for later improved models.\nFirst, based on the summary of Model 4, we noticed that the Precipitation variable did not perform well. However, rain conditions should be an important factor for bike demand. The original variable is very small in magnitude because the total amount of rain is spread across 24 hours. To better capture the effect of rain, we transform it into two dummy variables that indicate whether it is rainy and whether it is raining heavily.\nSecond, we believe the weekend indicator is important because the demand pattern on weekends is different from weekdays. In the previous model, it did not work well because the interaction term weekend:rush_hour is highly collinear with as.factor(hour). To keep the weekend/weekday effect in the model, we replace this term with the interaction as.factor(hour):weekend.\n\n\nCode\nmodel6 &lt;- lm(\n  Trip_Count ~ as.factor(hour)* weekend + dotw_simple + \n    Temperature + \n    Rain_any + Heavy_rain +\n    lag1Hour + lag3Hours + lag1day  + \n    as.factor(month) + \n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y + \n    as.factor(start_station),  # Rush hour effects different on weekends\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model6, test_data = test)\n\n\nModel: model6 \n  Type:               OLS \n  R-squared (train):  0.3702 \n  Adj R-squared:      0.3698 \n  MAE (train):        0.664 \n  MAE (test):         0.6783 \n\n\nInterpretation:\nModel 6 serves as our second baseline model after refining several key predictors.\nOn weekdays, the model shows strong morning and evening peaks that are consistent with commuting.\nOn weekends, the peaks flatten and shift toward the middle of the day.\nThe new rain dummies behave as expected: both rainy and heavy-rain hours are associated with lower trip counts compared with dry conditions.\nDemographics and station fixed effects mainly shift the overall level of demand at each station, while the refined weather and weekend terms improve the shape of the predicted time profile without a large change in overall MAE.\n\n\nModel 7: Add Holiday Indicator and Distance to center city Interation\nThis model is based on Model 6 and adds two new components: a holiday indicator and an interaction between distance to Center City and rush hour. As discussed above, the interaction term is not only used to capture the relationship between distance and peak-hour demand, but also to reduce collinearity with the station fixed effects. Adding these two variables may help the model better detect structural patterns in the data and improve overall performance.\n\n\nCode\nmodel7 &lt;- lm(\n  Trip_Count ~ as.factor(hour)* weekday + \n    dotw_simple + \n    Temperature + \n    Rain_any + Heavy_rain +\n    lag1Hour + lag3Hours + lag1day  + \n    as.factor(month) + \n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y + \n    dist_center_m*rush_hour  +                              # interaction\n    holiday_any +                                           # holiday indicator\n    as.factor(start_station),  \n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(model7, test_data = test)\n\n\nModel: model7 \n  Type:               OLS \n  R-squared (train):  0.3751 \n  Adj R-squared:      0.3748 \n  MAE (train):        0.6584 \n  MAE (test):         0.6762 \n\n\nInterpretation:\nModel 7 builds on Model 6 by adding two variables that are motivated directly by our error analysis and knowledge of Philadelphia‚Äôs spatial and temporal structure: a holiday indicator and an interaction between distance to Center City and rush hour. The idea is that holidays and downtown peak periods are exactly when we observed the largest systematic errors in previous models. These additions slightly improve performance: the test MAE decreases from 0.6783 in Model 6 to 0.6762 in Model 7, and (R^2) rises only marginally from 0.3702 to 0.3751. This shows that, even after trying to encode what we learned about the error patterns, urban form, and time-of-day effects into new variables, most of the predictive power still comes from the core ingredients introduced earlier (time of day, temporal lags, and station fixed effects). The remaining errors are likely driven by noise and unobserved factors‚Äîsuch as special events or local disruptions‚Äîthat are difficult to capture with simple linear terms.\n\n\nModel 8: Try a poisson model for count data (base on model 7)\nIn addition to the linear regression models, we also estimate a Poisson regression with a log link to better reflect the count nature of the bike trip data. For this specification, we keep the same set of predictors as in Model 7 and only change the estimation method from OLS to Poisson. This allows us to directly compare the two approaches under an identical model structure. The Poisson model serves as a robustness check: it tests whether the main effects of time-of-day, weekend, weather, lagged demand, distance to Center City, and holidays remain consistent when we use a count model that is more appropriate for non-negative integer outcomes.\n\n\nCode\n# Poisson model with log link\npoisson_model7 &lt;- glm(\n  Trip_Count ~ \n    as.factor(hour) * weekday +       \n    dotw_simple +                     \n    Temperature + Rain_any + Heavy_rain +\n    lag1Hour + lag3Hours + lag1day +\n    as.factor(month) +\n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y +\n    dist_center_m*rush_hour +          \n    holiday_any +\n    as.factor(start_station),\n  family = poisson(link = \"log\"),\n  data = train\n)\n\n\n\n\nCode\nmodel_metrics_lm(poisson_model7, test_data = test)\n\n\nModel: poisson_model7 \n  Type:               Poisson \n  R-squared (train):  0.3828 \n  Adj R-squared:      0.3824 \n  MAE (train):        0.6221 \n  MAE (test):         0.6461 \n\n\nInterpretation:\nFinally, we re-estimate Model 7 using a Poisson regression with a log link while keeping exactly the same set of predictors. This change in estimation method leads to a modest but clear improvement in predictive performance: the test MAE decreases from 0.6762 in the OLS version of Model 7 to 0.6461 in the Poisson model, and the (R^2) on the training set increases slightly from 0.3751 to 0.3828. The Poisson specification fits the count nature of bike trips better and reduces some of the systematic under- and over-prediction we saw in the linear model, but the gain is not dramatic, which suggests that most remaining errors are due to noise and unobserved shocks rather than the choice between OLS and Poisson."
  },
  {
    "objectID": "Assignments/assignment_5/Assignment_5_coop.html#overall-model-evaluation",
    "href": "Assignments/assignment_5/Assignment_5_coop.html#overall-model-evaluation",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Overall Model Evaluation",
    "text": "Overall Model Evaluation\n\n\nCode\n# Get predictions on test set\n\ntest &lt;- test %&gt;%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test),\n    pred6 = predict(model6, newdata = test),\n    pred7 = predict(model7, newdata = test),\n    pred8 = predict(poisson_model7, newdata = test, type = \"response\")\n  )\n\n# Calculate MAE for each model\nmae_results2 &lt;- data.frame(\n  \n  Model = paste0(\n    \"Model \", 1:8\n  ),\n  Description = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\",\n    \"6. Adjust variables (Base on M4)\",\n    \"7. + Holiday Indicator and Distance Interaction\",\n    \"8. Poisson using variables in model 7\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred6), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred7), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred8), na.rm = TRUE)\n  ) \n) %&gt;%\n  mutate(\n    improvement_from_baseline = round((MAE[1] - MAE) / MAE[1] * 100, 1),\n    improvement_from_lags = round((MAE[6] - MAE) / MAE[2] * 100, 1)\n  )\n\nkable(mae_results2, \n      digits = 4,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"Description\", \"MAE (trips)\", \"% Better than M1\", \"% Better than M6\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nDescription\nMAE (trips)\n% Better than M1\n% Better than M6\n\n\n\n\nModel 1\n1. Time + Weather\n0.8293\n0.0\n-22.0\n\n\nModel 2\n2. + Temporal Lags\n0.6857\n17.3\n-1.1\n\n\nModel 3\n3. + Demographics\n0.6868\n17.2\n-1.2\n\n\nModel 4\n4. + Station FE\n0.6831\n17.6\n-0.7\n\n\nModel 5\n5. + Rush Hour Interaction\n0.6833\n17.6\n-0.7\n\n\nModel 6\n6. Adjust variables (Base on M4)\n0.6783\n18.2\n0.0\n\n\nModel 7\n7. + Holiday Indicator and Distance Interaction\n0.6762\n18.5\n0.3\n\n\nModel 8\n8. Poisson using variables in model 7\n0.6461\n22.1\n4.7\n\n\n\n\n\nInterpretation:\nFrom Model 3 to Model 7, we keep adding more variables and refining the specification, but the improvement over Model 2 is quite limited. After including temporal lags in Model 2, the MAE already drops from 0.83 to about 0.69 trips, and this remains the dominant gain. Models 3‚Äì5 add demographics, station fixed effects, and a rush-hour interaction, and Models 6‚Äì7 further adjust weather, weekend, distance, and holiday variables. However, the MAE stays in a very narrow range around 0.68‚Äì0.676, which is only a small improvement compared to Model 2.\nThis pattern suggests that once we control for time-of-day and recent demand history, most additional predictors only provide marginal extra information for short-term station-level demand. Demographics and station dummies mainly shift overall levels rather than change hour-to-hour variation, and the added interaction terms capture more subtle structure but do not drastically reduce prediction error on the test set.\nIn contrast, Model 8 changes the estimation method rather than the predictor set. It applies a Poisson regression using the same variables as Model 7. This shift to a count model leads to a more noticeable improvement: the MAE decreases to 0.6461 trips, which is substantially better than the OLS models with the same predictors. This indicates that better matching the distributional form of the outcome (non-negative counts) can yield additional gains even when the set of explanatory variables is already rich.\n\nVisualize Model Comparison\n\n\nCode\nggplot(mae_results2, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\",alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 4)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#assignment-overview",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\n\n# Load required packages\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(tigris)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(here)\nlibrary(units)\n\n# Load spatial data\ncensus_api_key(\"fef2af5588030f3fd4b474bd111e0d63a7ec2837\")\n\npa_county &lt;- st_read(\"data/Pennsylvania_County_Boundaries.shp\") \npa_hos &lt;- st_read(\"data/hospitals.geojson\")\npa_tracts &lt;- tracts(state = \"PA\", year = 2020, class = \"sf\")\n\n# Check that all data loaded correctly\n\nggplot(data = pa_county) +\n  geom_sf(fill = \"lightblue\", color = \"white\", size = 0.2) +\n  labs(title = \"Census Tracts in Philadelphia County, PA (2020)\",\n       caption = \"Source: US Census Bureau TIGER/Line\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(data = pa_tracts) +\n  geom_sf(fill = \"lightblue\", color = \"white\", size = 0.2) +\n  labs(title = \"Census Tracts in Philadelphia County, PA (2020)\",\n       caption = \"Source: US Census Bureau TIGER/Line\") +\n  theme_minimal()\n\n\n\n\n\n\n\nst_crs(pa_hos)\nst_crs(pa_county)\nst_crs(pa_tracts)\n\nQuestions to answer: - How many hospitals are in your dataset?\n223 rows in the data.\n\nHow many census tracts?\n\n67 rows in the data.\n\nWhat coordinate reference system is each dataset in?\n\nPA hospitals:EPSG 4326 PA county boundaries:EPSG 3857 PA census tracts:EPSG 4269\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables: - Total population - Median household income - Population 65 years and over (you may need to sum multiple age categories)\nYour Task:\n\n# Get demographic data from ACS\npa_pop &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B01003_001\",\n  state = \"PA\",\n  year = 2022,\n  geometry = FALSE\n)\n\npa_income &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  state = \"PA\",\n  year = 2022,\n  geometry = FALSE\n)\nage65_vars &lt;- c(\n  \"B01001_020\", \"B01001_021\", \"B01001_022\", \"B01001_023\", \"B01001_024\", \"B01001_025\", # Male 65+\n  \"B01001_044\", \"B01001_045\", \"B01001_046\", \"B01001_047\", \"B01001_048\", \"B01001_049\"  # Female 65+\n)\n\npa_age65 &lt;- get_acs(\n  geography = \"tract\",\n  variables = age65_vars,\n  state = \"PA\",\n  year = 2022,\n  geometry = FALSE\n) %&gt;%\n  group_by(GEOID) %&gt;%\n  summarize(age65_total = sum(estimate))\n\n\n\n# Join to tract boundaries\n\npa_demographics &lt;- pa_tracts %&gt;%\n  left_join(pa_pop %&gt;% select(GEOID, total_pop = estimate), by = \"GEOID\") %&gt;%\n  left_join(pa_income %&gt;% select(GEOID, median_income = estimate), by = \"GEOID\") %&gt;%\n  left_join(pa_age65, by = \"GEOID\") %&gt;%\n  mutate(pct_65plus = 100 * age65_total / total_pop)\npa_demographics\n\nSimple feature collection with 3446 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.51985 ymin: 39.7198 xmax: -74.68956 ymax: 42.51607\nGeodetic CRS:  NAD83\nFirst 10 features:\n   STATEFP COUNTYFP TRACTCE       GEOID    NAME             NAMELSAD MTFCC\n1       42      095  017703 42095017703  177.03  Census Tract 177.03 G5020\n2       42      101  012204 42101012204  122.04  Census Tract 122.04 G5020\n3       42      129  804300 42129804300    8043    Census Tract 8043 G5020\n4       42      129  804801 42129804801 8048.01 Census Tract 8048.01 G5020\n5       42      129  805200 42129805200    8052    Census Tract 8052 G5020\n6       42      101  012203 42101012203  122.03  Census Tract 122.03 G5020\n7       42      101  013602 42101013602  136.02  Census Tract 136.02 G5020\n8       42      101  034502 42101034502  345.02  Census Tract 345.02 G5020\n9       42      101  000902 42101000902    9.02    Census Tract 9.02 G5020\n10      42      101  001201 42101001201   12.01   Census Tract 12.01 G5020\n   FUNCSTAT   ALAND AWATER    INTPTLAT     INTPTLON total_pop median_income\n1         S 3708021   9639 +40.6498956 -075.3948647      3197         87232\n2         S  879459  56473 +40.0009407 -075.2120769      3915         48000\n3         S 1014739      0 +40.2918690 -079.5478131      2012         54318\n4         S 6852606      0 +40.3011416 -079.6013733      2281         48412\n5         S 1873022      0 +40.1523157 -079.8745928      1865         44375\n6         S  227696      0 +40.0074872 -075.2093403      1074         63750\n7         S  246812      0 +39.9716057 -075.1800748      3975        116375\n8         S 1021734      0 +40.0814101 -075.0384569      5299         55439\n9         S  155167      0 +39.9471596 -075.1566389      2736         89063\n10        S  327555  30728 +39.9470373 -075.1799169      4323        116935\n   age65_total                       geometry pct_65plus\n1         1001 MULTIPOLYGON (((-75.4109 40...  31.310604\n2         1118 MULTIPOLYGON (((-75.22066 4...  28.556833\n3          491 MULTIPOLYGON (((-79.55534 4...  24.403579\n4          513 MULTIPOLYGON (((-79.62842 4...  22.490136\n5          347 MULTIPOLYGON (((-79.88358 4...  18.605898\n6           24 MULTIPOLYGON (((-75.21389 4...   2.234637\n7          393 MULTIPOLYGON (((-75.18637 3...   9.886792\n8          946 MULTIPOLYGON (((-75.04729 4...  17.852425\n9          702 MULTIPOLYGON (((-75.16017 3...  25.657895\n10         428 MULTIPOLYGON (((-75.18714 3...   9.900532\n\nsum(is.na(pa_demographics$median_income))\n\n[1] 63\n\nmedian(pa_demographics$median_income, na.rm = TRUE)\n\n[1] 70188\n\n\npa_hos &lt;- st_transform(pa_hos, st_crs(pa_county)) pa_tracts &lt;- st_transform(pa_tracts, st_crs(pa_county))\nQuestions to answer: - What year of ACS data are you using?\n2022\n\nHow many tracts have missing income data?\n\n63\n\nWhat is the median income across all PA census tracts?\n\n$70188\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\n\n# bottom 25%\nlow_income_threshold &lt;- quantile(pa_demographics$median_income, 0.25, na.rm = TRUE)  \n# top 25%\nhigh_elderly_threshold &lt;- quantile(pa_demographics$pct_65plus, 0.75, na.rm = TRUE)   \n\n\npa_vulnerable &lt;- pa_demographics %&gt;%\n  mutate(\n    vulnerable_income = median_income &lt; low_income_threshold,\n    vulnerable_age = pct_65plus &gt; high_elderly_threshold,\n    vulnerable = vulnerable_income & vulnerable_age\n  )\n\nvulnerable_count &lt;- sum(pa_vulnerable$vulnerable, na.rm = TRUE)\ntotal_tracts &lt;- nrow(pa_vulnerable)\nvulnerable_pct &lt;- 100 * vulnerable_count / total_tracts\n\nvulnerable_count\n\n[1] 165\n\nvulnerable_pct\n\n[1] 4.78816\n\n\nQuestions to answer: - What income threshold did you choose and why?\nI chose the bottom 25% of tracts based on median household income, which corresponds to tracts with income below approximately $54,000. This threshold identifies communities that are relatively low-income compared to the rest of Pennsylvania.\n\nWhat elderly population threshold did you choose and why?\n\nElderly population threshold: I selected the top 25% of tracts by percentage of residents aged 65 and over, representing tracts where more than about 20.5% of residents are elderly. This captures areas with a significantly aging population that may face accessibility and healthcare challenges.\n\nHow many tracts meet your vulnerability criteria?\n\n165\n\nWhat percentage of PA census tracts are considered vulnerable by your definition?\n\n4.78816%\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n# Transform to appropriate projected CRS\npa_vul &lt;- pa_vulnerable %&gt;%\n  filter(vulnerable)\npa_vul_proj &lt;- st_transform(pa_vul, 26917)\npa_hos_proj &lt;- st_transform(pa_hos, 26917)\n\n# Calculate distance from each tract centroid to nearest hospital\n\nvul_center &lt;- pa_vul_proj %&gt;%\n  st_centroid()\n\nvul_center &lt;- vul_center %&gt;%\n  mutate(\n    nearest_idx = st_nearest_feature(., pa_hos_proj)  \n  ) %&gt;%\n  mutate(\n    dist_mi = st_distance(., pa_hos_proj[nearest_idx, ], by_element = TRUE) %&gt;%\n      set_units(\"mi\") %&gt;%\n      drop_units() \n  )\n\nsummary_stats &lt;- vul_center %&gt;%\n  summarize(\n    avg_dist_mi = mean(dist_mi, na.rm = TRUE),\n    max_dist_mi = max(dist_mi, na.rm = TRUE),\n    over15_count = sum(dist_mi &gt; 15, na.rm = TRUE),\n    total_vul = n()\n  ) %&gt;%\n  mutate(\n    pct_over15 = 100 * over15_count / total_vul\n  )\n\nsummary_stats %&gt;%\n  st_drop_geometry() %&gt;%\n  kable(\n    caption = \"Table. Hospital Access Among Vulnerable Census Tracts in Pennsylvania\",\n    digits = 2,\n    align = \"lrrrr\"\n  )\n\n\nTable. Hospital Access Among Vulnerable Census Tracts in Pennsylvania\n\n\navg_dist_mi\nmax_dist_mi\nover15_count\ntotal_vul\npct_over15\n\n\n\n\n4.75\n19.18\n9\n165\n5.45\n\n\n\n\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles - Explain why you chose your projection\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts?\n4.75miles\n\nWhat is the maximum distance?\n\n19.16miles\n\nHow many vulnerable tracts are more than 15 miles from the nearest hospital?\n\n9\n\n\n\nStep 5: Identify Underserved Areas\nDefine ‚Äúunderserved‚Äù as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n# Create underserved variable\n\nvul_center &lt;- vul_center %&gt;%\n  mutate(\n    underserved= (as.numeric(dist_mi&gt; 15))\n  )\n\nunder_count= sum(vul_center$underserved)  \nvul_pct= under_count/ nrow(vul_center)\nunder_count\n\n[1] 9\n\nvul_pct\n\n[1] 0.05454545\n\n\nQuestions to answer: - How many tracts are underserved?\n9\n\nWhat percentage of vulnerable tracts are underserved?\n\n5.45%\n\nDoes this surprise you? Why or why not?\n\nYes ‚Äî I didn‚Äôt expect any tracts to be this far from a hospital. This suggests that while most of Pennsylvania‚Äôs vulnerable areas have reasonable healthcare access, a few rural or remote tracts still face significant geographic barriers to medical services.\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\npa_county &lt;- st_transform(pa_county, crs=26917)\nvul_center&lt;- st_transform(vul_center, crs=st_crs(pa_county))\n\nst_county &lt;- st_join(\n  pa_county,\n  vul_center,\n  join  = st_contains,\n  left  = TRUE                # keep all counties\n)\n\n# Aggregate statistics by county\n\nagg_county &lt;- st_county %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarise(\n    vul_count= sum(vulnerable),\n    und_count= sum(underserved),\n    pct_vul_underserved = 100 * sum(vulnerable & underserved) / sum(vulnerable),\n    avg_dist_vulnerable = mean(dist_mi[vulnerable], na.rm = TRUE),\n    vul_pop = sum(total_pop[vulnerable]),\n    und_pop = sum(total_pop[underserved])\n  )\n\nagg_county &lt;- agg_county %&gt;%\n  mutate(across(where(is.numeric), ~replace(., is.na(.), 0)))\nagg_county\n\nSimple feature collection with 67 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 539820.7 ymin: 4396937 xmax: 1034915 ymax: 4680425\nProjected CRS: NAD83 / UTM zone 17N\n# A tibble: 67 √ó 8\n   COUNTY_NAM vul_count und_count pct_vul_underserved avg_dist_vulnerable\n * &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;               &lt;dbl&gt;               &lt;dbl&gt;\n 1 ADAMS              0         0                   0                0   \n 2 ALLEGHENY         22         0                   0                2.84\n 3 ARMSTRONG          1         0                   0               10.0 \n 4 BEAVER             5         0                   0                2.83\n 5 BEDFORD            2         0                   0                7.97\n 6 BERKS              0         0                   0                0   \n 7 BLAIR              3         0                   0                3.07\n 8 BRADFORD           1         1                 100               16.7 \n 9 BUCKS              0         0                   0                0   \n10 BUTLER             0         0                   0                0   \n# ‚Ñπ 57 more rows\n# ‚Ñπ 3 more variables: vul_pop &lt;dbl&gt;, und_pop &lt;dbl&gt;, geometry &lt;MULTIPOLYGON [m]&gt;\n\n\n\n# --- Step 6: Top 5 counties with highest % of underserved vulnerable tracts ---\ntop5_underserved &lt;- agg_county %&gt;%\n  st_drop_geometry() %&gt;%\n  arrange(desc(pct_vul_underserved)) %&gt;%\n  select(COUNTY_NAM, pct_vul_underserved, vul_count, und_count) %&gt;%\n  head(5)\n\n# --- Step 7: Top 5 counties with largest vulnerable population far from hospitals ---\ntop5_vulpop_far &lt;- agg_county %&gt;%\n  st_drop_geometry() %&gt;%\n  arrange(desc(und_pop)) %&gt;%\n  select(COUNTY_NAM, und_pop, vul_pop, avg_dist_vulnerable) %&gt;%\n  head(5)\n\n# --- Step 8: Display results ---\ntop5_underserved\n\n# A tibble: 5 √ó 4\n  COUNTY_NAM pct_vul_underserved vul_count und_count\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 BRADFORD                   100         1         1\n2 FOREST                     100         1         1\n3 JUNIATA                    100         1         1\n4 MONROE                     100         1         1\n5 SULLIVAN                   100         1         1\n\ntop5_vulpop_far\n\n# A tibble: 5 √ó 4\n  COUNTY_NAM und_pop vul_pop avg_dist_vulnerable\n  &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;               &lt;dbl&gt;\n1 BRADFORD      5466    5466               16.7 \n2 CLEARFIELD    5285   13056               11.4 \n3 DAUPHIN       4392    8410                9.96\n4 POTTER        4257    9062                9.55\n5 CRAWFORD      2842    9072                8.35\n\n\n\n\n\nRequired county-level statistics: - Number of vulnerable tracts - Number of underserved tracts\n- Percentage of vulnerable tracts that are underserved - Average distance to nearest hospital for vulnerable tracts - Total vulnerable population\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts?\nBRADFORD FOREST JUNIATA MONROE SULLIVAN\n\nWhich counties have the most vulnerable people living far from hospitals?\n\nBRADFORD CLEARFIELD DAUPHIN POTTER CRAWFORD\n\nAre there any patterns in where underserved counties are located?\n\nThese are typically larger rural counties where population is dispersed and hospitals are sparse. Urban counties (like Philadelphia or Allegheny) rarely appear because of dense hospital networks.\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\nagg_county &lt;- agg_county %&gt;%\n  mutate(\n    z_vul_pop     = scales::rescale(vul_pop),       # vulnerability\n    z_underserved = scales::rescale(pct_vul_underserved), # service deficit\n    z_distance    = scales::rescale(avg_dist_vulnerable), # accessibility\n    priority_score = 0.25*z_vul_pop + 0.1*z_underserved + 0.65*z_distance\n  )\n\npriority_table &lt;- agg_county %&gt;%\n  arrange(desc(priority_score)) %&gt;%\n  slice(1:10) %&gt;%\n  transmute(\n    `County` = COUNTY_NAM,\n    `Priority Score` = round(priority_score*100, 2),\n    `Vulnerable Population` = comma(vul_pop),\n    `Avg. Dist. to Hospital (mi)` = round(avg_dist_vulnerable, 2),\n    `Underserved (%)` = percent(pct_vul_underserved / 100, accuracy = 0.01),\n    `Vulnerable Tracts` = vul_count,\n    `Underserved Tracts` = und_count,\n  )\n\npriority_table %&gt;%\n  st_drop_geometry() %&gt;%\n  kable(\n    caption = \"***Table 1. Top 10 Priority Counties for Healthcare Investment in Pennsylvania***\",\n    align = \"lrrrrrr\",\n    format = \"pipe\"\n  )\n\n\nTable 1. Top 10 Priority Counties for Healthcare Investment in Pennsylvania\n\n\n\n\n\n\n\n\n\n\n\nCounty\nPriority Score\nVulnerable Population\nAvg. Dist. to Hospital (mi)\nUnderserved (%)\nVulnerable Tracts\nUnderserved Tracts\n\n\n\n\nFOREST\n76.09\n2,701\n18.12\n100.00%\n1\n1\n\n\nMONROE\n74.10\n1,299\n17.73\n100.00%\n1\n1\n\n\nBRADFORD\n72.03\n5,466\n16.68\n100.00%\n1\n1\n\n\nSULLIVAN\n71.10\n918\n16.93\n100.00%\n1\n1\n\n\nJUNIATA\n67.81\n1,782\n15.92\n100.00%\n1\n1\n\n\nHUNTINGDON\n51.77\n2,558\n14.15\n0.00%\n1\n0\n\n\nCLEARFIELD\n48.54\n13,056\n11.37\n25.00%\n4\n1\n\n\nDAUPHIN\n44.11\n8,410\n9.96\n50.00%\n2\n1\n\n\nNORTHUMBERLAND\n43.73\n9,087\n11.17\n0.00%\n4\n0\n\n\nPOTTER\n41.26\n9,062\n9.55\n33.33%\n3\n1\n\n\n\n\n\nRequirements: - Use knitr::kable() or similar for formatting - Include descriptive column names - Format numbers appropriately (commas for population, percentages, etc.) - Add an informative caption - Sort by priority (you decide the metric)"
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#part-2-comprehensive-visualization",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\npa_county_plot &lt;- st_transform(agg_county, crs = 26917)     # NAD83 / UTM zone 17N\npa_hospitals_plot &lt;- st_transform(pa_hos_proj, crs = st_crs(pa_county_plot))\n\n\nggplot() +\n  geom_sf(\n    data = pa_county_plot,\n    aes(fill = pct_vul_underserved),\n    color = \"white\",\n    size = 0.2\n  ) +\n  \n  #Hospital points\n  geom_sf(\n    data = pa_hospitals_plot,\n    aes(shape = \"Hospital\"),  \n    color=\"orange\",\n    size = 1.5,\n    stroke = 0.4\n  ) +\n  \n  scale_fill_gradientn(\n    colors = c(\"grey92\", \"#6baed6\", \"#08306b\"),\n    name = \"% Vulnerable Tracts\\nUnderserved\",\n    labels = label_percent(scale = 1, accuracy = 1)\n  ) +\n  \n  labs(\n    title = \"Healthcare Access Challenges in Pennsylvania\",\n    subtitle = \"Counties shaded by share of vulnerable tracts that are underserved\",\n    caption = \"Source: ACS & Healthcare Facility Data\"\n  ) +\n  \n  theme_void(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, margin = margin(b = 5)),\n    plot.caption = element_text(size = 9, color = \"gray30\"),\n    legend.title = element_text(size = 11, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\npa_tracts &lt;- st_transform(pa_tracts, crs=26917)\nvul_center &lt;- st_transform(vul_center, crs=26917)\nvul_tracts_poly &lt;- st_join(pa_tracts, vul_center, join= st_contains)\n\n# Create detailed tract-level map\nggplot() +\n  geom_sf(\n    data = vul_tracts_poly,\n    aes(fill = as.factor(underserved)),\n    color = \"grey80\",\n    size = 0.5\n  ) +\n  \n  geom_sf(\n    data = pa_hospitals_plot,\n    aes(shape = \"Hospital\", color = \"Hospital\"),  # üëà ÈÄöËøá aes() ËøõÂÖ•Âõæ‰æã\n    size = 1.2,\n    stroke = 0.4\n  ) +\n  \n scale_fill_manual(\n    name = \"Underserved Status\",\n    values = c(\"0\" = \"lightcoral\", \"1\" = \"firebrick3\"),  # üëà ÂÆö‰πâ 0/1/NA È¢úËâ≤\n    labels = c(\"0\"=\"Vulnerable\", \"1\" = \"Vulnerabale & Underserved\" ),\n )+\n  \n  scale_color_manual(\n    name = \"\",  \n    values = c(\"Hospital\" = \"orange\")\n  ) +\n  scale_shape_manual(\n    name = \"\",\n    values = c(\"Hospital\" = 16)\n  ) +\n  \n  labs(\n    title = \"Healthcare Access Challenges in Pennsylvania\",\n    subtitle = \"Counties shaded by share of vulnerable tracts that are underserved\",\n    caption = \"Source: ACS & Healthcare Facility Data\"\n  ) +\n  \n  theme_void(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, margin = margin(b = 5)),\n    plot.caption = element_text(size = 9, color = \"gray30\"),\n    legend.title = element_text(size = 11, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\n\nggplot(vul_tracts_poly, aes(x = dist_mi, y = total_pop)) +\n  geom_point(alpha = 0.6, color = \"#FF7F50\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray30\") +\n  labs(\n    title = \"Hospital Distance vs. Vulnerable Population Size\",\n    x = \"Distance to Nearest Hospital (miles)\",\n    y = \"Vulnerable Population (people)\",\n    caption = \"Tracts farther from hospitals tend to have smaller but more isolated vulnerable populations.\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs.¬†vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\n\nEducation & Youth Services\nOption A: Educational Desert Analysis - Data: Schools, Libraries, Recreation Centers, Census tracts (child population) - Question: ‚ÄúWhich neighborhoods lack adequate educational infrastructure for children?‚Äù - Operations: Buffer schools/libraries (0.5 mile walking distance), identify coverage gaps, overlay with child population density - Policy relevance: School district planning, library placement, after-school program siting\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: ‚ÄúAre school zones safe for walking/biking, or are they crime hotspots?‚Äù - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nEnvironmental Justice\nOption C: Green Space Equity - Data: Parks, Street Trees, Census tracts (race/income demographics) - Question: ‚ÄúDo low-income and minority neighborhoods have equitable access to green space?‚Äù - Operations: Buffer parks (10-minute walk = 0.5 mile), calculate tree canopy or park acreage per capita, compare by demographics - Policy relevance: Climate resilience, environmental justice, urban forestry investment ‚Äî\n\n\nPublic Safety & Justice\nOption D: Crime & Community Resources - Data: Crime Incidents, Recreation Centers, Libraries, Street Lights - Question: ‚ÄúAre high-crime areas underserved by community resources?‚Äù - Operations: Aggregate crime counts to census tracts or neighborhoods, count community resources per area, spatial correlation analysis - Policy relevance: Community investment, violence prevention strategies ‚Äî\n\n\nInfrastructure & Services\nOption E: Polling Place Accessibility - Data: Polling Places, SEPTA stops, Census tracts (elderly population, disability rates) - Question: ‚ÄúAre polling places accessible for elderly and disabled voters?‚Äù - Operations: Buffer polling places and transit stops, identify vulnerable populations, find areas lacking access - Policy relevance: Voting rights, election infrastructure, ADA compliance\n\n\n\nHealth & Wellness\nOption F: Recreation & Population Health - Data: Recreation Centers, Playgrounds, Parks, Census tracts (demographics) - Question: ‚ÄúIs lack of recreation access associated with vulnerable populations?‚Äù - Operations: Calculate recreation facilities per capita by neighborhood, buffer facilities for walking access, overlay with demographic indicators - Policy relevance: Public health investment, recreation programming, obesity prevention\n\n\n\nEmergency Services\nOption G: EMS Response Coverage - Data: Fire Stations, EMS stations, Population density, High-rise buildings - Question: ‚ÄúAre population-dense areas adequately covered by emergency services?‚Äù - Operations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas - Policy relevance: Emergency preparedness, station siting decisions\n\n\n\nArts & Culture\nOption H: Cultural Asset Distribution - Data: Public Art, Museums, Historic sites/markers, Neighborhoods - Question: ‚ÄúDo all neighborhoods have equitable access to cultural amenities?‚Äù - Operations: Count cultural assets per neighborhood, normalize by population, compare distribution across demographic groups - Policy relevance: Cultural equity, tourism, quality of life, neighborhood identity\n\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nRecommended Starting Points\nIf you‚Äôre feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure: - You can access the spatial data - You can perform at least 2 spatial operations\n\n\nYour Analysis\nYour Task:\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\n\nphi_school &lt;- read_csv(\"data_2/PaSchoolDistricts2024_03/2025-2026 Master School List (20251001).csv\")\nphi_crime &lt;- read_csv(\"data_2/PaSchoolDistricts2024_03/incidents_part1_part2.csv\")\nbike &lt;- st_read(\"data_2/PaSchoolDistricts2024_03/Bike_Network.shp\")\n\nReading layer `Bike_Network' from data source \n  `C:\\Users\\12345\\Documents\\GitHub\\portfolio-setup-jyxu48\\Assignments\\assignment_2\\data_2\\PaSchoolDistricts2024_03\\Bike_Network.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5225 features and 8 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -8378937 ymin: 4847835 xmax: -8345146 ymax: 4883978\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ncity_limit &lt;- st_read(\"data_2/PaSchoolDistricts2024_03/City_Limits.geojson\")\n\nReading layer `City_Limits' from data source \n  `C:\\Users\\12345\\Documents\\GitHub\\portfolio-setup-jyxu48\\Assignments\\assignment_2\\data_2\\PaSchoolDistricts2024_03\\City_Limits.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -75.28031 ymin: 39.86747 xmax: -74.95575 ymax: 40.13793\nGeodetic CRS:  WGS 84\n\nphi_school &lt;- phi_school %&gt;%\n  separate(`GPS Location`, into = c(\"lat\", \"lng\"), sep = \",\") %&gt;%\n  mutate(\n    lat = as.numeric(lat),\n    lng = as.numeric(lng)\n  )\nphi_crime &lt;- phi_crime %&gt;% drop_na(lng, lat)\n\nst_school &lt;- st_as_sf(phi_school, coords = c(\"lng\", \"lat\"), crs = 4326)\nst_crime &lt;- st_as_sf(phi_crime, coords = c(\"lng\", \"lat\"), crs = 4326)\n\ntarget_crs &lt;- 26918\n\nst_school &lt;- st_transform(st_school, crs = target_crs)\nst_crime  &lt;- st_transform(st_crime,  crs = target_crs)\nbike       &lt;- st_transform(bike, crs = target_crs)\ncity_limit &lt;- st_transform(city_limit, crs = target_crs)\n\nggplot() +\n  geom_sf(data = city_limit, fill = \"grey95\", color = \"grey60\", size = 0.4) + \n  geom_sf(data = bike, color = \"#2b83ba\", size = 0.5) +\n  geom_sf(data = st_school, color = \"coral\", shape = 16, size = 1.2) +\n  labs(\n    title = \"Philadelphia Schools, Crime Incidents, and Bike Network\",\n    caption = \"Source: OpenDataPhilly\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\nQuestions to answer: - What dataset did you choose and why?\nI choose School List of Philadelphia, Crime Incidents of Philadelphia and Bike Networks. I choose these data because it‚Äôs a question directly related to urban safety and equitable access for students. - What is the data source and date?\nThe data is from ‚Äúopendataphilly.org‚Äù and it is the data of 2025. (date of bike network data is unknown) - How many features does it contain?\nSchool List contains 1:Administrative info 2:Contact info 3:Location Crime Incident contains 1:Time 2:Location 3:Crime category Bike Networks contains 1:Name 2:road info(type,length,location) - What CRS is it in? Did you need to transform it?\nThey are all in EPSG:4326 (WGS84). Yes I need to transform them into another CRS(EPSG:26918) which is more accurate locally.\n\n\nPose a research question\n\nWrite a clear research statement that your analysis will answer.\nAre schools located in high-crime areas also underserved by safe biking and walking infrastructure? Examples: - ‚ÄúDo vulnerable tracts have adequate public transit access to hospitals?‚Äù - ‚ÄúAre EMS stations appropriately located near vulnerable populations?‚Äù - ‚ÄúDo areas with low vehicle access have worse hospital access?‚Äù\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+): - Buffers - Spatial joins - Spatial filtering with predicates - Distance calculations - Intersections or unions - Point-in-polygon aggregation\nYour Task:\n\n# Your spatial analysis\n\nst_school &lt;- st_school %&gt;%\n  rename(Publication_Name = `Publication Name`)\nschool_buffers &lt;- st_buffer(st_school, dist = 305)  # 1000 ft ‚âà 305 m\n\ncrime_in_buffer &lt;- st_join(st_crime, school_buffers, join = st_within)\n\ncrime_summary &lt;- crime_in_buffer %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(Publication_Name) %&gt;%\n  summarise(crime_n = n())\n\n# Spatial intersection: calculate total length of bike network inside each buffer\nbike_in_buffer &lt;- st_intersection(bike, school_buffers)\n\nbike_summary &lt;- bike_in_buffer %&gt;%\n  group_by(Publication_Name) %&gt;%\n  summarise(bike_length_m = sum(st_length(geometry))) %&gt;%\n  mutate(bike_length_mi = set_units(bike_length_m, \"mi\"))\n\n# Combine results into a single summary table\nschool_safety &lt;- school_buffers %&gt;%\n  left_join(st_drop_geometry(crime_summary), by = \"Publication_Name\") %&gt;%\n  left_join(st_drop_geometry(bike_summary),  by = \"Publication_Name\") %&gt;%\n  mutate(\n    crime_n = replace_na(crime_n, 0),\n    bike_length_mi = replace_na(as.numeric(bike_length_mi), 0),\n    safety_index = crime_n / (bike_length_mi + 0.1)\n  )\n\n# Summary statistics\nsummary_stats &lt;- school_safety %&gt;%\n  st_drop_geometry() %&gt;%\n  summarise(\n    mean_crime = mean(crime_n),\n    mean_bike = mean(as.numeric(bike_length_mi)),\n    high_crime_schools = sum(crime_n &gt; mean_crime),\n    low_bike_schools = sum(as.numeric(bike_length_mi) &lt; mean_bike)\n  )\n\nprint(summary_stats)\n\n# A tibble: 1 √ó 4\n  mean_crime mean_bike high_crime_schools low_bike_schools\n       &lt;dbl&gt;     &lt;dbl&gt;              &lt;int&gt;            &lt;int&gt;\n1       183.     0.430                135              203\n\n# Visualization: highlight high-crime, low-bike schools\np1 &lt;- ggplot() +\n  geom_sf(data = city_limit, fill = \"grey95\", color = \"grey60\") +\n  geom_sf(data = bike, color = \"grey70\", size = 0.2) +\n  geom_sf(\n    data = school_safety,\n    aes(fill = safety_index),\n    shape = 21, color = \"black\", size = 2\n  ) +\n  scale_fill_gradientn(\n    colors = c(\n      \"#313695\",  \n      \"#74add1\",  \n      \"#e0f3f8\",  \n      \"#fee090\",  \n      \"#f46d43\",  \n      \"#a50026\"   \n    ),\n    name = \"Crime to Bike_length\"\n  ) +\n  \n  labs(\n    title = \"Philadelphia School Safety Zones\",\n    subtitle = \"High values indicate more crime and less bike coverage\",\n  ) +\n  \n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 8),\n    legend.position = \"right\"\n  )\n\np2 &lt;- ggplot() +\n  geom_sf(data = city_limit, fill = \"grey95\", color = \"grey60\") +\n  geom_sf(data = bike, color = \"grey70\", size = 0.4) +\n  geom_sf(\n    data = school_safety,\n    aes(fill = crime_n),\n    shape = 21, color = \"black\", size = 2\n  ) +\n  scale_fill_gradientn(\n    colors = c(\n      \"#313695\",  \n      \"#74add1\",  \n      \"#e0f3f8\",  \n      \"#fee090\",  \n      \"#f46d43\",  \n      \"#a50026\" \n    ),\n    name = \"Crime Incidents\\n(Count)\"\n  ) +\n  labs(\n    title = \"Philadelphia School Safety Zones\",\n    subtitle = \"Crime incidents within 1000ft school safety zones\",\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 8),\n    legend.position = \"right\"\n  )\n\np3 &lt;- ggplot() +\n  geom_sf(data = city_limit, fill = \"grey95\", color = \"grey60\") +\n  geom_sf(data = bike, color = \"grey70\", size = 0.4) +\n  geom_sf(\n    data = school_safety,\n    aes(fill = bike_length_mi),\n    shape = 21, color = \"black\", size = 2\n  ) +\n  scale_fill_gradientn(\n    colors = c(\n      \"#a50026\",\n      \"#f46d43\", \n      \"#fee090\",\n      \"#e0f3f8\", \n      \"#74add1\", \n      \"#313695\"\n    ),\n    name = \"Bike Network\\n(Miles)\"\n  ) +\n  labs(\n    title = \"Philadelphia School Safety Zones\",\n    subtitle = \"Bike infrastructure length within 1000ft school safety zones\",\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 8),\n    legend.position = \"right\"\n  )\n\n(p1 | p2 | p3) +\n  plot_annotation(\n    title = \"Philadelphia School Safety Zone Analysis\",\n    caption = \"Data: OpenDataPhilly\"\n  )\n\n\n\n\n\n\n\n\nAnalysis requirements: - Clear code comments explaining each step - Appropriate CRS transformations - Summary statistics or counts - At least one map showing your findings - Brief interpretation of results (3-5 sentences)\nYour interpretation:\nThe maps show that high-crime and low-bike-coverage school zones largely overlap in central to northern and Western Philadelphia, indicating a clear spatial correlation between the two. This suggests that areas with more crime tend to have less biking infrastructure, highlighting potential safety and accessibility concerns for students in those neighborhoods."
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nTake a few moments to clean up your markdown document and then write a line or two or three about how you may have incorporated feedback that you recieved after your first assignment.\nI deleted some descriptive sentences and symbols so now the Assignment1 looks cleaner."
  },
  {
    "objectID": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#submission-requirements",
    "href": "Assignments/assignment_2/Jinyang_Xu_Assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it‚Äôs a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Minnesota Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#scenario",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Minnesota Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#learning-objectives",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#data-retrieval",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = my_state,\n  variables = c(\n    median_income = \"B19013_001\",   # Median Household Income\n    total_population = \"B01003_001\" # Total Population\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\ncounty_data\n\n# A tibble: 87 √ó 6\n   GEOID NAME  median_incomeE median_incomeM total_populationE total_populationM\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n 1 27001 Aitk‚Ä¶          56406           2136             15859                NA\n 2 27003 Anok‚Ä¶          95782           1203            363985                NA\n 3 27005 Beck‚Ä¶          68683           2197             35202                NA\n 4 27007 Belt‚Ä¶          62173           3292             46274                NA\n 5 27009 Bent‚Ä¶          70346           3948             41300                NA\n 6 27011 Big ‚Ä¶          63024           5327              5161                NA\n 7 27013 Blue‚Ä¶          70906           3327             69022                NA\n 8 27015 Brow‚Ä¶          67038           3230             25880                NA\n 9 27017 Carl‚Ä¶          74660           4154             36362                NA\n10 27019 Carv‚Ä¶         116308           3386            107216                NA\n# ‚Ñπ 77 more rows\n\n# Clean the county names to remove state name and \"County\" \n\n# Hint: use mutate() with str_remove()\ncounty_data_clean &lt;- county_data %&gt;%\n  mutate(\n    NAME = str_remove(NAME, \",.*\"),         # remove state name after the comma\n    NAME = str_remove(NAME, \" County$\")     # remove trailing \"County\"\n  )\n# Display the first few rows\nhead(county_data_clean)\n\n# A tibble: 6 √ó 6\n  GEOID NAME   median_incomeE median_incomeM total_populationE total_populationM\n  &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1 27001 Aitkin          56406           2136             15859                NA\n2 27003 Anoka           95782           1203            363985                NA\n3 27005 Becker          68683           2197             35202                NA\n4 27007 Beltr‚Ä¶          62173           3292             46274                NA\n5 27009 Benton          70346           3948             41300                NA\n6 27011 Big S‚Ä¶          63024           5327              5161                NA"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#data-quality-assessment",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Create reliability categories\n# Calculate MOE percentage and reliability categories using mutate()\ncounty_data_clean &lt;- county_data_clean %&gt;%\n  mutate(MOE = (median_incomeM / median_incomeE) * 100)\n\ncounty_data_clean &lt;- county_data_clean %&gt;%\n  mutate(reliability = case_when(\n    MOE &lt; 5 ~ \"High Confidence\",\n    MOE &gt;= 5 & MOE &lt;= 10 ~ \"Moderate Confidence\",\n    MOE &gt; 10 ~ \"Low Confidence\"\n  ))\n\n\ncounty_data_clean &lt;- county_data_clean %&gt;%\n  mutate(unreliable_flag = MOE &gt; 10)\ncounty_data_clean\n\n# A tibble: 87 √ó 9\n   GEOID NAME  median_incomeE median_incomeM total_populationE total_populationM\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n 1 27001 Aitk‚Ä¶          56406           2136             15859                NA\n 2 27003 Anoka          95782           1203            363985                NA\n 3 27005 Beck‚Ä¶          68683           2197             35202                NA\n 4 27007 Belt‚Ä¶          62173           3292             46274                NA\n 5 27009 Bent‚Ä¶          70346           3948             41300                NA\n 6 27011 Big ‚Ä¶          63024           5327              5161                NA\n 7 27013 Blue‚Ä¶          70906           3327             69022                NA\n 8 27015 Brown          67038           3230             25880                NA\n 9 27017 Carl‚Ä¶          74660           4154             36362                NA\n10 27019 Carv‚Ä¶         116308           3386            107216                NA\n# ‚Ñπ 77 more rows\n# ‚Ñπ 3 more variables: MOE &lt;dbl&gt;, reliability &lt;chr&gt;, unreliable_flag &lt;lgl&gt;\n\n# Create a summary showing count of counties in each reliability category\n\nreliability_summary &lt;- county_data_clean %&gt;%\n  count(reliability) %&gt;%\n  mutate(percentage = (n / sum(n)) * 100)\n\nreliability_summary\n\n# A tibble: 3 √ó 3\n  reliability             n percentage\n  &lt;chr&gt;               &lt;int&gt;      &lt;dbl&gt;\n1 High Confidence        46      52.9 \n2 Low Confidence          1       1.15\n3 Moderate Confidence    40      46.0 \n\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#high-uncertainty-counties",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\ntop5 &lt;- county_data_clean %&gt;%\n  arrange(desc(MOE)) %&gt;%\n  slice(1:5) %&gt;%\n  select(\n    County = NAME,\n    Median_Income = median_incomeE,\n    Margin_of_Error = median_incomeM,\n    MOE_Percentage = MOE,\n    Reliability = reliability\n  )\n# Format as table with kable() - include appropriate column names and caption\nkable(top5, caption = \"Top 5 Counties with Highest MOE Percentage (Median Income)\")\n\n\nTop 5 Counties with Highest MOE Percentage (Median Income)\n\n\n\n\n\n\n\n\n\nCounty\nMedian_Income\nMargin_of_Error\nMOE_Percentage\nReliability\n\n\n\n\nTraverse\n63456\n6655\n10.487582\nLow Confidence\n\n\nCottonwood\n63586\n6049\n9.513100\nModerate Confidence\n\n\nLake of the Woods\n61667\n5812\n9.424814\nModerate Confidence\n\n\nChippewa\n62112\n5823\n9.375000\nModerate Confidence\n\n\nRed Lake\n73889\n6742\n9.124498\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\n[High MOE means higher reliability for decision making. Small sample size or hidden factors account for high MOEs.]"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#focus-area-selection",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\nhigh_conf &lt;- county_data_clean %&gt;%\n  filter(reliability == \"High Confidence\") %&gt;%\n  slice(2)\n\nmoderate_conf &lt;- county_data_clean %&gt;%\n  filter(reliability == \"Moderate Confidence\") %&gt;%\n  slice(9)\n\nlow_conf &lt;- county_data_clean %&gt;%\n  filter(reliability == \"Low Confidence\") %&gt;%\n  slice(1)\n\nselected_counties_allv &lt;- bind_rows(high_conf, moderate_conf, low_conf)\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nselected_counties &lt;- selected_counties_allv %&gt;%\n  select(NAME, median_incomeE, MOE, reliability, total_populationE)\nselected_counties\n\n# A tibble: 3 √ó 5\n  NAME     median_incomeE   MOE reliability         total_populationE\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt;\n1 Anoka             95782  1.26 High Confidence                363985\n2 Douglas           72472  6.57 Moderate Confidence             39081\n3 Traverse          63456 10.5  Low Confidence                   3345\n\n\nComment on the output: [The output is clear, but I don‚Äôt know if it‚Äôs good to ignore the GEOID :-]"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#tract-level-demographics",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You‚Äôll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\nrace &lt;- c(\n  white = \"B03002_003\",\n  black = \"B03002_004\",\n  hispanic = \"B03002_012\",\n  total_pop = \"B03002_001\"\n)\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\nselected_county_codes &lt;- selected_counties_allv %&gt;%\n  mutate(county_code = str_sub(GEOID, 3, 5)) %&gt;%  \n  pull(county_code)\n\ntract_data &lt;- get_acs(\n  geography = \"tract\",\n  state = \"27\",\n  county = selected_county_codes,\n  variables = race,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\ntract_data\n\n# A tibble: 103 √ó 10\n   GEOID       NAME   whiteE whiteM blackE blackM hispanicE hispanicM total_popE\n   &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 27003050107 Censu‚Ä¶   2776    244      8     13        27        38       2931\n 2 27003050108 Censu‚Ä¶   5072    342     97     94       240       227       5737\n 3 27003050109 Censu‚Ä¶   5296    592     19     26       273       229       5673\n 4 27003050110 Censu‚Ä¶   2553    415     11     16        97        96       2773\n 5 27003050111 Censu‚Ä¶   3286    413     55     82        35        36       3455\n 6 27003050114 Censu‚Ä¶   3076    355      0      9        20        31       3173\n 7 27003050115 Censu‚Ä¶   5140    550      0     13        52        59       5783\n 8 27003050116 Censu‚Ä¶   4089    254     34     35        77        68       4518\n 9 27003050208 Censu‚Ä¶   2698    512    428    337        32        29       3304\n10 27003050210 Censu‚Ä¶   3801    170      0      9        20        28       4180\n# ‚Ñπ 93 more rows\n# ‚Ñπ 1 more variable: total_popM &lt;dbl&gt;\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\ntract_data2 &lt;- tract_data %&gt;%\n  mutate(pct_white = (whiteE / total_popE) * 100) %&gt;%\n  mutate(pct_black = (blackE / total_popE) * 100) %&gt;%\n  mutate(pct_hispanic = (hispanicE / total_popE) * 100)\n\ntract_data2 \n\n# A tibble: 103 √ó 13\n   GEOID       NAME   whiteE whiteM blackE blackM hispanicE hispanicM total_popE\n   &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 27003050107 Censu‚Ä¶   2776    244      8     13        27        38       2931\n 2 27003050108 Censu‚Ä¶   5072    342     97     94       240       227       5737\n 3 27003050109 Censu‚Ä¶   5296    592     19     26       273       229       5673\n 4 27003050110 Censu‚Ä¶   2553    415     11     16        97        96       2773\n 5 27003050111 Censu‚Ä¶   3286    413     55     82        35        36       3455\n 6 27003050114 Censu‚Ä¶   3076    355      0      9        20        31       3173\n 7 27003050115 Censu‚Ä¶   5140    550      0     13        52        59       5783\n 8 27003050116 Censu‚Ä¶   4089    254     34     35        77        68       4518\n 9 27003050208 Censu‚Ä¶   2698    512    428    337        32        29       3304\n10 27003050210 Censu‚Ä¶   3801    170      0      9        20        28       4180\n# ‚Ñπ 93 more rows\n# ‚Ñπ 4 more variables: total_popM &lt;dbl&gt;, pct_white &lt;dbl&gt;, pct_black &lt;dbl&gt;,\n#   pct_hispanic &lt;dbl&gt;\n\n# Add readable tract and county name columns using str_extract() or similar\n\ntract_data3 &lt;- tract_data2 %&gt;%\n  mutate(\n    tract_id = str_extract(NAME, \"Census Tract \\\\d+\"),     \n    tract_id = str_remove(tract_id, \"Census Tract \"),\n    \n    county_name = str_extract(NAME, \"(?&lt;=; ).* County\"),     \n    county_name = str_remove(county_name, \" County$\"),\n    \n    state_name  = str_extract(NAME, \"(?&lt;=; )[^;]+$\")         \n  ) %&gt;%\n  relocate(tract_id, county_name, state_name, .after = NAME)\n\ntract_data3\n\n# A tibble: 103 √ó 16\n   GEOID       NAME  tract_id county_name state_name whiteE whiteM blackE blackM\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 27003050107 Cens‚Ä¶ 501      Anoka       Minnesota    2776    244      8     13\n 2 27003050108 Cens‚Ä¶ 501      Anoka       Minnesota    5072    342     97     94\n 3 27003050109 Cens‚Ä¶ 501      Anoka       Minnesota    5296    592     19     26\n 4 27003050110 Cens‚Ä¶ 501      Anoka       Minnesota    2553    415     11     16\n 5 27003050111 Cens‚Ä¶ 501      Anoka       Minnesota    3286    413     55     82\n 6 27003050114 Cens‚Ä¶ 501      Anoka       Minnesota    3076    355      0      9\n 7 27003050115 Cens‚Ä¶ 501      Anoka       Minnesota    5140    550      0     13\n 8 27003050116 Cens‚Ä¶ 501      Anoka       Minnesota    4089    254     34     35\n 9 27003050208 Cens‚Ä¶ 502      Anoka       Minnesota    2698    512    428    337\n10 27003050210 Cens‚Ä¶ 502      Anoka       Minnesota    3801    170      0      9\n# ‚Ñπ 93 more rows\n# ‚Ñπ 7 more variables: hispanicE &lt;dbl&gt;, hispanicM &lt;dbl&gt;, total_popE &lt;dbl&gt;,\n#   total_popM &lt;dbl&gt;, pct_white &lt;dbl&gt;, pct_black &lt;dbl&gt;, pct_hispanic &lt;dbl&gt;"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#demographic-analysis",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\ntop_hispanic &lt;- tract_data3 %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) \n\ntop_hispanic2 &lt;- top_hispanic %&gt;%\n  select(tract_id,county_name,state_name,total_popM,pct_white,pct_black,pct_hispanic)\n\nkable(\n  top_hispanic2, \n  caption = \"Tract with Highest Hispanic/Latino Percentage\"\n) \n\n\nTract with Highest Hispanic/Latino Percentage\n\n\n\n\n\n\n\n\n\n\n\ntract_id\ncounty_name\nstate_name\ntotal_popM\npct_white\npct_black\npct_hispanic\n\n\n\n\n512\nAnoka\nMinnesota\n593\n50.65913\n12.24105\n28.24859\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\ncounty_summary &lt;- tract_data3 %&gt;%\n  group_by(county_name) %&gt;%\n  summarise(\n    n_tracts = n(),\n    avg_white = mean(pct_white, na.rm = TRUE),\n    avg_black = mean(pct_black, na.rm = TRUE),\n    avg_hispanic = mean(pct_hispanic, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\n# Create a nicely formatted table of your results using kable()\nkable(\n  county_summary,\n  digits = 1,  \n  caption = \"Average Demographics by County\"\n)\n\n\nAverage Demographics by County\n\n\ncounty_name\nn_tracts\navg_white\navg_black\navg_hispanic\n\n\n\n\nAnoka\n90\n77.4\n7.8\n5.3\n\n\nDouglas\n11\n94.3\n0.3\n2.3\n\n\nTraverse\n2\n85.2\n0.5\n3.8"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#moe-analysis-for-demographic-variables",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\ntract_moe &lt;- tract_data3 %&gt;%\n  mutate(\n    white_moe_pct    = (whiteM / whiteE) * 100,\n    black_moe_pct    = (blackM / blackE) * 100,\n    hispanic_moe_pct = (hispanicM / hispanicE) * 100\n  )\ntract_moe\n\n# A tibble: 103 √ó 19\n   GEOID       NAME  tract_id county_name state_name whiteE whiteM blackE blackM\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 27003050107 Cens‚Ä¶ 501      Anoka       Minnesota    2776    244      8     13\n 2 27003050108 Cens‚Ä¶ 501      Anoka       Minnesota    5072    342     97     94\n 3 27003050109 Cens‚Ä¶ 501      Anoka       Minnesota    5296    592     19     26\n 4 27003050110 Cens‚Ä¶ 501      Anoka       Minnesota    2553    415     11     16\n 5 27003050111 Cens‚Ä¶ 501      Anoka       Minnesota    3286    413     55     82\n 6 27003050114 Cens‚Ä¶ 501      Anoka       Minnesota    3076    355      0      9\n 7 27003050115 Cens‚Ä¶ 501      Anoka       Minnesota    5140    550      0     13\n 8 27003050116 Cens‚Ä¶ 501      Anoka       Minnesota    4089    254     34     35\n 9 27003050208 Cens‚Ä¶ 502      Anoka       Minnesota    2698    512    428    337\n10 27003050210 Cens‚Ä¶ 502      Anoka       Minnesota    3801    170      0      9\n# ‚Ñπ 93 more rows\n# ‚Ñπ 10 more variables: hispanicE &lt;dbl&gt;, hispanicM &lt;dbl&gt;, total_popE &lt;dbl&gt;,\n#   total_popM &lt;dbl&gt;, pct_white &lt;dbl&gt;, pct_black &lt;dbl&gt;, pct_hispanic &lt;dbl&gt;,\n#   white_moe_pct &lt;dbl&gt;, black_moe_pct &lt;dbl&gt;, hispanic_moe_pct &lt;dbl&gt;\n\n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\ntract_moe &lt;- tract_moe %&gt;%\n  mutate(\n    high_moe_flag = ifelse(\n      white_moe_pct &gt; 10 | black_moe_pct &gt; 10 | hispanic_moe_pct &gt; 10,\n      TRUE, FALSE\n    )\n  )\n\n# Create summary statistics showing how many tracts have data quality issues\nmoe_summary &lt;- tract_moe %&gt;%\n  summarise(\n    total_tracts = n(),\n    tracts_high_moe = sum(high_moe_flag, na.rm = TRUE),\n    pct_high_moe = (tracts_high_moe / total_tracts) * 100\n  )\n\nmoe_summary\n\n# A tibble: 1 √ó 3\n  total_tracts tracts_high_moe pct_high_moe\n         &lt;int&gt;           &lt;int&gt;        &lt;dbl&gt;\n1          103             103          100"
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#pattern-analysis",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n# Use group_by() and summarize() to create this comparison\\\n\npattern_sum &lt;- tract_moe %&gt;%\n  group_by(high_moe_flag) %&gt;%\n  summarise(\n    n_tracts       = n(),\n    avg_pop = mean(total_popE, na.rm = TRUE),\n    avg_white_pct  = mean(pct_white, na.rm = TRUE),\n    avg_black_pct  = mean(pct_black, na.rm = TRUE),\n    avg_hispanic_pct = mean(pct_hispanic, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\n\n# Create a professional table showing the patterns\n\nkable(\n  pattern_sum,\n  digits = 2,\n  caption = \"Comparison of Tracts by High vs. Low MOE\"\n)\n\n\nComparison of Tracts by High vs.¬†Low MOE\n\n\n\n\n\n\n\n\n\n\nhigh_moe_flag\nn_tracts\navg_pop\navg_white_pct\navg_black_pct\navg_hispanic_pct\n\n\n\n\nTRUE\n103\n3945.74\n79.39\n6.82\n4.93\n\n\n\n\n# INCIDENTÔºö The result showed that 100% of tracts were flagged as TRUE.\n# Therefore, the next step is to take a more detailed approach\n\ntract_moe1 &lt;- tract_moe %&gt;%\n  mutate(\n    white_high_moe    = white_moe_pct &gt; 10,\n    black_high_moe    = black_moe_pct &gt; 10,\n    hispanic_high_moe = hispanic_moe_pct &gt; 10\n  )\n\n\nrace_moe_sum &lt;- tract_moe1 %&gt;%\n  summarise(\n    white_flagged    = sum(white_high_moe, na.rm = TRUE),\n    black_flagged    = sum(black_high_moe, na.rm = TRUE),\n    hispanic_flagged = sum(hispanic_high_moe, na.rm = TRUE)\n  )\n\nrace_moe_sum\n\n# A tibble: 1 √ó 3\n  white_flagged black_flagged hispanic_flagged\n          &lt;int&gt;         &lt;int&gt;            &lt;int&gt;\n1            83           103              103\n\n\nPattern Analysis: The analysis reveals a clear pattern: racial and ethnic minority populations have consistently less reliable data at the tract level. Specifically, the MOE exceeded 10% for all Black and Hispanic/Latino population estimates across the selected tracts, while White population estimates were somewhat more reliable, though still flagged in over half of the tracts."
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#analysis-integration-and-professional-summary",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\nOverall Pattern Identification The multi-level analysis ‚Äî spanning county-level median income estimates and tract-level demographic breakdowns ‚Äî reveals systematic data reliability challenges in ACS 5-year estimates County-level: At county scale, median household income estimates show notable variability. Using a 10% MOE cutoff, most counties fell into moderate or low confidence categories, highlighting that uncertainty exists even at higher aggregation levels Tract-level: Reliability issues intensify at the tract scale. Every tract exceeded the 10% MOE threshold for at least one demographic subgroup, 100% of tracts were flagged as high-MOE:(\nEquity Assessment The communities at greatest risk of algorithmic bias are: Smaller or rural tracts: Populations with fewer respondents also face higher sampling error, compounding reliability problems. Black and Hispanic/Latino communities: Their tract-level ACS estimates never meet the 10% reliability threshold. If an allocation algorithm treats these noisy estimates as precise, it will undervalue their needs and direct resources elsewhere.\nRoot Cause Analysis The patterns are rooted in structural features of ACS data collection: Sampling limits: Smaller population groups produce larger standard errors. Differential nonresponse: Minority and immigrant communities may less likely to respond due to barriers and distrust. Survey design constraints: The ACS 5-year estimates aggregate limited samples, and subgroup breakdowns at tract scale exceed the data‚Äôs design precision.\nStrategic Recommendations Data useÔºö Flag any tract with MOE% &gt; 10 and avoid treating estimates as precise since sample size is small in tract level. Use fallback geography (county) for variables with extreme MOE.\nStrengthen inputsÔºö Blend multiple indicators (poverty, rent burden, child poverty) to reduce reliance on a single noisy variable. Improve ACS participation via targeted outreach and language support in undercounted communities.\nOutside the data: Introduce human review for flagged tracts before final funding decisions. Pilot and monitor the algorithm annually, recalibrating thresholds as survey data quality evolves. Document and communicate uncertainty explicitly to policymakers and stakeholders."
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#specific-recommendations",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\nsum_table &lt;- county_data_clean %&gt;%\n  select(NAME, median_incomeE, MOE, reliability) %&gt;%\n  mutate(\n    recommendation = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  )\n\n# Format as a professional table with kable()\n\nkable(\n  sum_table,\n  digits = 1,\n  caption = \"Decision Framework for Algorithmic Implementation\"\n)\n\n\nDecision Framework for Algorithmic Implementation\n\n\n\n\n\n\n\n\n\nNAME\nmedian_incomeE\nMOE\nreliability\nrecommendation\n\n\n\n\nAitkin\n56406\n3.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAnoka\n95782\n1.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBecker\n68683\n3.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBeltrami\n62173\n5.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nBenton\n70346\n5.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nBig Stone\n63024\n8.5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nBlue Earth\n70906\n4.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBrown\n67038\n4.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCarlton\n74660\n5.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCarver\n116308\n2.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCass\n61970\n2.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nChippewa\n62112\n9.4\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nChisago\n97446\n3.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nClay\n75006\n4.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nClearwater\n62723\n5.1\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCook\n71937\n6.5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCottonwood\n63586\n9.5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCrow Wing\n65975\n3.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nDakota\n101360\n1.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nDodge\n92890\n4.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nDouglas\n72472\n6.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFaribault\n64000\n5.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFillmore\n73234\n4.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nFreeborn\n65679\n3.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGoodhue\n78338\n4.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGrant\n67600\n7.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHennepin\n92595\n1.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHouston\n71580\n8.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHubbard\n67197\n3.5\nHigh Confidence\nSafe for algorithmic decisions\n\n\nIsanti\n84063\n4.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nItasca\n63962\n4.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nJackson\n68368\n5.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nKanabec\n68995\n4.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKandiyohi\n73285\n4.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKittson\n66000\n6.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nKoochiching\n59779\n7.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLac qui Parle\n66967\n5.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLake\n73860\n4.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLake of the Woods\n61667\n9.4\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLe Sueur\n87180\n4.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLincoln\n64750\n7.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLyon\n68919\n4.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMcLeod\n73296\n5.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMahnomen\n52739\n5.5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMarshall\n69396\n4.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMartin\n61674\n8.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMeeker\n75926\n3.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMille Lacs\n68088\n4.5\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMorrison\n66264\n4.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMower\n66972\n5.7\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMurray\n71500\n5.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nNicollet\n79113\n5.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nNobles\n62973\n8.1\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nNorman\n65278\n6.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nOlmsted\n90420\n3.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOtter Tail\n67990\n3.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPennington\n71504\n6.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPine\n65059\n3.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPipestone\n68341\n8.5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPolk\n69540\n5.4\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPope\n71212\n2.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRamsey\n78108\n1.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRed Lake\n73889\n9.1\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nRedwood\n65617\n5.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nRenville\n66313\n3.5\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRice\n78214\n3.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRock\n75060\n8.8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nRoseau\n70122\n3.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSt.¬†Louis\n66491\n2.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nScott\n118268\n3.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSherburne\n99431\n4.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSibley\n74781\n4.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nStearns\n73105\n2.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSteele\n79722\n4.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nStevens\n69737\n8.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSwift\n58362\n8.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nTodd\n63216\n5.5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nTraverse\n63456\n10.5\nLow Confidence\nRequires manual review or additional data\n\n\nWabasha\n75063\n4.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWadena\n54747\n4.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWaseca\n71856\n5.1\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWashington\n110828\n2.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWatonwan\n65197\n8.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWilkin\n67114\n5.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWinona\n66162\n6.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWright\n102980\n3.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYellow Medicine\n70605\n6.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: Aitkin, Anoka, Becker, Blue Earth, Brown, Carver, Cass, Chisago, Clay, Crow Wing, Dakota, Dodge, Fillmore, Freeborn, Goodhue, Hennepin, Hubbard, Isanti, Itasca, Kanabec, Kandiyohi, Lake, Le Sueur, Lyon, Marshall, Meeker, Mille Lacs, Morrison, Olmsted, Otter Tail, Pine, Pope, Ramsey, Renville, Rice, Roseau, St.¬†Louis, Scott, Sherburne, Sibley, Stearns, Steele, Wabasha, Wadena, Washington, Wright\n\nCounties listed above have suitable sample size and stronger ACS samples, which means lower MOE. Demographic estimates are robust enough to support algorithm-driven funding without significant reliability concerns. Algorithms here can be implemented immediately with standard fairness monitoring.\n\nCounties requiring additional oversight: Beltrami, Benton, Big Stone, Carlton, Chippewa, Clearwater, Cook, Cottonwood, Douglas, Faribault, Grant, Houston, Jackson, Kittson, Koochiching, Lac qui Parle, Lincoln, McLeod, Mahnomen, Martin, Mower, Murray, Nicollet, Nobles, Norman, Pennington, Pipestone, Polk, Red Lake, Redwood, Rock, Swift, Todd, Waseca, Watonwan, Wilkin, Winona, Yellow Medicine.\n\nRecommended monitoring actions for counties above: Outcome audits: Compare algorithmic allocations against program participation data. Sensitivity testing: Run models with upper/lower confidence bounds (estimate ¬± MOE) to test robustness. Flagging system: Automatically flag tracts/counties with borderline estimates for human review.\n\nCounties needing alternative approaches:ss Traverse\n\nSuggested alternatives: Manual review: Use staff expertise and local knowledge to adjust allocations. Supplemental data: Incorporate administrative records (school eligibility, Medicaid enrollment, unemployment claims) to cross-validate needs. Community engagement: Consult local service providers or conduct targeted surveys to supplement missing precision."
  },
  {
    "objectID": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#questions-for-further-investigation",
    "href": "Assignments/assignment_1/Jinyang_Xu_Assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\nBeyond race/ethnicity, do other vulnerable groups (e.g., low-income households, elderly residents, non-English speakers) also experience systematically higher MOE? Could combining ACS with administrative datasets (SNAP, TANF, Medicaid) reduce disparities in data reliability? How stable are tract-level estimates over time, and do reliability issues persist across survey cycles?"
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html",
    "title": "Spatial Predictive Analysis",
    "section": "",
    "text": "In this Assignment, I will build a spatial predictive model for burglaries using count regression and spatial features.I will document the process, and interpret results."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#assignment-overview",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#assignment-overview",
    "title": "Spatial Predictive Analysis",
    "section": "",
    "text": "In this Assignment, I will build a spatial predictive model for burglaries using count regression and spatial features.I will document the process, and interpret results."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-1.1-load-chicago-spatial-data",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-1.1-load-chicago-spatial-data",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 1.1: Load Chicago Spatial Data",
    "text": "Exercise 1.1: Load Chicago Spatial Data\n\n\nCode\n# Load police districts (used for spatial cross-validation)\npoliceDistricts &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(District = dist_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 25 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load police beats (smaller administrative units)\npoliceBeats &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(Beat = beat_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 277 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load Chicago boundary\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\") %&gt;%\n  st_transform('ESRI:102271')\n\n\nReading layer `chicagoBoundary' from data source \n  `https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -87.8367 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304\nGeodetic CRS:  WGS 84\n\n\nCode\ncat(\"‚úì Loaded spatial boundaries\\n\")\n\n\n‚úì Loaded spatial boundaries\n\n\nCode\ncat(\"  - Police districts:\", nrow(policeDistricts), \"\\n\")\n\n\n  - Police districts: 25 \n\n\nCode\ncat(\"  - Police beats:\", nrow(policeBeats), \"\\n\")\n\n\n  - Police beats: 277 \n\n\n\n\n\n\n\n\nNoteCoordinate Reference System\n\n\n\nWe‚Äôre using ESRI:102271 (Illinois State Plane East, NAD83, US Feet). This is appropriate for Chicago because:\n\nIt minimizes distortion in this region\nUses feet (common in US planning)\nAllows accurate distance calculations"
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-1.2-load-burglary-data",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-1.2-load-burglary-data",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 1.2: Load Burglary Data",
    "text": "Exercise 1.2: Load Burglary Data\n\n\nCode\n# Load from provided data file (downloaded from Chicago open data portal)\nburglaries &lt;- st_read(here(\"labs/lab4/data\", \"burglaries.shp\")) %&gt;% \n  st_transform('ESRI:102271')\n\n\nReading layer `burglaries' from data source \n  `C:\\Users\\12345\\Documents\\GitHub\\portfolio-setup-jyxu48\\labs\\lab4\\data\\burglaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 7482 features and 22 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 340492 ymin: 552959.6 xmax: 367153.5 ymax: 594815.1\nProjected CRS: NAD83(HARN) / Illinois East\n\n\nCode\n# Check the data\ncat(\"\\n‚úì Loaded burglary data\\n\")\n\n\n\n‚úì Loaded burglary data\n\n\nCode\ncat(\"  - Number of burglaries:\", nrow(burglaries), \"\\n\")\n\n\n  - Number of burglaries: 7482 \n\n\nCode\ncat(\"  - CRS:\", st_crs(burglaries)$input, \"\\n\")\n\n\n  - CRS: ESRI:102271 \n\n\nCode\ncat(\"  - Date range:\", min(burglaries$Date, na.rm = TRUE), \"to\", \n    max(burglaries$Date, na.rm = TRUE), \"\\n\")\n\n\n  - Date range: 17167 to 17532 \n\n\nInterpretation 1.1: There are 7482 rows of burglaries in the dataset. The time period is from 2017/01/01 to 2018/01/01, indicates that it contains 366 days of data.\nIt is essential that all datasets share the same Coordinate Reference System (CRS). If layers are kept in different CRS formats, spatial operations will produce incorrect or misaligned results.\nFor this project, because we are working in Chicago, we use the specific projected CRS ‚ÄúESRI: 102271 ‚Äî Illinois StatePlane East (meters)‚Äù that provides accurate distance and area measurements for Chicago.\n\n\n\n\n\n\nWarningCritical Pause #1: Data Provenance\n\n\n\nBefore proceeding, consider where this data came from:\nWho recorded this data? Chicago Police Department officers and detectives\nWhat might be missing?\n\nUnreported burglaries (victims didn‚Äôt call police)\nIncidents police chose not to record\nDowngraded offenses (burglary recorded as trespassing)\nSpatial bias (more patrol = more recorded crime)\n\nThink About Was there a Department of Justice investigation of CPD during this period? What did they find about data practices?"
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-1.3-visualize-point-data",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-1.3-visualize-point-data",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 1.3: Visualize Point Data",
    "text": "Exercise 1.3: Visualize Point Data\n\n\nCode\n# Simple point map\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = burglaries, color = \"#d62828\", size = 0.1, alpha = 0.4) +\n  labs(\n    title = \"Burglary Locations\",\n    subtitle = paste0(\"Chicago 2017, n = \", nrow(burglaries))\n  )\n\n# Density surface using modern syntax\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(burglaries)),\n    aes(X, Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"  # Modern ggplot2 syntax (not guide = FALSE)\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Kernel density estimation\"\n  )\n\n# Combine plots using patchwork (modern approach)\np1 + p2 + \n  plot_annotation(\n    title = \"Spatial Distribution of Burglaries in Chicago\",\n    tag_levels = 'A'\n  )\n\n\n\n\n\n\n\n\n\nInterpretation 1.2:\nThe spatial pattern of burglaries in Chicago is clustered rather than evenly distributed. The point map shows that incidents concentrate in several distinct corridors rather than being spread uniformly across the city. The KDE surface reinforces this pattern with three major hotspot zones:\n1.Near North / River North area 2.West Side around Humboldt Park and Logan Square 3.South Side corridor extending from Washington Park toward Englewood\nThese locations share certain environmental features commonly associated with elevated burglary risk. In addition, some hotspots overlap areas with documented social and economic disadvantage, vacant housing, and higher levels of physical disorder‚Äîfactors linked to burglary opportunity structures in environmental criminology."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-2.1-understanding-the-fishnet",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-2.1-understanding-the-fishnet",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 2.1: Understanding the Fishnet",
    "text": "Exercise 2.1: Understanding the Fishnet\nA fishnet grid converts irregular point data into a regular grid of cells where we can:\n\nAggregate counts\nCalculate spatial features\nApply regression models\n\n\n\nCode\n# Create 500m x 500m grid\nfishnet &lt;- st_make_grid(\n  chicagoBoundary,\n  cellsize = 500,  # 500 meters per cell\n  square = TRUE\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Keep only cells that intersect Chicago\nfishnet &lt;- fishnet[chicagoBoundary, ]\n\n# View basic info\ncat(\"‚úì Created fishnet grid\\n\")\n\n\n‚úì Created fishnet grid\n\n\nCode\ncat(\"  - Number of cells:\", nrow(fishnet), \"\\n\")\n\n\n  - Number of cells: 2458 \n\n\nCode\ncat(\"  - Cell size:\", 500, \"x\", 500, \"meters\\n\")\n\n\n  - Cell size: 500 x 500 meters\n\n\nCode\ncat(\"  - Cell area:\", round(st_area(fishnet[1,])), \"square meters\\n\")\n\n\n  - Cell area: 250000 square meters\n\n\nInterpretation 2.1:\nI use a regular grid because it gives us equal-sized, neutral spatial units instead of boundaries that were drawn for political or administrative reasons. This makes the analysis cleaner: cell areas are comparable, distance-based features are easier to compute, and we reduce some of the MAUP problems that come from highly uneven, irregular neighborhoods or census tracts.\nThe trade-off is that grid cells are analytically convenient but socially artificial. They do not correspond to real neighborhoods, police beats, or planning districts, so the results are harder to explain to practitioners and often need to be translated back into those official units.\nIn short: grids are better for modelling, official boundaries are better for communication."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-2.2-aggregate-burglaries-to-grid",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-2.2-aggregate-burglaries-to-grid",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 2.2: Aggregate Burglaries to Grid",
    "text": "Exercise 2.2: Aggregate Burglaries to Grid\n\n\nCode\n# Spatial join: which cell contains each burglary?\nburglaries_fishnet &lt;- st_join(burglaries, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(countBurglaries = n())\n\n# Join back to fishnet (cells with 0 burglaries will be NA)\nfishnet &lt;- fishnet %&gt;%\n  left_join(burglaries_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(countBurglaries = replace_na(countBurglaries, 0))\n\n# Summary statistics\ncat(\"\\nBurglary count distribution:\\n\")\n\n\n\nBurglary count distribution:\n\n\nCode\nsummary(fishnet$countBurglaries)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   3.042   5.000  40.000 \n\n\nCode\ncat(\"\\nCells with zero burglaries:\", \n    sum(fishnet$countBurglaries == 0), \n    \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$countBurglaries == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero burglaries: 781 / 2458 ( 31.8 %)\n\n\n\n\nCode\n# Visualize aggregated counts\nggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  geom_sf(data = chicagoBoundary, fill = NA, color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(\n    name = \"Burglaries\",\n    option = \"plasma\",\n    trans = \"sqrt\",  # Square root for better visualization of skewed data\n    breaks = c(0, 1, 5, 10, 20, 40)\n  ) +\n  labs(\n    title = \"Burglary Counts by Grid Cell\",\n    subtitle = \"500m x 500m cells, Chicago 2017\"\n  ) +\n  theme_crime()\n\n\n\n\n\n\n\n\n\nInterpretation 2.2:\nThe burglary counts are skewed: a large number of grid cells have little or zero incidents, while a small number of cells have relatively high counts. This produces a classic zero-inflated and overdispersed distribution, where the variance is much larger than the mean.\nSo many zeros appear because burglary is a spatially clustered event. Most parts of the city experience little or no burglary activity, while a few hotspots absorb most incidents. This distribution is not well-fit by a standard Poisson model, which assumes that the mean and variance are equal. Instead, the overdispersion suggests that models like the Negative Binomial or a zero-inflated count model are more appropriate."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.1-load-311-sanitation-code-complaints",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.1-load-311-sanitation-code-complaints",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 4.1: Load 311 Sanitation Code Complaints",
    "text": "Exercise 4.1: Load 311 Sanitation Code Complaints\n\n\nCode\nsanitation_coms &lt;- read_csv(\"data/311_Service_Requests_-_Sanitation_Code_Complaints_-_Historical_20251113.csv\")%&gt;%\n    filter(\n    mdy(`Creation Date`) &gt;= mdy(\"12/31/2016\") &\n    mdy(`Creation Date`) &lt;  mdy(\"01/02/2018\")\n  ) %&gt;% \n  filter(!is.na(Latitude), !is.na(Longitude)) %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102271')\n\ncat(\"‚úì Loaded Sanitation Code Complaints\\n\")\n\n\n‚úì Loaded Sanitation Code Complaints\n\n\nCode\ncat(\"  - Number of calls:\", nrow(sanitation_coms), \"\\n\")\n\n\n  - Number of calls: 19767 \n\n\n\n\n\n\n\n\nNoteData Loading Note\n\n\n\nThe data was downloaded from Chicago‚Äôs Open Data Portal. You can now request an api from the Chicago portal and tap into the data there.\nConsider: How might the 311 reporting system itself be biased? Who calls 311? What neighborhoods have better 311 awareness?\nThe 311 system is not a neutral measure of neighborhood conditions. Reporting depends on who knows about 311, who trusts government systems, and who feels empowered to complain. Higher-income and higher-resource neighborhoods tend to have greater 311 awareness, more digital access, and stronger expectations of government responsiveness, which leads to more reporting. In contrast, lower-income communities, immigrant neighborhoods, and historically over-policed areas may underreport due to limited awareness, language barriers, distrust of city services, or simply different norms around what is ‚Äúworth‚Äù reporting."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.2-count-of-sanitation-code-complaints-per-cell",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.2-count-of-sanitation-code-complaints-per-cell",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 4.2: Count of Sanitation Code Complaints per Cell",
    "text": "Exercise 4.2: Count of Sanitation Code Complaints per Cell\n\n\nCode\n# Aggregate Sanitation Code Complaints to fishnet\n\nsanitation_fishnet &lt;- st_join(sanitation_coms, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(sanitation_coms = n())\n\n# Join to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(sanitation_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(sanitation_coms = replace_na(sanitation_coms, 0))\n\ncat(\"Sanitation code complaints distribution:\\n\")\n\n\nSanitation code complaints distribution:\n\n\nCode\nsummary(sanitation_coms)\n\n\n Creation Date         Status          Completion Date   \n Length:19767       Length:19767       Length:19767      \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n Service Request Number Type of Service Request\n Length:19767           Length:19767           \n Class :character       Class :character       \n Mode  :character       Mode  :character       \n                                               \n                                               \n                                               \n                                               \n What is the Nature of this Code Violation? Street Address        ZIP Code    \n Length:19767                               Length:19767       Min.   :60601  \n Class :character                           Class :character   1st Qu.:60618  \n Mode  :character                           Mode  :character   Median :60628  \n                                                               Mean   :60630  \n                                                               3rd Qu.:60641  \n                                                               Max.   :60827  \n                                                               NA's   :38     \n  X Coordinate      Y Coordinate          Ward       Police District\n Min.   :1108635   Min.   :1814788   Min.   : 1.00   Min.   : 1.00  \n 1st Qu.:1152208   1st Qu.:1858904   1st Qu.:13.50   1st Qu.: 7.00  \n Median :1163111   Median :1896362   Median :25.00   Median :12.00  \n Mean   :1162881   Mean   :1889196   Mean   :24.43   Mean   :12.62  \n 3rd Qu.:1172575   3rd Qu.:1916944   3rd Qu.:35.00   3rd Qu.:18.00  \n Max.   :1205116   Max.   :1951488   Max.   :50.00   Max.   :25.00  \n                                                                    \n Community Area    Location                  geometry    \n Min.   : 1.00   Length:19767       POINT        :19767  \n 1st Qu.:19.00   Class :character   epsg:NA      :    0  \n Median :29.00   Mode  :character   +proj=tmer...:    0  \n Mean   :36.12                                           \n 3rd Qu.:58.00                                           \n Max.   :77.00                                           \n                                                         \n\n\n\n\nCode\nfishnet$san_cont &lt;- pmin(fishnet$sanitation_coms, 60)\n\nfishnet$san_over60 &lt;- fishnet$sanitation_coms &gt; 60\n\n# Sanitation code complaints\np1 &lt;- ggplot() +\n  geom_sf(\n    data = fishnet,\n    aes(fill = san_cont),\n    color = NA\n  ) +\n  scale_fill_viridis_c(\n    option = \"plasma\",     # purple ‚Üí yellow\n    limits = c(0, 60),     \n    name = \"Sanitation\"\n  ) +\n  geom_sf(\n    data = subset(fishnet, san_over60),\n    fill = \"#f0f921\",    \n    color = NA\n  ) +\n  labs(title = \"Sanitation Code Complaints (Continuous Gradient, 60+ Highlighted)\") +\n  theme_crime()\n\n\n# Burglaries\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\") +\n  labs(title = \"Burglaries\") +\n  theme_crime()\n\np1 + p2 +\n  plot_annotation(\n    title = \"Comparison of Sanitation Complaints and Burglaries (Fixed Bins)\"\n  )\n\n\n\n\n\n\n\n\n\nInterpretation 4.1:\nBoth maps show broadly overlapping hotspot corridors, especially in the south side. This visual overlap suggests that physical disorder and burglary risk cluster in similar environments‚Äîplaces with poor maintenance, dumping, or abandoned properties also show elevated burglary activity. This aligns with environmental criminology and ‚Äúbroken windows‚Äù dynamics: neighborhoods with more visible disorder may signal lower guardianship and create more opportunities for property crime.\nIn short: the two patterns are not identical, but they cluster in many of the same spaces, indicating a potential relationship worth modeling statistically."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.3-nearest-neighbor-features",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.3-nearest-neighbor-features",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 4.3: Nearest Neighbor Features",
    "text": "Exercise 4.3: Nearest Neighbor Features\nCount in a cell is one measure. Distance to the nearest 3 sanitation complaints captures local context.\n\n\nCode\n# Calculate mean distance to 3 nearest sanitation complaints\n# (Do this OUTSIDE of mutate to avoid sf conflicts)\n\n# Get coordinates\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\nsanitation_coords &lt;- st_coordinates(sanitation_coms)\n\n# Calculate k nearest neighbors and distances\nnn_result &lt;- get.knnx(sanitation_coords, fishnet_coords, k = 3)\n\n# Add to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    sanitation_coms.nn = rowMeans(nn_result$nn.dist)\n  )\n\ncat(\"‚úì Calculated nearest neighbor distances\\n\")\n\n\n‚úì Calculated nearest neighbor distances\n\n\nCode\nsummary(fishnet$sanitation_coms.nn)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   1.031  108.565  181.196  294.538  332.417 2213.320 \n\n\nInterpretation 4.2:\nA low value of sanitation_coms.nn means that the cell is very close to several sanitation code complaints.This indicates the cell is surrounded by visible physical disorder.\nA high value means the nearest sanitation complaints are far away, suggesting that the surrounding area has few sanitation-related problems or lower reporting activity.\nThis measure is informative because distance to sanitation complaints captures the spatial context of disorder, not just what happens inside a single grid cell. Even if a cell has zero complaints itself, being close to areas with repeated sanitation violations may still signal neighborhood sanitation problem ‚Äîcondition may associated with higher burglary risk."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.4-distance-to-hot-spots",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-4.4-distance-to-hot-spots",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 4.4: Distance to Hot Spots",
    "text": "Exercise 4.4: Distance to Hot Spots\nLet‚Äôs identify clusters of sanitation complaints using Local Moran‚Äôs I, then calculate distance to these hot spots.\n\n\nCode\n# Function to calculate Local Moran's I\ncalculate_local_morans &lt;- function(data, variable, k = 5) {\n  \n  # Create spatial weights\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n  \n  # Calculate Local Moran's I\n  local_moran &lt;- localmoran(data[[variable]], weights)\n  \n  # Classify clusters\n  mean_val &lt;- mean(data[[variable]], na.rm = TRUE)\n  \n  data %&gt;%\n    mutate(\n      local_i = local_moran[, 1],\n      p_value = local_moran[, 5],\n      is_significant = p_value &lt; 0.05,\n      \n      moran_class = case_when(\n        !is_significant ~ \"Not Significant\",\n        local_i &gt; 0 & .data[[variable]] &gt; mean_val ~ \"High-High\",\n        local_i &gt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-Low\",\n        local_i &lt; 0 & .data[[variable]] &gt; mean_val ~ \"High-Low\",\n        local_i &lt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-High\",\n        TRUE ~ \"Not Significant\"\n      )\n    )\n}\n\n# Apply to sanitation code complaints\nfishnet &lt;- calculate_local_morans(fishnet, \"sanitation_coms\", k = 5)\n\n\n\n\nCode\n# Visualize hot spots\nggplot() +\n  geom_sf(\n    data = fishnet, \n    aes(fill = moran_class), \n    color = NA\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"High-High\" = \"#d7191c\",\n      \"High-Low\" = \"#fdae61\",\n      \"Low-High\" = \"#abd9e9\",\n      \"Low-Low\" = \"#2c7bb6\",\n      \"Not Significant\" = \"gray90\"\n    ),\n    name = \"Cluster Type\"\n  ) +\n  labs(\n    title = \"Local Moran's I: sanitation code complatins Clusters\",\n    subtitle = \"High-High = Hot spots of disorder\"\n  ) +\n  theme_crime()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Get centroids of \"High-High\" cells (hot spots)\nhotspots &lt;- fishnet %&gt;%\n  filter(moran_class == \"High-High\") %&gt;%\n  st_centroid()\n\n# Calculate distance from each cell to nearest hot spot\nif (nrow(hotspots) &gt; 0) {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(\n      dist_to_hotspot = as.numeric(\n        st_distance(st_centroid(fishnet), hotspots %&gt;% st_union())\n      )\n    )\n  \n  cat(\"‚úì Calculated distance to sanitation code comlaints hot spots\\n\")\n  cat(\"  - Number of hot spot cells:\", nrow(hotspots), \"\\n\")\n} else {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(dist_to_hotspot = 0)\n  cat(\"‚ö† No significant hot spots found\\n\")\n}\n\n\n‚úì Calculated distance to sanitation code comlaints hot spots\n  - Number of hot spot cells: 203 \n\n\nInterpretation 4.3:\nDistance to a cluster of sanitation complaints is more informative than distance to a single complaint because clusters capture persistent, spatially concentrated disorder, not just isolated events. A single sanitation complaint could be random, one-time, or caused by a single resident. But when many nearby cells all show high sanitation complaints, it signals a structural problem in the poor sanitation maintenance.\nLocal Moran‚Äôs I identifies exactly these patterns by detecting spatial autocorrelation. High-High clusters indicate areas where a cell with high sanitation complaints is surrounded by other high-value cells. These are true hotspots of disorder, not random noise. Low-High areas reveal local outliers, where a relatively clean cell sits next to a disorder hotspot.\n\n\n\n\n\n\nNote\n\n\n\nLocal Moran‚Äôs I identifies:\n\nHigh-High: Hot spots (high values surrounded by high values)\nLow-Low: Cold spots (low values surrounded by low values)\nHigh-Low / Low-High: Spatial outliers\n\nThis helps us understand spatial clustering patterns."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-6.1-poisson-regression",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-6.1-poisson-regression",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 6.1: Poisson Regression",
    "text": "Exercise 6.1: Poisson Regression\nBurglary counts are count data (0, 1, 2, 3‚Ä¶). We‚Äôll use Poisson regression.\n\n\nCode\n# Create clean modeling dataset\nfishnet_model &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    uniqueID,\n    District,\n    countBurglaries,\n    sanitation_coms,\n    sanitation_coms.nn,\n    dist_to_hotspot\n  ) %&gt;%\n  na.omit()  # Remove any remaining NAs\n\ncat(\"‚úì Prepared modeling data\\n\")\n\n\n‚úì Prepared modeling data\n\n\nCode\ncat(\"  - Observations:\", nrow(fishnet_model), \"\\n\")\n\n\n  - Observations: 1708 \n\n\nCode\ncat(\"  - Variables:\", ncol(fishnet_model), \"\\n\")\n\n\n  - Variables: 6 \n\n\n\n\nCode\n# Fit Poisson regression\nmodel_poisson &lt;- glm(\n  countBurglaries ~ sanitation_coms + sanitation_coms.nn + \n    dist_to_hotspot,\n  data = fishnet_model,\n  family = \"poisson\"\n)\n\n# Summary\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = countBurglaries ~ sanitation_coms + sanitation_coms.nn + \n    dist_to_hotspot, family = \"poisson\", data = fishnet_model)\n\nCoefficients:\n                      Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept)         2.22789690  0.03588604  62.083 &lt;0.0000000000000002 ***\nsanitation_coms     0.00264926  0.00106871   2.479              0.0132 *  \nsanitation_coms.nn -0.00449667  0.00017781 -25.289 &lt;0.0000000000000002 ***\ndist_to_hotspot    -0.00014660  0.00001029 -14.247 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6710.3  on 1707  degrees of freedom\nResidual deviance: 4060.3  on 1704  degrees of freedom\nAIC: 8128.6\n\nNumber of Fisher Scoring iterations: 5\n\n\nInterpretation 6.1:\nInterpret the coefficients.\n\nsanitation_coms (positive coefficient) Estimate: +0.00265 Cells with more sanitation code complaints tend to have slightly higher burglary counts. This suggests that areas with more visible disorder (trash issues, sanitation violations) also experience more burglaries, consistent with environmental criminology.\nsanitation_coms.nn (negative coefficient) Estimate: ‚Äì0.00450 This variable measures the average distance to nearby sanitation complaints. A negative sign means: Closer to sanitation complaint clusters ‚Üí more burglaries Farther away ‚Üí fewer burglaries This strengthens the idea that being near clusters of disorder is more predictive of burglary risk than just having a complaint inside the cell.\ndist_to_hotspot (negative coefficient) Estimate: ‚Äì0.00015 Cells closer to the burglary hotspot have higher burglary counts. As distance increases, burglaries decline‚Äîtypical spatial decay around a known hotspot."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-6.2-check-for-overdispersion",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-6.2-check-for-overdispersion",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 6.2: Check for Overdispersion",
    "text": "Exercise 6.2: Check for Overdispersion\nPoisson regression assumes mean = variance. Real count data often violates this (overdispersion).\n\n\nCode\n# Calculate dispersion parameter\ndispersion &lt;- sum(residuals(model_poisson, type = \"pearson\")^2) / \n              model_poisson$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 2), \"\\n\")\n\n\nDispersion parameter: 2.55 \n\n\nCode\ncat(\"Rule of thumb: &gt;1.5 suggests overdispersion\\n\")\n\n\nRule of thumb: &gt;1.5 suggests overdispersion\n\n\nCode\nif (dispersion &gt; 1.5) {\n  cat(\"‚ö† Overdispersion detected! Consider Negative Binomial model.\\n\")\n} else {\n  cat(\"‚úì Dispersion looks okay for Poisson model.\\n\")\n}\n\n\n‚ö† Overdispersion detected! Consider Negative Binomial model."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-6.3-negative-binomial-regression",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-6.3-negative-binomial-regression",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 6.3: Negative Binomial Regression",
    "text": "Exercise 6.3: Negative Binomial Regression\nIf overdispersed, use Negative Binomial regression (more flexible).\n\n\nCode\n# Fit Negative Binomial model\nmodel_nb &lt;- glm.nb(\n  countBurglaries ~ sanitation_coms + sanitation_coms.nn + \n    dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Summary\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = countBurglaries ~ sanitation_coms + sanitation_coms.nn + \n    dist_to_hotspot, data = fishnet_model, init.theta = 2.394358425, \n    link = log)\n\nCoefficients:\n                      Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept)         2.26783716  0.06164584  36.788 &lt;0.0000000000000002 ***\nsanitation_coms     0.00338771  0.00206738   1.639               0.101    \nsanitation_coms.nn -0.00483958  0.00025566 -18.930 &lt;0.0000000000000002 ***\ndist_to_hotspot    -0.00014494  0.00001549  -9.359 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.3944) family taken to be 1)\n\n    Null deviance: 3090.5  on 1707  degrees of freedom\nResidual deviance: 1784.5  on 1704  degrees of freedom\nAIC: 7152.6\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.394 \n          Std. Err.:  0.154 \n\n 2 x log-likelihood:  -7142.640 \n\n\nCode\n# Compare AIC (lower is better)\ncat(\"\\nModel Comparison:\\n\")\n\n\n\nModel Comparison:\n\n\nCode\ncat(\"Poisson AIC:\", round(AIC(model_poisson), 1), \"\\n\")\n\n\nPoisson AIC: 8128.6 \n\n\nCode\ncat(\"Negative Binomial AIC:\", round(AIC(model_nb), 1), \"\\n\")\n\n\nNegative Binomial AIC: 7152.6 \n\n\nInterpretation 6.2:\nmodel comparison\nThe Negative Binomial model fits better, with an AIC of 7152.6, compared to the Poisson model‚Äôs AIC of 8128.6. Lower AIC indicates a superior model, so the Negative Binomial clearly provides a better description of the burglary counts.\nThis tells us again that the data exhibits overdispersion: the variance is much larger than the mean, which violates the core assumption of the Poisson model. The Negative Binomial handles this extra variability by adding a dispersion parameter (theta), allowing it to model the heavy-tailed, zero-inflated structure of the burglary data more accurately."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-8.1-generate-final-predictions",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-8.1-generate-final-predictions",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 8.1: Generate Final Predictions",
    "text": "Exercise 8.1: Generate Final Predictions\n\n\nCode\n# Fit final model on all data\nfinal_model &lt;- glm.nb(\n  countBurglaries ~ sanitation_coms + sanitation_coms.nn + \n    dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Add predictions back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_nb = predict(final_model, fishnet_model, type = \"response\")[match(uniqueID, fishnet_model$uniqueID)]\n  )\n\n# Also add KDE predictions (normalize to same scale as counts)\nkde_sum &lt;- sum(fishnet$kde_value, na.rm = TRUE)\ncount_sum &lt;- sum(fishnet$countBurglaries, na.rm = TRUE)\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_kde = (kde_value / kde_sum) * count_sum\n  )"
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-8.2-compare-model-vs.-kde-baseline",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-8.2-compare-model-vs.-kde-baseline",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 8.2: Compare Model vs.¬†KDE Baseline",
    "text": "Exercise 8.2: Compare Model vs.¬†KDE Baseline\n\n\nCode\n# Create three maps\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Actual Burglaries\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Model Predictions (Neg. Binomial)\") +\n  theme_crime()\n\np3 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"KDE Baseline Predictions\") +\n  theme_crime()\n\np1 + p2 + p3 +\n  plot_annotation(\n    title = \"Actual vs. Predicted Burglaries\",\n    subtitle = \"Does our complex model outperform simple KDE?\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate performance metrics\ncomparison &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %&gt;%\n  summarize(\n    model_mae = mean(abs(countBurglaries - prediction_nb)),\n    model_rmse = sqrt(mean((countBurglaries - prediction_nb)^2)),\n    kde_mae = mean(abs(countBurglaries - prediction_kde)),\n    kde_rmse = sqrt(mean((countBurglaries - prediction_kde)^2))\n  )\n\ncomparison %&gt;%\n  pivot_longer(everything(), names_to = \"metric\", values_to = \"value\") %&gt;%\n  separate(metric, into = c(\"approach\", \"metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = metric, values_from = value) %&gt;%\n  kable(\n    digits = 2,\n    caption = \"Model Performance Comparison\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nModel Performance Comparison\n\n\napproach\nmae\nrmse\n\n\n\n\nmodel\n2.21\n3.27\n\n\nkde\n2.06\n2.95\n\n\n\n\n\nInterpretation 8.1: Does the complex model outperform the simple KDE baseline? By how much? Is the added complexity worth it?\nThe KDE baseline achieves slightly better predictive accuracy (MAE 2.06 vs.¬†2.21), but this does not mean the regression model is unnecessary or ‚Äúnot worth it.‚Äù The two approaches serve fundamentally different purposes.\nKDE is a purely spatial smoother: it captures clustering extremely well because burglary is highly spatially dependent. Its strength lies in interpolating hotspots, not in explaining why those hotspots exist. In contrast, the regression model provides something KDE cannot: insight into the underlying mechanisms that correlate with burglary patterns. These findings are meaningful because they speak to environmental criminology and ‚Äúbroken windows‚Äù dynamics‚Äîrelationships that KDE, by design, cannot reveal.\nSo while KDE edges out the model in raw predictive accuracy, the regression model is still valuable because it helps us understand the social and spatial processes behind crime, not just map it."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-8.3-where-does-the-model-work-well",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-8.3-where-does-the-model-work-well",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 8.3: Where Does the Model Work Well?",
    "text": "Exercise 8.3: Where Does the Model Work Well?\n\n\nCode\n# Calculate errors\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    error_nb = countBurglaries - prediction_nb,\n    error_kde = countBurglaries - prediction_kde,\n    abs_error_nb = abs(error_nb),\n    abs_error_kde = abs(error_kde)\n  )\n\n# Map errors\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#2166ac\", mid = \"white\", high = \"#b2182b\",\n    midpoint = 0,\n    limits = c(-10, 10)\n  ) +\n  labs(title = \"Model Errors (Actual - Predicted)\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Abs. Error\", option = \"magma\") +\n  labs(title = \"Absolute Model Errors\") +\n  theme_crime()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nInterpretation 8.2: Where does the model make the biggest errors? Are there spatial patterns in the errors? What might this reveal?\nThe largest model errors appear in a few concentrated spatial areas, especially in the strong burglary hotspots, and several zones where burglary levels change sharply from one grid cell to the next.\nThe absolute-error map shows that the largest absolute errors are spatially clustered, not randomly scattered. This suggests the model struggles most in places with very high burglary levels, and places with abrupt local variation, where neighboring cells differ strongly.\nThe model‚Äôs predictors capture broad trends but cannot fully account for the highly localized, uneven nature of burglary hotspots, which leads to systematic errors in those areas."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-9.1-model-summary-table",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-9.1-model-summary-table",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 9.1: Model Summary Table",
    "text": "Exercise 9.1: Model Summary Table\n\n\nCode\n# Create nice summary table\nmodel_summary &lt;- broom::tidy(final_model, exponentiate = TRUE) %&gt;%\n  mutate(\n    across(where(is.numeric), ~round(., 3))\n  )\n\nmodel_summary %&gt;%\n  kable(\n    caption = \"Final Negative Binomial Model Coefficients (Exponentiated)\",\n    col.names = c(\"Variable\", \"Rate Ratio\", \"Std. Error\", \"Z\", \"P-Value\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = \"Rate ratios &gt; 1 indicate positive association with burglary counts.\"\n  )\n\n\n\nFinal Negative Binomial Model Coefficients (Exponentiated)\n\n\nVariable\nRate Ratio\nStd. Error\nZ\nP-Value\n\n\n\n\n(Intercept)\n9.658\n0.062\n36.788\n0.000\n\n\nsanitation_coms\n1.003\n0.002\n1.639\n0.101\n\n\nsanitation_coms.nn\n0.995\n0.000\n-18.930\n0.000\n\n\ndist_to_hotspot\n1.000\n0.000\n-9.359\n0.000\n\n\n\nNote: \n\n\n\n\n\n\n Rate ratios &gt; 1 indicate positive association with burglary counts."
  },
  {
    "objectID": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-9.2-key-findings-summary",
    "href": "Assignments/assignment_4/Jinyang_Xu_Assignment4.html#exercise-9.2-key-findings-summary",
    "title": "Spatial Predictive Analysis",
    "section": "Exercise 9.2: Key Findings Summary",
    "text": "Exercise 9.2: Key Findings Summary\nTechnical Performance:\n\nCross-validation MAE: 2.51\nModel vs.¬†KDE: [KDE performs slightly better (MAE 2.21 vs.¬†2.06)]\nMost predictive variable: [Distance to sanitation complaint clusters (sanitation_coms.nn)]\n\nSpatial Patterns:\n\nBurglaries are [clustered]\nHot spots are located in [South Side, the West Side, and parts of the Near North]\nModel errors show [systematic] patterns\n\nModel Limitations:\n\nOverdispersion: [Yes]\nSpatial autocorrelation in residuals: [Haven‚Äôt tested yet]\nCells with zero counts: [781 / 2458 ( 31.8 %)]"
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Load necessary libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE, tigris_class = \"sf\")\nlibrary(MASS)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(nngeo)\nlibrary(car)\nlibrary(knitr)\nlibrary(readr)\nlibrary(patchwork)\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  \n\n\n1.1 Load and clean Philadelphia sales data:\n\n1.1.1 Load data\n\n\n\nCode\nlibrary(tidyverse)\nopa &lt;- read_csv(\"opa_properties_public1.csv\")\n\n\n\n1.1.2 Filter to residential properties, 2023-2024 sales\n\n\n\nCode\n# data in 2022 will be used as predictor, so keep them as well.\nopa_clean &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# record the amount of data we will focus on\nopa_clean2 &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# Select relevant variables \nopa_var &lt;- opa_clean %&gt;%\n  dplyr::select(\n    sale_date, sale_price, market_value, building_code_description,\n    total_livable_area, number_of_bedrooms, number_of_bathrooms,\n    number_stories, garage_spaces, central_air, quality_grade,\n    interior_condition, exterior_condition, year_built, \n    off_street_open, zip_code, census_tract, zoning, owner_1,\n    category_code_description, shape, fireplaces\n  )\n\n\n\n1.1.3 Remove obvious errors & Handle missing values\n\n\n\nCode\n# ! check before remove NA value\n\ncat(\"&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\")\n\n\n&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\n\n\nCode\n#remove errors and drop rows with small NA counts in specific columns\nopa_var &lt;- opa_var %&gt;% \n  distinct() %&gt;%  #Remove duplicate lines\n  filter(\n    !is.na(total_livable_area) & total_livable_area &gt; 0,\n    !is.na(year_built) & year_built &gt; 0 & year_built &lt; 2025,\n    !is.na(number_of_bathrooms),\n    !is.na(fireplaces),\n    !is.na(interior_condition),\n    garage_spaces&lt;30\n  )   \n\n\n\n1.1.4 Other cleaning decisions\n\n\n\nCode\n#numeric quality_grade\nvalid_grades &lt;- c(\"A+\", \"A\", \"A-\", \n                  \"B+\", \"B\", \"B-\", \n                  \"C+\", \"C\", \"C-\", \n                  \"D+\", \"D\", \"D-\", \n                  \"E+\", \"E\", \"E-\")\n\nopa_var &lt;- opa_var %&gt;%\n  filter(quality_grade %in% valid_grades) %&gt;%\n  mutate(\n    quality_grade = factor(quality_grade, levels = valid_grades, ordered = TRUE),\n    quality_grade_num = rev(seq_along(valid_grades))[as.numeric(quality_grade)]\n  )\n\n#central_air (keep and transform the large NA values)\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    central_air_dummy = case_when(\n      central_air %in% c(1, \"Y\", \"y\") ~ 1,\n      central_air %in% c(0, \"N\", \"n\") ~ 0,\n      TRUE ~ NA_real_\n    ),\n    central_air_missing = if_else(is.na(central_air), 1, 0),\n    central_air_dummy = if_else(is.na(central_air_dummy), 0, central_air_dummy)\n  )\n\n#house age\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    house_age = 2025 - year_built\n  )\n\n\n\n1.1.5 Explore structural data biases and identify non-market transactions\n\n\n\nCode\n# check the relation of Sale Price ~ Market Value\noptions(scipen = 999)\nplot(opa_var$sale_price, opa_var$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value (Predicted)  vs  Sale Price (Actual)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# create a new column to record the identified non-market transactions\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    non_market = \n      (((sale_price &lt; 0.10*market_value) | sale_price &lt; 2000) | (sale_price&gt; 5*market_value))\n  )\n\n\nInterpretation: - The goal of this model is to predict typical, market-driven sale prices. The provided scatter plot of Market Value (Predicted) vs.¬†Sale Price (Actual) is an essential diagnostic tool for assessing data quality. - The red line on the plot represents a perfect 1-to-1 relationship (Y=X), where the property‚Äôs actual sale price is exactly equal to its predicted market value. While most data points cluster tightly around this line‚Äîindicating the ‚ÄúMarket Value‚Äù is a strong predictor‚Äîthe plot clearly reveals two distinct types of extreme outliers that do not represent typical market transactions. 1. Removing Non-Market Sales (The Sale Price &lt; 0.05 * Market Value Rule) - There is a dense cluster of points stacked vertically along the y-axis, where Sale Price (X) is near zero, but Market Value (Y) is high (e.g., $1M, $2M, even $4M). These points represent transactions where a property was ‚Äúsold‚Äù for a price that is a tiny fraction of its assessed worth (e.g., a $2,000,000 house sold for $50,000). - These are non-arm‚Äôs-length transactions, not true market sales. Examples include sales between family members, inheritance transfers, or other legal transactions where the price does not reflect the market. 2. Removing Anomalous High Sales (The Sale Price &gt; 5 * Market Value Rule) - There are several data points scattered far to the right and bottom of the plot, far below the red Y=X line. These points represent properties where the Sale Price (X) was massively higher than its Market Value (Y). For example, a property with an assessed value of $500,000 (Y) selling for $4,000,000 (X). - These are also not typical sales. They could represent (a) data entry errors, (b) sales where the ‚ÄúMarket Value‚Äù figure is obsolete (e.g., a run-down property sold for land value/development potential), or (c) properties that underwent major renovations not yet captured in the assessed value. - Retaining these two groups of outliers would severely skew the model‚Äôs coefficients and reduce its predictive accuracy for the vast majority of normal, market-based transactions. This filtering rule is a necessary step to clean the data, ensure the model learns from valid transactions, and improve its overall reliability.\n\n1.1.6 enhance the sales data\n\nWe have 8000+ non market transactions, that is 1/4 of the total sales data! That‚Äôs too big to let go, The model that we want to generate will become much more stable if we can make use of them.\n\n\nCode\n# filter the REAL MARKET data in the time period we need\nopa_selected &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n  )   \n\n#filter the NON MARKET data in the time period we need\nopa_non_market &lt;- opa_var %&gt;%\n  filter(\n    non_market ==1,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n    )\n\nopa_selected2 &lt;- opa_var\nopa_bonus &lt;- opa_var \n\n\n\n\nCode\n#try to find the relationship between market_value and sale_price\noptions(scipen = 999)\nplot(opa_selected$sale_price, opa_selected$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value vs  Sale Price (cleaned)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# That's quite linear, let's try to build a simple OLS model\nopa_mdata &lt;- opa_bonus %&gt;%\n  filter(\n    non_market == 0\n    )\n\nmodel_non &lt;- lm(sale_price ~ market_value, data = opa_mdata)\nsummary(model_non)\n\n\n\nCall:\nlm(formula = sale_price ~ market_value, data = opa_mdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1432832   -35207    -1669    30014  1975714 \n\nCoefficients:\n                 Estimate   Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  -9189.286424   663.771782  -13.84 &lt;0.0000000000000002 ***\nmarket_value     1.027924     0.001773  579.62 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 86310 on 40927 degrees of freedom\nMultiple R-squared:  0.8914,    Adjusted R-squared:  0.8914 \nF-statistic: 3.36e+05 on 1 and 40927 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\npred &lt;- predict(model_non, newdata = opa_mdata)\nresid &lt;- opa_mdata$sale_price - pred\n\n# RMSE\nrmse_value &lt;- sqrt(mean(resid^2, na.rm = TRUE))\nrmse_value\n\n\n[1] 86311.29\n\n\nThe results demonstrate a very strong linear relationship between sale_price and market_value. The market_value variable in the dataset effectively predicts the sale_price. Therefore, we can leverage this relationship to estimate the normal market prices for non-market transactions. We record these estimated values as sale_price_predicted. By doing this, we enhance our data!\n\n\nCode\n#bring data back\nopa_non_market$sale_price_predicted &lt;- predict(model_non, newdata = opa_non_market)\n\n#join back to the main data\nopa_selected &lt;- opa_selected %&gt;%\n  mutate(sale_price_predicted= sale_price)\n\nset.seed(123)\nopa_bind &lt;- bind_rows(opa_selected, opa_non_market) %&gt;%\n  slice_sample(prop = 1)\n\n\n1.2 Load and clean secondary dataset:\n\n1.2.1 Census data (tidycensus):\n\n\n\nCode\n# Transform to sf object \nopa_bind &lt;- st_as_sf(opa_bind, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(4326)\nopa_sf&lt;- opa_bind\n\n\n\n\nCode\n# Load Census data for Philadelphia tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    \n    ba_degree = \"B15003_022\",\n    total_edu = \"B15003_001\",\n    \n    median_income = \"B19013_001\",\n   \n    labor_force = \"B23025_003\",\n    unemployed = \"B23025_005\",\n    \n    total_housing = \"B25002_001\",\n    vacant_housing = \"B25002_003\"\n  ),\n  year = 2023,\n  state = \"PA\",\n  county = \"Philadelphia\",\n  geometry = TRUE\n) %&gt;%\n  dplyr::select(GEOID, variable, estimate, geometry) %&gt;%   \n  tidyr::pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  dplyr::mutate(\n    ba_rate = 100 * ba_degree / total_edu,\n    unemployment_rate = 100 * unemployed / labor_force,\n    vacancy_rate = 100 * vacant_housing / total_housing\n  ) %&gt;%\n  st_transform(st_crs(opa_sf))\n\n# Spatial join of OPA data with Census data\nopa_census &lt;- st_join(opa_sf, philly_census, join = st_within) %&gt;%\n  filter(!is.na(median_income))\n\n\n\n1.2.2 Spatial amenities (OpenDataPhilly)\n\n\n\nCode\n#load crime,poi,transit,hospital\nopa_census &lt;- st_transform(opa_census, 3857)\n\ncrime &lt;- read_csv(\"crime_sel.csv\") %&gt;% \n  filter(!is.na(lat) & !is.na(lng)) \ncrime_sf &lt;- st_as_sf(crime, coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census)) \n\npoi_sf &lt;- st_read(\"data/gis_osm_pois_a_free_1.shp\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census)) \n\nTransit &lt;- read_csv(\"Transit.csv\")\ntransit_sf &lt;- st_as_sf(Transit, coords = c(\"Lon\", \"Lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census))\n\nhospital_sf &lt;- st_read(\"hospitals.geojson\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census))\n\n\n1.3 Summary tables showing before/after dimensions\n\n\nCode\n# Original data dimensions\nopa_dims &lt;- tibble(\n  dataset = \"raw CSV\",\n  rows = nrow(opa),\n  columns = ncol(opa)\n)\n\n# cleaned data dimensions\nopa_filter_dims &lt;- tibble(\n  dataset = \"after fixed criteria\",\n  rows = nrow(opa_clean2),\n  columns = ncol(opa_clean2)\n)\n\nopa_selected_dims &lt;- tibble(\n  dataset = \"after cleaned\",\n  rows = nrow(opa_bind),\n  columns = ncol(opa_bind)\n)\n# data dimensions (within census tracts)\nopa_census_dims &lt;- tibble(\n  dataset = \"after census joined\",\n  rows = nrow(opa_census),\n  columns = ncol(opa_census)\n)\n\n# create summary table\nsummary_table &lt;- bind_rows(opa_dims, opa_filter_dims,opa_selected_dims, opa_census_dims)\nlibrary(knitr)\nsummary_table %&gt;%\n  kable(caption = \"Summary of OPA data before and after cleaning\")\n\n\n\nSummary of OPA data before and after cleaning\n\n\ndataset\nrows\ncolumns\n\n\n\n\nraw CSV\n153267\n79\n\n\nafter fixed criteria\n34559\n79\n\n\nafter cleaned\n31968\n28\n\n\nafter census joined\n31613\n40\n\n\n\n\n\nPrinciples of Data Processing\n\nopa Data\n\nFilter sale_date between 2023-01-01 and 2024-12-31.\n\nKeep only residential properties (category_code == 1).\n\nRemove records with missing values in total_livable_area, sale_price, or number_of_bathrooms.\n\nFilter records year_built &gt; 0 .\n\nCensus data\n\nLoad data including total_pop, ba_degree, total_edu, median_income, labor_force, unemployed, total_housing, vacant_housing.\nTransform to spatial format and remove records with missing values.\n\nSpatial amenities\n\nLoad datasets Transit, crime, POIs, Hospitals.\nTransform to spatial format and remove records with missing values.\n\n\nInterpretation: The selected variables can be grouped into several meaningful categories that are theoretically and empirically linked to housing prices:\n- Neighborhood Safety: - Crime data: Areas with lower crime rates are generally more desirable, leading to higher property values. Including crime incidents helps capture the effect of public safety on housing prices. - Accessibility and Transportation: - Transit points: Proximity to public transportation (e.g., bus stops, subway stations) increases accessibility and convenience, which is often capitalized into higher home values. - Points of Interest (POIs): Nearby amenities such as shops, restaurants, and parks improve quality of life and can positively influence housing prices. - Healthcare Access: - Hospitals: Easy access to healthcare facilities is a valued neighborhood characteristic, especially for families and older residents, and can contribute to higher property values. - Socioeconomic and Demographic Context (from Census data): - Total population: Indicates population density, which can affect demand for housing. - Educational attainment (e.g., % with BA degree): Higher education levels are often correlated with higher income and neighborhood desirability. - Median income: Directly influences purchasing power and demand for housing in an area. - Employment status (labor force and unemployment): Reflects economic stability and local job market health, which affect housing demand. - Housing market conditions (total and vacant housing): Vacancy rates can signal neighborhood decline or oversupply, both of which impact prices. - Together, these variables provide a multidimensional view of a neighborhood‚Äôs appeal, safety, convenience, and economic health‚Äîall key determinants of housing prices."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-1-data-preparation",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-1-data-preparation",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Load necessary libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE, tigris_class = \"sf\")\nlibrary(MASS)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(nngeo)\nlibrary(car)\nlibrary(knitr)\nlibrary(readr)\nlibrary(patchwork)\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  \n\n\n1.1 Load and clean Philadelphia sales data:\n\n1.1.1 Load data\n\n\n\nCode\nlibrary(tidyverse)\nopa &lt;- read_csv(\"opa_properties_public1.csv\")\n\n\n\n1.1.2 Filter to residential properties, 2023-2024 sales\n\n\n\nCode\n# data in 2022 will be used as predictor, so keep them as well.\nopa_clean &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# record the amount of data we will focus on\nopa_clean2 &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# Select relevant variables \nopa_var &lt;- opa_clean %&gt;%\n  dplyr::select(\n    sale_date, sale_price, market_value, building_code_description,\n    total_livable_area, number_of_bedrooms, number_of_bathrooms,\n    number_stories, garage_spaces, central_air, quality_grade,\n    interior_condition, exterior_condition, year_built, \n    off_street_open, zip_code, census_tract, zoning, owner_1,\n    category_code_description, shape, fireplaces\n  )\n\n\n\n1.1.3 Remove obvious errors & Handle missing values\n\n\n\nCode\n# ! check before remove NA value\n\ncat(\"&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\")\n\n\n&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\n\n\nCode\n#remove errors and drop rows with small NA counts in specific columns\nopa_var &lt;- opa_var %&gt;% \n  distinct() %&gt;%  #Remove duplicate lines\n  filter(\n    !is.na(total_livable_area) & total_livable_area &gt; 0,\n    !is.na(year_built) & year_built &gt; 0 & year_built &lt; 2025,\n    !is.na(number_of_bathrooms),\n    !is.na(fireplaces),\n    !is.na(interior_condition),\n    garage_spaces&lt;30\n  )   \n\n\n\n1.1.4 Other cleaning decisions\n\n\n\nCode\n#numeric quality_grade\nvalid_grades &lt;- c(\"A+\", \"A\", \"A-\", \n                  \"B+\", \"B\", \"B-\", \n                  \"C+\", \"C\", \"C-\", \n                  \"D+\", \"D\", \"D-\", \n                  \"E+\", \"E\", \"E-\")\n\nopa_var &lt;- opa_var %&gt;%\n  filter(quality_grade %in% valid_grades) %&gt;%\n  mutate(\n    quality_grade = factor(quality_grade, levels = valid_grades, ordered = TRUE),\n    quality_grade_num = rev(seq_along(valid_grades))[as.numeric(quality_grade)]\n  )\n\n#central_air (keep and transform the large NA values)\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    central_air_dummy = case_when(\n      central_air %in% c(1, \"Y\", \"y\") ~ 1,\n      central_air %in% c(0, \"N\", \"n\") ~ 0,\n      TRUE ~ NA_real_\n    ),\n    central_air_missing = if_else(is.na(central_air), 1, 0),\n    central_air_dummy = if_else(is.na(central_air_dummy), 0, central_air_dummy)\n  )\n\n#house age\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    house_age = 2025 - year_built\n  )\n\n\n\n1.1.5 Explore structural data biases and identify non-market transactions\n\n\n\nCode\n# check the relation of Sale Price ~ Market Value\noptions(scipen = 999)\nplot(opa_var$sale_price, opa_var$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value (Predicted)  vs  Sale Price (Actual)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# create a new column to record the identified non-market transactions\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    non_market = \n      (((sale_price &lt; 0.10*market_value) | sale_price &lt; 2000) | (sale_price&gt; 5*market_value))\n  )\n\n\nInterpretation: - The goal of this model is to predict typical, market-driven sale prices. The provided scatter plot of Market Value (Predicted) vs.¬†Sale Price (Actual) is an essential diagnostic tool for assessing data quality. - The red line on the plot represents a perfect 1-to-1 relationship (Y=X), where the property‚Äôs actual sale price is exactly equal to its predicted market value. While most data points cluster tightly around this line‚Äîindicating the ‚ÄúMarket Value‚Äù is a strong predictor‚Äîthe plot clearly reveals two distinct types of extreme outliers that do not represent typical market transactions. 1. Removing Non-Market Sales (The Sale Price &lt; 0.05 * Market Value Rule) - There is a dense cluster of points stacked vertically along the y-axis, where Sale Price (X) is near zero, but Market Value (Y) is high (e.g., $1M, $2M, even $4M). These points represent transactions where a property was ‚Äúsold‚Äù for a price that is a tiny fraction of its assessed worth (e.g., a $2,000,000 house sold for $50,000). - These are non-arm‚Äôs-length transactions, not true market sales. Examples include sales between family members, inheritance transfers, or other legal transactions where the price does not reflect the market. 2. Removing Anomalous High Sales (The Sale Price &gt; 5 * Market Value Rule) - There are several data points scattered far to the right and bottom of the plot, far below the red Y=X line. These points represent properties where the Sale Price (X) was massively higher than its Market Value (Y). For example, a property with an assessed value of $500,000 (Y) selling for $4,000,000 (X). - These are also not typical sales. They could represent (a) data entry errors, (b) sales where the ‚ÄúMarket Value‚Äù figure is obsolete (e.g., a run-down property sold for land value/development potential), or (c) properties that underwent major renovations not yet captured in the assessed value. - Retaining these two groups of outliers would severely skew the model‚Äôs coefficients and reduce its predictive accuracy for the vast majority of normal, market-based transactions. This filtering rule is a necessary step to clean the data, ensure the model learns from valid transactions, and improve its overall reliability.\n\n1.1.6 enhance the sales data\n\nWe have 8000+ non market transactions, that is 1/4 of the total sales data! That‚Äôs too big to let go, The model that we want to generate will become much more stable if we can make use of them.\n\n\nCode\n# filter the REAL MARKET data in the time period we need\nopa_selected &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n  )   \n\n#filter the NON MARKET data in the time period we need\nopa_non_market &lt;- opa_var %&gt;%\n  filter(\n    non_market ==1,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n    )\n\nopa_selected2 &lt;- opa_var\nopa_bonus &lt;- opa_var \n\n\n\n\nCode\n#try to find the relationship between market_value and sale_price\noptions(scipen = 999)\nplot(opa_selected$sale_price, opa_selected$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value vs  Sale Price (cleaned)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# That's quite linear, let's try to build a simple OLS model\nopa_mdata &lt;- opa_bonus %&gt;%\n  filter(\n    non_market == 0\n    )\n\nmodel_non &lt;- lm(sale_price ~ market_value, data = opa_mdata)\nsummary(model_non)\n\n\n\nCall:\nlm(formula = sale_price ~ market_value, data = opa_mdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1432832   -35207    -1669    30014  1975714 \n\nCoefficients:\n                 Estimate   Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  -9189.286424   663.771782  -13.84 &lt;0.0000000000000002 ***\nmarket_value     1.027924     0.001773  579.62 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 86310 on 40927 degrees of freedom\nMultiple R-squared:  0.8914,    Adjusted R-squared:  0.8914 \nF-statistic: 3.36e+05 on 1 and 40927 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\npred &lt;- predict(model_non, newdata = opa_mdata)\nresid &lt;- opa_mdata$sale_price - pred\n\n# RMSE\nrmse_value &lt;- sqrt(mean(resid^2, na.rm = TRUE))\nrmse_value\n\n\n[1] 86311.29\n\n\nThe results demonstrate a very strong linear relationship between sale_price and market_value. The market_value variable in the dataset effectively predicts the sale_price. Therefore, we can leverage this relationship to estimate the normal market prices for non-market transactions. We record these estimated values as sale_price_predicted. By doing this, we enhance our data!\n\n\nCode\n#bring data back\nopa_non_market$sale_price_predicted &lt;- predict(model_non, newdata = opa_non_market)\n\n#join back to the main data\nopa_selected &lt;- opa_selected %&gt;%\n  mutate(sale_price_predicted= sale_price)\n\nset.seed(123)\nopa_bind &lt;- bind_rows(opa_selected, opa_non_market) %&gt;%\n  slice_sample(prop = 1)\n\n\n1.2 Load and clean secondary dataset:\n\n1.2.1 Census data (tidycensus):\n\n\n\nCode\n# Transform to sf object \nopa_bind &lt;- st_as_sf(opa_bind, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(4326)\nopa_sf&lt;- opa_bind\n\n\n\n\nCode\n# Load Census data for Philadelphia tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    \n    ba_degree = \"B15003_022\",\n    total_edu = \"B15003_001\",\n    \n    median_income = \"B19013_001\",\n   \n    labor_force = \"B23025_003\",\n    unemployed = \"B23025_005\",\n    \n    total_housing = \"B25002_001\",\n    vacant_housing = \"B25002_003\"\n  ),\n  year = 2023,\n  state = \"PA\",\n  county = \"Philadelphia\",\n  geometry = TRUE\n) %&gt;%\n  dplyr::select(GEOID, variable, estimate, geometry) %&gt;%   \n  tidyr::pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  dplyr::mutate(\n    ba_rate = 100 * ba_degree / total_edu,\n    unemployment_rate = 100 * unemployed / labor_force,\n    vacancy_rate = 100 * vacant_housing / total_housing\n  ) %&gt;%\n  st_transform(st_crs(opa_sf))\n\n# Spatial join of OPA data with Census data\nopa_census &lt;- st_join(opa_sf, philly_census, join = st_within) %&gt;%\n  filter(!is.na(median_income))\n\n\n\n1.2.2 Spatial amenities (OpenDataPhilly)\n\n\n\nCode\n#load crime,poi,transit,hospital\nopa_census &lt;- st_transform(opa_census, 3857)\n\ncrime &lt;- read_csv(\"crime_sel.csv\") %&gt;% \n  filter(!is.na(lat) & !is.na(lng)) \ncrime_sf &lt;- st_as_sf(crime, coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census)) \n\npoi_sf &lt;- st_read(\"data/gis_osm_pois_a_free_1.shp\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census)) \n\nTransit &lt;- read_csv(\"Transit.csv\")\ntransit_sf &lt;- st_as_sf(Transit, coords = c(\"Lon\", \"Lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census))\n\nhospital_sf &lt;- st_read(\"hospitals.geojson\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census))\n\n\n1.3 Summary tables showing before/after dimensions\n\n\nCode\n# Original data dimensions\nopa_dims &lt;- tibble(\n  dataset = \"raw CSV\",\n  rows = nrow(opa),\n  columns = ncol(opa)\n)\n\n# cleaned data dimensions\nopa_filter_dims &lt;- tibble(\n  dataset = \"after fixed criteria\",\n  rows = nrow(opa_clean2),\n  columns = ncol(opa_clean2)\n)\n\nopa_selected_dims &lt;- tibble(\n  dataset = \"after cleaned\",\n  rows = nrow(opa_bind),\n  columns = ncol(opa_bind)\n)\n# data dimensions (within census tracts)\nopa_census_dims &lt;- tibble(\n  dataset = \"after census joined\",\n  rows = nrow(opa_census),\n  columns = ncol(opa_census)\n)\n\n# create summary table\nsummary_table &lt;- bind_rows(opa_dims, opa_filter_dims,opa_selected_dims, opa_census_dims)\nlibrary(knitr)\nsummary_table %&gt;%\n  kable(caption = \"Summary of OPA data before and after cleaning\")\n\n\n\nSummary of OPA data before and after cleaning\n\n\ndataset\nrows\ncolumns\n\n\n\n\nraw CSV\n153267\n79\n\n\nafter fixed criteria\n34559\n79\n\n\nafter cleaned\n31968\n28\n\n\nafter census joined\n31613\n40\n\n\n\n\n\nPrinciples of Data Processing\n\nopa Data\n\nFilter sale_date between 2023-01-01 and 2024-12-31.\n\nKeep only residential properties (category_code == 1).\n\nRemove records with missing values in total_livable_area, sale_price, or number_of_bathrooms.\n\nFilter records year_built &gt; 0 .\n\nCensus data\n\nLoad data including total_pop, ba_degree, total_edu, median_income, labor_force, unemployed, total_housing, vacant_housing.\nTransform to spatial format and remove records with missing values.\n\nSpatial amenities\n\nLoad datasets Transit, crime, POIs, Hospitals.\nTransform to spatial format and remove records with missing values.\n\n\nInterpretation: The selected variables can be grouped into several meaningful categories that are theoretically and empirically linked to housing prices:\n- Neighborhood Safety: - Crime data: Areas with lower crime rates are generally more desirable, leading to higher property values. Including crime incidents helps capture the effect of public safety on housing prices. - Accessibility and Transportation: - Transit points: Proximity to public transportation (e.g., bus stops, subway stations) increases accessibility and convenience, which is often capitalized into higher home values. - Points of Interest (POIs): Nearby amenities such as shops, restaurants, and parks improve quality of life and can positively influence housing prices. - Healthcare Access: - Hospitals: Easy access to healthcare facilities is a valued neighborhood characteristic, especially for families and older residents, and can contribute to higher property values. - Socioeconomic and Demographic Context (from Census data): - Total population: Indicates population density, which can affect demand for housing. - Educational attainment (e.g., % with BA degree): Higher education levels are often correlated with higher income and neighborhood desirability. - Median income: Directly influences purchasing power and demand for housing in an area. - Employment status (labor force and unemployment): Reflects economic stability and local job market health, which affect housing demand. - Housing market conditions (total and vacant housing): Vacancy rates can signal neighborhood decline or oversupply, both of which impact prices. - Together, these variables provide a multidimensional view of a neighborhood‚Äôs appeal, safety, convenience, and economic health‚Äîall key determinants of housing prices."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-2-feature-engineering",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-2-feature-engineering",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 2: Feature Engineering",
    "text": "Phase 2: Feature Engineering\n2.1 Buffer-based features:\n\n2.1.1 neighborhood avg sale price in the past year\n\n\n\nCode\n#filter the past sales data\n\nopa_census &lt;- opa_census %&gt;%\n  mutate(sale_id = row_number())\n\n\n\nopa_past &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2022-12-31\"\n  ) \n\nopa_past &lt;- opa_past %&gt;%\n  mutate(sale_id2 = row_number())\n\nopa_past &lt;- st_as_sf(opa_past, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(3857)\n\nopa_census &lt;- st_transform(opa_census, 3857)\nopa_past   &lt;- st_transform(opa_past, 3857)\n\nopa_census_buffer &lt;- st_buffer(opa_census, 300)\ndrop_cols &lt;- names(opa_census_buffer) %in% c(\"sale_price\", \"total_livable_area\",\n                                             \"sale_price.y\", \"total_livable_area.y\")\nopa_census_buffer &lt;- opa_census_buffer[ , !drop_cols, drop = FALSE]\n\njoin_result &lt;- st_join(\n  opa_census_buffer,\n  opa_past,\n  join = st_intersects,\n  left = TRUE\n)\n\n\njoin_result &lt;- st_join(\n  opa_census_buffer,\n  opa_past,\n  join = st_intersects,\n  left = TRUE\n)\njoin_dedup &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  distinct(sale_id, sale_id2, .keep_all = TRUE)\n\nopa_summary &lt;- join_dedup %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(\n    past_count = sum(!is.na(sale_id2)),\n    avg_past_price_density = ifelse(\n      sum(!is.na(total_livable_area)) == 0, NA_real_,\n      sum(sale_price, na.rm = TRUE) / sum(total_livable_area, na.rm = TRUE)\n    ),\n    .groups = \"drop\"\n  )\nopa_census &lt;- opa_census %&gt;%\n  left_join(opa_summary, by = \"sale_id\")\n\n\n\n2.1.2 crime numbers\n\n\n\nCode\nradius_cri &lt;- 250 \n\nopa_census$crime_count &lt;- lengths(st_is_within_distance(opa_census, crime_sf, dist = radius_cri)) \n\n\n\n2.1.3 POI numbers\n\n\n\nCode\nopa_census_m &lt;- st_transform(opa_census, 3857)\npoi_sf_m     &lt;- st_transform(poi_sf, 3857)\n\n\nradius_poi &lt;- 400\nopa_census_buffer &lt;- st_buffer(opa_census_m, radius_poi)\n\njoin_result &lt;- st_join(opa_census_buffer, poi_sf_m, join = st_intersects, left = TRUE)\n\npoi_summary &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(poi_count = sum(!is.na(osm_id)))  \n\nopa_census &lt;- opa_census_m %&gt;%\n  left_join(poi_summary, by = \"sale_id\")\n\n\n\n2.1.4 Transit numbers\n\n\n\nCode\nopa_census_m &lt;- st_transform(opa_census, 3857)\ntransit_sf_m &lt;- st_transform(transit_sf, 3857)\n\nradius_ts &lt;- 400\nopa_census_buffer &lt;- st_buffer(opa_census_m, radius_ts)\n\njoin_result &lt;- st_join(opa_census_buffer, transit_sf_m, join = st_intersects, left = TRUE)\n\ntransit_summary &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(transit_count = sum(!is.na(FID)))  \n\nopa_census &lt;- opa_census_m %&gt;%\n  left_join(transit_summary, by = \"sale_id\")\n\n\n2.2 k-Nearest Neighbor features:\n\n2.2.1 Hospitals (KNN-3)\n\n\n\nCode\nnearest_hospital_index &lt;- st_nn(\n  opa_census,\n  hospital_sf,\n  k = 3,            \n  returnDist = TRUE \n)\n\nopa_census$nearest_hospital_knn3 &lt;- sapply(nearest_hospital_index$dist, mean)\n\n\n2.3 Weights:\n\n\nCode\n# add different weights to actual and non market transactions\n\nopa_census_all &lt;- opa_census\n\nnon_market_share&lt;- mean(opa_census_all$non_market == 1, na.rm = TRUE)\nnon_market_share\n\n\n[1] 0.261791\n\n\nCode\nopa_census_all &lt;- opa_census %&gt;%\n  mutate(weight_mix = ifelse(non_market == 1, non_market_share, 1))\n\n\n2.4 Transformation:\n\n\nCode\n#standardize houseage and median income\nmean_age_all &lt;- mean(opa_census_all$house_age, na.rm = TRUE)\nmean_income_log &lt;- mean(log(opa_census_all$median_income), na.rm = TRUE)\n\n# centralize\nopa_census_all &lt;- opa_census_all %&gt;%\n  mutate(\n    house_age_c  = house_age - mean_age_all,\n    house_age_c2 = house_age_c^2,\n    income_log   = log(median_income),\n    income_scaled = income_log - mean_income_log\n  )\n\nopa_census_2&lt;- opa_census_all %&gt;%\n  filter(\n    non_market==0\n  )"
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-3-exploratory-data-analysis",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-3-exploratory-data-analysis",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 3: Exploratory Data Analysis",
    "text": "Phase 3: Exploratory Data Analysis\n\n3.1 Distribution of sale prices (histogram)\n\n\nCode\nggplot(opa_census_all, aes(x = sale_price_predicted)) +\n  geom_histogram(\n    bins = 50,                 # Ë∞ÉÊï¥ÂàÜÁÆ±Êï∞Èáè\n    fill = \"#6A1B9A\",          # Êü±Â≠êÈ¢úËâ≤\n    color = \"white\",           # ËæπÊ°ÜÈ¢úËâ≤\n    alpha = 0.8                # ÈÄèÊòéÂ∫¶\n  ) +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Distribution of  Sale Prices\",\n    x = \"Predicted Sale Price (USD)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(opa_census_all, aes(x = log(sale_price_predicted))) +\n  geom_histogram(\n    bins = 50,                 # Ë∞ÉÊï¥ÂàÜÁÆ±Êï∞Èáè\n    fill = \"#6A1B9A\",          # Êü±Â≠êÈ¢úËâ≤\n    color = \"white\",           # ËæπÊ°ÜÈ¢úËâ≤\n    alpha = 0.8                # ÈÄèÊòéÂ∫¶\n  ) +\n  scale_x_continuous() +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Distribution of  Log(Sale Prices)\",\n    x = \"Log(Predicted Sale Price) \",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\nKey Findings: - Highly Right-Skewed: The bulk of the distribution is concentrated on the left side (low price range), while the right tail is very long, extending towards higher prices. This means that the majority of houses have lower prices, and very expensive houses are very few in number.\n- Concentration and Outliers: The count of houses in each bin drops rapidly as the price increases. The number of houses above $2,500,000 is very small, indicating the presence of extreme high-price outliers.\n- Preprocessing Requirement: This distribution strongly suggests that before building a house price prediction model, the Sale Price variable will need a transformation, most commonly a log transformation, to make the distribution more closely resemble a normal distribution.\n\n\n3.2 Spatial distribution of sale prices (map)\n\n\nCode\nopa_census_all &lt;- opa_census_all %&gt;%\n  mutate(price_quartile = ntile(sale_price_predicted, 4))\n\nggplot() +\n  geom_sf(data = philly_census, fill = \"lightgrey\", color = \"white\") +\n  geom_sf(data = opa_census_all, aes(color = factor(price_quartile)), size = 0.5, alpha = 0.7) +\n  scale_color_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    labels = c(\"0%-25%\", \"25%-50%\", \"50%-75%\", \"75%-100%\")\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Housing Sales Price in Philadelphia (2023‚Äì2024)\",\n    color = \"Sale Price Quartile\"\n  )  +\n  guides(color = guide_legend(override.aes = list(size = 3)))\n\n\n\n\n\n\n\n\n\nKey Findings:\n- Spatial Concentration of Housing Prices: Highest Prices are clearly concentrated in the Center City/Downtown area of Philadelphia and its immediate surroundings, which indicates that the most expensive property transactions occur in the high-value areas of and around the city center.\n- Discontinuous Price Transitions: Housing prices do not exhibit smooth gradients across the city; instead, they show abrupt changes between different price quartiles, forming distinct spatial clusters. This suggests that price variations are influenced by fixed boundaries such as neighborhoods, infrastructure, or socio-economic factors, rather than continuous spatial diffusion. - Peripheral Price Patterns: Lower-price quartiles (0%-25% and 25%-50%) are predominantly located in the outer regions of the city, particularly in the northern and southern peripheries, indicating a clear core-periphery divide in housing values.\n\n\n3.3 Price vs.¬†structural features (scatter plots)\n\n\nCode\nopa_census_plot &lt;- opa_census_all %&gt;%\n  mutate(log_sale_price = log(sale_price_predicted))\n\n# 1Ô∏è‚É£ total_livable_area\np1 &lt;- ggplot(opa_census_plot, aes(x = log(total_livable_area), y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Log(Livable Area)\",\n    x = \"Log(Total Livable Area)\",\n    y = \"Log(Sale Price)\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 2Ô∏è‚É£ number_of_bathrooms\np2 &lt;- ggplot(opa_census_plot, aes(x = number_of_bathrooms, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Bathrooms\",\n    x = \"Number of Bathrooms\",  \n    y = \"\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 3Ô∏è‚É£ interior_condition\np3 &lt;- ggplot(opa_census_plot, aes(x = interior_condition, y = log_sale_price)) +\n  geom_jitter(alpha = 0.5, color = \"#6A1B9A\", width = 0.2, height = 0) +  \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Interior Condition\",\n    x = \"Interior Condition\",\n    y = \"Log(Sale Price)\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 4Ô∏è‚É£ house_age\np4 &lt;- ggplot(opa_census_plot, aes(x = house_age^2, y = log_sale_price)) + \n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. House Age¬≤\",\n    x = \"House Age¬≤ (2025 - Year Built)¬≤\",\n    y = \"\"\n  ) +\n  theme_minimal(base_size = 10)\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\nquarto check Key Findings:\n- Strong Positive Correlation with Size and Bathrooms: There is a clear positive linear relationship between the log of sale price and both the log of total livable area and the number of bathrooms. This indicates that larger properties and those with more bathrooms command significantly higher market prices. - Non-Linear Relationship with Interior Condition: While a general positive trend exists, the relationship between log sale price and interior condition rating is not perfectly linear. The data dispersion suggests that the effect of interior condition on price may be subject to diminishing marginal returns or other non-linear dynamics. - Negative Correlation with House Age Squared: A significant negative relationship is observed between log sale price and the squared term of house age. This indicates that property values depreciate as homes get older, and this depreciation effect may accelerate over time, reflecting a non-linear aging effect on housing value.\n\n\n3.4 Price vs.¬†spatial & Social features (scatter plots)\n\n\nCode\nopa_census_plot &lt;- opa_census_all %&gt;%\n  mutate(\n    log_sale_price = log(sale_price_predicted),\n    sqrt_crime_count = sqrt(crime_count)\n  )\n\np1 &lt;- ggplot(opa_census_plot, aes(x = ba_rate, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  labs(title = \"Log(Sale Price) vs. BA Rate\",\n       x = \"Bachelor's Degree Rate\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np2 &lt;- ggplot(opa_census_plot, aes(x = unemployment_rate, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  labs(title = \"Log(Sale Price) vs. Unemployment Rate\",\n       x = \"Unemployment Rate\", y = \"\") +\n  theme_minimal(base_size = 10)\n\np3 &lt;- ggplot(opa_census_plot, aes(x = median_income, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = dollar_format()) +\n  labs(title = \"Log(Sale Price) vs. Median Income\",\n       x = \"Median Household Income (USD)\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np4 &lt;- ggplot(opa_census_plot, aes(x = avg_past_price_density, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. Past Price Density\",\n       x = \"Average Past Price Density\", y = \"\") +\n  theme_minimal(base_size = 10)\n\np5 &lt;- ggplot(opa_census_plot, aes(x = sqrt_crime_count, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. ‚àö(Crime Count)\",\n       x = \"Square Root of Crime Count\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np6 &lt;- ggplot(opa_census_plot, aes(x = transit_count, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. Transit Count\",\n       x = \"Number of Transit Stops Nearby\", y = \"\") +\n  theme_minimal(base_size = 10)\n\n(p1 | p2 | p3) / (p4 | p5 | p6)\n\n\n\n\n\n\n\n\n\nKey Findings:\n- Strong Socioeconomic Influence: Housing prices show clear positive correlations with key socioeconomic indicators. Both bachelor‚Äôs degree rate and median household income exhibit strong positive relationships with sale prices, indicating that neighborhoods with higher educational attainment and income levels command substantially higher property values. - Negative Impact of Crime and Unemployment: There are evident negative relationships between housing prices and both crime levels (measured by square root of crime count) and unemployment rates. This demonstrates that public safety and local economic vitality are significant determinants of property values in Philadelphia. - Positive Effects of Historical Prices and Transit Access: Sale prices maintain a positive relationship with both historical price density and accessibility to public transportation. This suggests that areas with established high-value characteristics and better transit infrastructure maintain their premium in the housing market, reflecting path dependence in neighborhood valuation and the value of transportation accessibility.\n\n\nOther visualization\n\n\nCode\ntract_price &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(avg_past_price_density = mean(avg_past_price_density, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_price, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = avg_past_price_density), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"white\", high = \"#6A1B9A\",\n    name = \"Mean Sale Price Per sqft\",\n    labels = scales::dollar_format(),\n    na.value = \"grey60\"\n  ) +\n  scale_alpha(range = c(0.2, 0.8), name = \"Interior Condition\") +\n  \n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Buffered Area Mean Sold Price of 2022\",\n    subtitle = \"clustered in census tracts\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ntract_condition &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_interior_condition = mean(interior_condition, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_condition, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_interior_condition), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"#6A1B9A\", high = \"white\",      # ÊµÖÊ©ô‚ÜíÊ∑±Á∫¢ÔºåÊõ¥Áõ¥ËßÇË°®Áé∞È´ò‰Ωé\n    name = \"Avg Interior Condition\",\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Average Interior Condition by Census Tract\",\n    subtitle = \"Darker color = better average condition\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ntract_area &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_total_livable_area = mean(total_livable_area, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_area, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_total_livable_area), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"white\", high = \"#6A1B9A\",   \n    name = \"Avg Livable Area (sqft)\",\n    labels = scales::comma_format(),\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Average Livable Area by Census Tract\",\n    subtitle = \"Darker color indicates larger average livable area\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\nKey Findings:\n- Spatial Variation in Property Values: The average sale price per square foot shows significant geographic clustering across census tracts, with distinct high-value areas concentrated in specific neighborhoods. This indicates strong spatial autocorrelation in housing prices, where adjacent tracts tend to have similar price levels. - Correlation Between Property Condition and Location: Better average interior conditions are systematically concentrated in particular geographic areas, suggesting that housing maintenance and quality are not randomly distributed but follow spatial patterns that may correlate with neighborhood characteristics and property values. - Heterogeneous Distribution of Housing Size: The average livable area varies substantially across census tracts, with larger properties clustered in specific regions. This spatial patterning of housing size complements the price distribution, indicating that both property characteristics and location factors contribute to the overall housing market structure in Philadelphia."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-4-model-building",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-4-model-building",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 4: Model Building",
    "text": "Phase 4: Model Building\n\nBuild models progressively\n4.1 Structural features only:\n\n\nCode\nmodel_1 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +  #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing, \n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_1)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing, data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-3.2930 -0.2144  0.0510  0.2911  2.4585 \n\nCoefficients:\n                            Estimate   Std. Error t value            Pr(&gt;|t|)\n(Intercept)              6.435857858  0.077629460  82.905 &lt;0.0000000000000002\nlog(total_livable_area)  0.744348571  0.010689344  69.635 &lt;0.0000000000000002\nnumber_of_bathrooms      0.051667122  0.005580674   9.258 &lt;0.0000000000000002\nhouse_age_c              0.000002273  0.000118374   0.019               0.985\nhouse_age_c2             0.000046457  0.000001746  26.607 &lt;0.0000000000000002\ninterior_condition      -0.112636239  0.004358818 -25.841 &lt;0.0000000000000002\nquality_grade_num        0.069613279  0.002849720  24.428 &lt;0.0000000000000002\nfireplaces               0.116405937  0.010884143  10.695 &lt;0.0000000000000002\ngarage_spaces            0.140429585  0.006549989  21.440 &lt;0.0000000000000002\ncentral_air_dummy        0.458743112  0.008036173  57.085 &lt;0.0000000000000002\ncentral_air_missing     -0.237637667  0.008809496 -26.975 &lt;0.0000000000000002\n                           \n(Intercept)             ***\nlog(total_livable_area) ***\nnumber_of_bathrooms     ***\nhouse_age_c                \nhouse_age_c2            ***\ninterior_condition      ***\nquality_grade_num       ***\nfireplaces              ***\ngarage_spaces           ***\ncentral_air_dummy       ***\ncentral_air_missing     ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4911 on 31602 degrees of freedom\nMultiple R-squared:  0.5212,    Adjusted R-squared:  0.521 \nF-statistic:  3439 on 10 and 31602 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation:\n- log(total_livable_area) (0.752): An elasticity coefficient. A 1% increase in livable area is associated with a 0.752% increase in price. This is a strong positive driver. - number_of_bathrooms (0.046): Each additional bathroom is associated with a 4.6% increase in price. - house_age_c (-0.00001): The linear term for house age is statistically insignificant (p=0.929). - house_age_c2 (0.000047): The squared term for age is positive and significant. Combined with the insignificant linear term, this suggests a slight U-shaped relationship, where new homes and very old homes (perhaps with historical value) command a premium over middle-aged homes. - interior_condition (-0.114): Assuming a higher value means worse condition, each one-unit worsening in condition is associated with an 11.4% decrease in price. - quality_grade_num (0.070): Each one-unit increase in the quality grade is associated with a 7.0% increase in price. - fireplaces (0.117): Each additional fireplace is associated with a 11.7% increase in price. - garage_spaces (0.143): Each additional garage space is associated with a 14.3% increase in price. - central_air_dummy (0.458): Homes with central air are estimated to be 45.8% more expensive than the baseline (e.g., no AC). This is a very significant amenity premium. - central_air_missing (-0.230): Homes where central air data is missing are 23.0% cheaper than the baseline.\n4.2 Census variables:\n\n\nCode\nmodel_2 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census\n                  ba_rate +\n                  unemployment_rate,\n  \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_2)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate, \n    data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-3.2514 -0.1415  0.0546  0.2237  2.0880 \n\nCoefficients:\n                            Estimate   Std. Error t value             Pr(&gt;|t|)\n(Intercept)              7.279501574  0.064443586 112.959 &lt; 0.0000000000000002\nlog(total_livable_area)  0.700431343  0.008755698  79.997 &lt; 0.0000000000000002\nnumber_of_bathrooms      0.057764788  0.004568488  12.644 &lt; 0.0000000000000002\nhouse_age_c             -0.000064246  0.000097377  -0.660                0.509\nhouse_age_c2             0.000009814  0.000001468   6.686    0.000000000023255\ninterior_condition      -0.126482820  0.003572164 -35.408 &lt; 0.0000000000000002\nquality_grade_num        0.001702051  0.002417186   0.704                0.481\nfireplaces               0.064233267  0.008917462   7.203    0.000000000000602\ngarage_spaces            0.168324520  0.005472052  30.761 &lt; 0.0000000000000002\ncentral_air_dummy        0.219136581  0.006851612  31.983 &lt; 0.0000000000000002\ncentral_air_missing     -0.155840316  0.007252663 -21.487 &lt; 0.0000000000000002\nincome_scaled            0.453407655  0.009052596  50.086 &lt; 0.0000000000000002\nba_rate                  0.012813063  0.000358216  35.769 &lt; 0.0000000000000002\nunemployment_rate       -0.006619614  0.000527486 -12.549 &lt; 0.0000000000000002\n                           \n(Intercept)             ***\nlog(total_livable_area) ***\nnumber_of_bathrooms     ***\nhouse_age_c                \nhouse_age_c2            ***\ninterior_condition      ***\nquality_grade_num          \nfireplaces              ***\ngarage_spaces           ***\ncentral_air_dummy       ***\ncentral_air_missing     ***\nincome_scaled           ***\nba_rate                 ***\nunemployment_rate       ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4017 on 31599 degrees of freedom\nMultiple R-squared:  0.6796,    Adjusted R-squared:  0.6794 \nF-statistic:  5155 on 13 and 31599 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation:\n- Coefficient Evolution (vs.¬†Model 1): - log(total_livable_area) (0.710 vs 0.752): The elasticity of area decreased. This suggests Model 1 overestimated the impact of area. Why? Because larger homes are often located in wealthier neighborhoods. Model 1 incorrectly attributed some of the ‚Äúwealthy neighborhood‚Äù premium to ‚Äúlarge area.‚Äù - quality_grade_num (0.0015 vs 0.070): The coefficient for quality grade became statistically insignificant (p=0.520). This is a key finding: home quality is highly correlated with neighborhood income. Once we directly control for income (income_scaled), the independent effect of quality grade disappears. - central_air_dummy (0.219 vs 0.458): The premium for central air was halved. This also indicates that central air is more common in affluent areas, and Model 1 suffered from significant Omitted Variable Bias (OVB). - New Variable (Census) Interpretation: - income_scaled (0.455): A one-unit increase in standardized census tract income is associated with a 45.5% increase in price. A very strong positive effect. - ba_rate (0.0129): A 1 percentage point increase in the neighborhood‚Äôs bachelor‚Äôs degree attainment rate is associated with a 1.3% price increase. - unemployment_rate (-0.0066): A 1 percentage point increase in the unemployment rate is associated with a 0.66% decrease in price.\n4.3 Spatial features:\n\n\nCode\nmodel_3 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census \n                  ba_rate +\n                  unemployment_rate +\n                \n                  transit_count+\n                  avg_past_price_density+      #Spatial \n                  sqrt(crime_count) +\n                  log(nearest_hospital_knn3),\n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_3)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate + \n    transit_count + avg_past_price_density + sqrt(crime_count) + \n    log(nearest_hospital_knn3), data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-3.03217 -0.09905  0.05666  0.18464  2.17114 \n\nCoefficients:\n                               Estimate   Std. Error t value\n(Intercept)                 6.562309985  0.084718417  77.460\nlog(total_livable_area)     0.742766329  0.007908779  93.917\nnumber_of_bathrooms         0.057510788  0.004057815  14.173\nhouse_age_c                -0.000093444  0.000087876  -1.063\nhouse_age_c2               -0.000005113  0.000001322  -3.866\ninterior_condition         -0.161139310  0.003179231 -50.685\nquality_grade_num          -0.029091664  0.002254432 -12.904\nfireplaces                  0.031025554  0.008023515   3.867\ngarage_spaces               0.096993528  0.005071631  19.125\ncentral_air_dummy           0.099045830  0.006176862  16.035\ncentral_air_missing        -0.164351875  0.006393736 -25.705\nincome_scaled               0.166039882  0.008635180  19.228\nba_rate                     0.003250996  0.000349765   9.295\nunemployment_rate          -0.002568034  0.000466955  -5.500\ntransit_count               0.000311944  0.000171205   1.822\navg_past_price_density      0.003018698  0.000039395  76.626\nsqrt(crime_count)          -0.049042531  0.001408152 -34.828\nlog(nearest_hospital_knn3)  0.081937118  0.006616703  12.383\n                                       Pr(&gt;|t|)    \n(Intercept)                &lt; 0.0000000000000002 ***\nlog(total_livable_area)    &lt; 0.0000000000000002 ***\nnumber_of_bathrooms        &lt; 0.0000000000000002 ***\nhouse_age_c                            0.287626    \nhouse_age_c2                           0.000111 ***\ninterior_condition         &lt; 0.0000000000000002 ***\nquality_grade_num          &lt; 0.0000000000000002 ***\nfireplaces                             0.000110 ***\ngarage_spaces              &lt; 0.0000000000000002 ***\ncentral_air_dummy          &lt; 0.0000000000000002 ***\ncentral_air_missing        &lt; 0.0000000000000002 ***\nincome_scaled              &lt; 0.0000000000000002 ***\nba_rate                    &lt; 0.0000000000000002 ***\nunemployment_rate                  0.0000000384 ***\ntransit_count                          0.068458 .  \navg_past_price_density     &lt; 0.0000000000000002 ***\nsqrt(crime_count)          &lt; 0.0000000000000002 ***\nlog(nearest_hospital_knn3) &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3536 on 31481 degrees of freedom\n  (114 observations deleted due to missingness)\nMultiple R-squared:  0.7505,    Adjusted R-squared:  0.7504 \nF-statistic:  5571 on 17 and 31481 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation:\n- Coefficient Evolution (vs.¬†Model 2): - income_scaled (0.146 vs 0.455): The effect of income dropped sharply (by ~2/3). This again reveals OVB in Model 2. The large ‚Äúincome‚Äù effect in Model 2 was confounded with ‚Äúspatial amenities‚Äù‚Äîhigh-income individuals tend to live in low-crime, accessible areas. - ba_rate (0.0027 vs 0.0129): The education premium also dropped significantly for the same reason. - garage_spaces (0.095 vs 0.170): The garage premium decreased, likely because spatial variables (like density or transit access) have captured related information. - New Variable (Spatial) Interpretation: - transit_count (0.00029): Each additional nearby public transit stop is associated with a 0.029% increase in price. - avg_past_price_density (0.0032): As a proxy for local market heat or locational value, each unit increase is associated with a 0.32% price increase. - sqrt(crime_count) (-0.040): A one-unit increase in the square root of the crime count is associated with a 4.0% decrease in price. - log(nearest_hospital_knn3) (0.087): A 1% increase in the distance from the nearest hospital is associated with a 0.087% increase in price. This suggests people prefer to live further away from hospitals (perhaps to avoid noise, traffic, or sirens), not closer.\n4.4 Interactions and fixed effects:\n\n\nCode\nmodel_4 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census \n                  ba_rate +\n                  unemployment_rate +\n                \n                  transit_count+\n                  avg_past_price_density+      #Spatial \n                  sqrt(crime_count) +\n                  log(nearest_hospital_knn3)+\n                \n                  (interior_condition * income_scaled)+  #FE & Interaction\n                  factor(zip_code),\n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_4)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate + \n    transit_count + avg_past_price_density + sqrt(crime_count) + \n    log(nearest_hospital_knn3) + (interior_condition * income_scaled) + \n    factor(zip_code), data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-2.86007 -0.09067  0.05500  0.16974  2.17553 \n\nCoefficients:\n                                     Estimate   Std. Error t value\n(Intercept)                       7.147174262  0.125631514  56.890\nlog(total_livable_area)           0.762702187  0.007951214  95.923\nnumber_of_bathrooms               0.062545809  0.003951966  15.827\nhouse_age_c                       0.000081673  0.000091303   0.895\nhouse_age_c2                      0.000002396  0.000001333   1.797\ninterior_condition               -0.167693153  0.003180473 -52.726\nquality_grade_num                -0.020614687  0.002369293  -8.701\nfireplaces                        0.040919276  0.008065426   5.073\ngarage_spaces                     0.062755291  0.005224896  12.011\ncentral_air_dummy                 0.088693723  0.006224102  14.250\ncentral_air_missing              -0.145545244  0.006529396 -22.291\nincome_scaled                    -0.308308801  0.022099101 -13.951\nba_rate                           0.004727434  0.000434125  10.890\nunemployment_rate                -0.002009575  0.000512327  -3.922\ntransit_count                     0.000210309  0.000188166   1.118\navg_past_price_density            0.002676947  0.000053851  49.710\nsqrt(crime_count)                -0.040746561  0.001633811 -24.940\nlog(nearest_hospital_knn3)       -0.007368586  0.012879355  -0.572\nfactor(zip_code)19103            -0.191888874  0.038623356  -4.968\nfactor(zip_code)19104             0.043168643  0.044596804   0.968\nfactor(zip_code)19106            -0.072911591  0.039431744  -1.849\nfactor(zip_code)19107             0.002870411  0.041113568   0.070\nfactor(zip_code)19111             0.092033842  0.042730139   2.154\nfactor(zip_code)19114             0.046991631  0.045784241   1.026\nfactor(zip_code)19115             0.036786258  0.045577032   0.807\nfactor(zip_code)19116             0.074693711  0.047046924   1.588\nfactor(zip_code)19118             0.042277408  0.050964443   0.830\nfactor(zip_code)19119            -0.005571634  0.045415173  -0.123\nfactor(zip_code)19120            -0.001979921  0.042451411  -0.047\nfactor(zip_code)19121            -0.079268931  0.042967693  -1.845\nfactor(zip_code)19122            -0.046067555  0.043677075  -1.055\nfactor(zip_code)19123            -0.124474312  0.042021722  -2.962\nfactor(zip_code)19124            -0.042922550  0.042779720  -1.003\nfactor(zip_code)19125            -0.053924641  0.040136398  -1.344\nfactor(zip_code)19126            -0.096134211  0.050099576  -1.919\nfactor(zip_code)19127            -0.054620383  0.046526938  -1.174\nfactor(zip_code)19128            -0.063379705  0.041111172  -1.542\nfactor(zip_code)19129            -0.081840032  0.044983229  -1.819\nfactor(zip_code)19130             0.004465219  0.038447508   0.116\nfactor(zip_code)19131            -0.124963371  0.044224170  -2.826\nfactor(zip_code)19132            -0.353731640  0.042632020  -8.297\nfactor(zip_code)19133            -0.360578685  0.045760207  -7.880\nfactor(zip_code)19134            -0.167019067  0.042150583  -3.962\nfactor(zip_code)19135             0.066237684  0.045406809   1.459\nfactor(zip_code)19136             0.093091097  0.045399194   2.051\nfactor(zip_code)19137             0.003922355  0.050119037   0.078\nfactor(zip_code)19138            -0.061547285  0.045851962  -1.342\nfactor(zip_code)19139            -0.125514020  0.043923079  -2.858\nfactor(zip_code)19140            -0.243638620  0.042070333  -5.791\nfactor(zip_code)19141            -0.104140516  0.045656220  -2.281\nfactor(zip_code)19142            -0.117894811  0.045192100  -2.609\nfactor(zip_code)19143            -0.121037876  0.042647300  -2.838\nfactor(zip_code)19144            -0.119218035  0.044443147  -2.682\nfactor(zip_code)19145             0.000416732  0.040490607   0.010\nfactor(zip_code)19146            -0.065296494  0.038364344  -1.702\nfactor(zip_code)19147            -0.022134932  0.038396424  -0.576\nfactor(zip_code)19148             0.034927326  0.039935049   0.875\nfactor(zip_code)19149             0.163264326  0.043056311   3.792\nfactor(zip_code)19150            -0.044481749  0.048200789  -0.923\nfactor(zip_code)19151            -0.087776350  0.045219855  -1.941\nfactor(zip_code)19152             0.091653850  0.044520396   2.059\nfactor(zip_code)19153            -0.033554531  0.052491824  -0.639\nfactor(zip_code)19154             0.056967215  0.044374083   1.284\ninterior_condition:income_scaled  0.119746969  0.005588581  21.427\n                                             Pr(&gt;|t|)    \n(Intercept)                      &lt; 0.0000000000000002 ***\nlog(total_livable_area)          &lt; 0.0000000000000002 ***\nnumber_of_bathrooms              &lt; 0.0000000000000002 ***\nhouse_age_c                                   0.37105    \nhouse_age_c2                                  0.07241 .  \ninterior_condition               &lt; 0.0000000000000002 ***\nquality_grade_num                &lt; 0.0000000000000002 ***\nfireplaces                        0.00000039295484010 ***\ngarage_spaces                    &lt; 0.0000000000000002 ***\ncentral_air_dummy                &lt; 0.0000000000000002 ***\ncentral_air_missing              &lt; 0.0000000000000002 ***\nincome_scaled                    &lt; 0.0000000000000002 ***\nba_rate                          &lt; 0.0000000000000002 ***\nunemployment_rate                 0.00008784000945812 ***\ntransit_count                                 0.26371    \navg_past_price_density           &lt; 0.0000000000000002 ***\nsqrt(crime_count)                &lt; 0.0000000000000002 ***\nlog(nearest_hospital_knn3)                    0.56724    \nfactor(zip_code)19103             0.00000067928689067 ***\nfactor(zip_code)19104                         0.33306    \nfactor(zip_code)19106                         0.06446 .  \nfactor(zip_code)19107                         0.94434    \nfactor(zip_code)19111                         0.03126 *  \nfactor(zip_code)19114                         0.30472    \nfactor(zip_code)19115                         0.41960    \nfactor(zip_code)19116                         0.11238    \nfactor(zip_code)19118                         0.40680    \nfactor(zip_code)19119                         0.90236    \nfactor(zip_code)19120                         0.96280    \nfactor(zip_code)19121                         0.06507 .  \nfactor(zip_code)19122                         0.29156    \nfactor(zip_code)19123                         0.00306 ** \nfactor(zip_code)19124                         0.31571    \nfactor(zip_code)19125                         0.17911    \nfactor(zip_code)19126                         0.05501 .  \nfactor(zip_code)19127                         0.24042    \nfactor(zip_code)19128                         0.12316    \nfactor(zip_code)19129                         0.06887 .  \nfactor(zip_code)19130                         0.90754    \nfactor(zip_code)19131                         0.00472 ** \nfactor(zip_code)19132            &lt; 0.0000000000000002 ***\nfactor(zip_code)19133             0.00000000000000339 ***\nfactor(zip_code)19134             0.00007435204084637 ***\nfactor(zip_code)19135                         0.14464    \nfactor(zip_code)19136                         0.04032 *  \nfactor(zip_code)19137                         0.93762    \nfactor(zip_code)19138                         0.17951    \nfactor(zip_code)19139                         0.00427 ** \nfactor(zip_code)19140             0.00000000705409283 ***\nfactor(zip_code)19141                         0.02256 *  \nfactor(zip_code)19142                         0.00909 ** \nfactor(zip_code)19143                         0.00454 ** \nfactor(zip_code)19144                         0.00731 ** \nfactor(zip_code)19145                         0.99179    \nfactor(zip_code)19146                         0.08876 .  \nfactor(zip_code)19147                         0.56429    \nfactor(zip_code)19148                         0.38180    \nfactor(zip_code)19149                         0.00015 ***\nfactor(zip_code)19150                         0.35610    \nfactor(zip_code)19151                         0.05225 .  \nfactor(zip_code)19152                         0.03953 *  \nfactor(zip_code)19153                         0.52268    \nfactor(zip_code)19154                         0.19922    \ninterior_condition:income_scaled &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3423 on 31435 degrees of freedom\n  (114 observations deleted due to missingness)\nMultiple R-squared:  0.7665,    Adjusted R-squared:  0.7661 \nF-statistic:  1638 on 63 and 31435 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\nvif(model_4)\n\n\n                                       GVIF Df GVIF^(1/(2*Df))\nlog(total_livable_area)            1.523908  1        1.234467\nnumber_of_bathrooms                1.611921  1        1.269614\nhouse_age_c                        1.580710  1        1.257263\nhouse_age_c2                       1.470007  1        1.212439\ninterior_condition                 1.533402  1        1.238306\nquality_grade_num                  1.779716  1        1.334060\nfireplaces                         1.295084  1        1.138018\ngarage_spaces                      1.586272  1        1.259473\ncentral_air_dummy                  2.084180  1        1.443669\ncentral_air_missing                1.453277  1        1.205519\nincome_scaled                     24.491928  1        4.948932\nba_rate                            6.178507  1        2.485660\nunemployment_rate                  2.016927  1        1.420186\ntransit_count                      1.715169  1        1.309645\navg_past_price_density             7.837830  1        2.799612\nsqrt(crime_count)                  2.815547  1        1.677959\nlog(nearest_hospital_knn3)         8.102261  1        2.846447\nfactor(zip_code)                 565.126609 45        1.072950\ninterior_condition:income_scaled  20.090358  1        4.482227\n\n\nCoefficient Interpretation:\n- Fixed Effects Interpretation: - These coefficients represent the price difference for each zip code relative to the ‚Äúreference zip code‚Äù (which is omitted from the list, e.g., 19102). - Example: factor(zip_code)19106 (-0.107): A home in zip code 19106 is, on average, 10.7% less expensive than a home in the reference zip code, holding all other variables constant. - Example: factor(zip_code)19149 (0.183): A home in zip code 19149 is, on average, 18.3% more expensive. - interior_condition:income_scaled (0.117) (Interaction Term): - This is one of the most interesting findings. It shows that the impact of interior_condition depends on income_scaled. - The total marginal effect of interior_condition is:\\[= -0.1695 + 0.1165 \\times \\text{income\\_scaled}\\] - At the baseline income level (income_scaled = 0), each one-unit worsening in condition is associated with a 17.0% price decrease (-0.1695). However, this penalty is mitigated (lessened) in higher-income areas. For each one-unit increase in income_scaled, the negative penalty of poor condition is reduced by 11.7 percentage points. This may imply that in high-income neighborhoods, ‚Äúfixer-uppers‚Äù (homes in poor condition) are seen as investment opportunities with high renovation potential. Therefore, the market penalty for ‚Äúpoor condition‚Äù is smaller."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-5-model-validation",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-5-model-validation",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 5: Model Validation",
    "text": "Phase 5: Model Validation\n\n10-fold cross-validation\n5.1 Compare all 4 models:"
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-6-model-diagnostics",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-6-model-diagnostics",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 6: Model Diagnostics",
    "text": "Phase 6: Model Diagnostics\n\nCheck assumptions for best model:\n6.1 Residual plot:\n\n\nCode\nmodel_data &lt;- data.frame(\n  Fitted = fitted(model_4),\n  Residuals = resid(model_4)\n)\n\np_resid_fitted &lt;- ggplot(model_data, aes(x = Fitted, y = Residuals)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\", size = 2) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"loess\", color = \"black\", se = FALSE, linewidth = 0.8) +\n  labs(\n    title = \"Residuals vs Fitted Values\",\n    subtitle = \"Checking linearity and homoscedasticity for Model 4\",\n    x = \"Fitted Values (Log(Sale Price))\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_resid_fitted\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\nresid_full &lt;- rep(NA, nrow(opa_census_all))\nresid_full[-as.numeric(model_4$na.action)] &lt;- resid(model_4)\n\nopa_census_all$residuals &lt;- resid_full\n\ntract_resid &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_residual = mean(residuals, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_resid, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_residual), color = \"white\", size = 0.2) +\n  scale_fill_gradient2(\n    low = \"#6A1B9A\", mid = \"white\", high = \"#FFB300\",\n    midpoint = 0,\n    limits = c(-0.5, 0.5),\n    name = \"Mean Log Residual\",\n    breaks = c(-0.3, 0, 0.3),\n    labels = c(\"Overestimated\", \"Accurate\", \"Underestimated\"),\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Hardest to Predict Neighborhoods in Philadelphia\",\n    subtitle = \"Yellow = underestimation | Purple = overestimation\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\nInterpretation:\n- Central Tendency of Residuals: The residuals are generally centered around the zero line, showing no systematic deviation. This indicates that the model captures the overall linear relationship between predictors and the response variable effectively.\n- Homoscedasticity: The residuals show greater variability and dispersion in the lower fitted value range (approximately 10‚Äì12), while they appear more stable at higher fitted values. This suggests that the model performs less effectively for observations with lower predicted values.\n- Model Assumption Assessment: Overall, the assumptions of linearity and homoscedasticity are largely satisfied, indicating a sound model fit, with only slight deviations to monitor at higher fitted values.\n6.2 Q-Q plot:\n\n\nCode\np_qq &lt;- ggplot(model_data, aes(sample = Residuals)) +\n  stat_qq(color = \"#6A1B9A\", size = 2, alpha = 0.6) +\n  stat_qq_line(color = \"red\",linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Normal Q-Q Plot\",\n    subtitle = \"Checking normality of residuals for Model 4\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_qq\n\n\n\n\n\n\n\n\n\nInterpretation:\n- Overall Shape of the Plot: The residual points generally follow the diagonal line, indicating that the overall distribution of residuals is roughly consistent with a normal distribution.\n- Good Fit in the Central Range: In the central range (around -1 to 1 quantiles), the sample quantiles align closely with the theoretical quantiles, suggesting that most residuals conform well to the assumption of normality.\n- Deviation in the Tails: At both tails‚Äîespecially the upper quantiles‚Äîthe points deviate noticeably from the red dashed line, indicating that the residuals have heavier tails than a normal distribution, suggesting slight non-normality.\n- Assessment of Normality Assumption: Although deviations appear in the tails, the overall alignment with the reference line is strong, indicating that the normality assumption largely holds, with only minor deviations for extreme residuals.\n6.3 Cook‚Äôs distance:\n\n\nCode\ncooks_d &lt;- cooks.distance(model_4)\nmodel_data &lt;- data.frame(\n  Index = 1:length(cooks_d),\n  CooksD = cooks_d\n)\nthreshold &lt;- 4 / nrow(model_4$model)\n\np_cook &lt;- ggplot(model_data, aes(x = Index, y = CooksD)) +\n  geom_segment(aes(xend = Index, yend = 0), color = \"#6A1B9A\", alpha = 0.7) +  # vertical lines\n  geom_point(color = \"#6A1B9A\", size = 0.15) +\n  geom_hline(yintercept = threshold, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Cook's Distance\",\n    subtitle = \"Identifying influential observations for Model 4\",\n    x = \"Observation Index\",\n    y = \"Cook's Distance\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_cook\n\n\n\n\n\n\n\n\n\nInterpretation:\n- Overall Distribution Pattern: Most observations have Cook‚Äôs Distance values close to zero, indicating that the dataset‚Äôs overall influence on the model is balanced, with no widespread undue impact.\n- Presence of Influential Points: A few vertical spikes rise noticeably above the rest, indicating the presence of some influential observations that may affect the estimated model coefficients. - Model Robustness Conclusion: Overall, the model appears robust, with no single observation exerting excessive influence."
  },
  {
    "objectID": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-7-conclusions-recommendations",
    "href": "Assignments/midterm/slides/Jinyang_Xu_appendix.html#phase-7-conclusions-recommendations",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 7: Conclusions & Recommendations",
    "text": "Phase 7: Conclusions & Recommendations\n\nConclusion:\n\nOur final model‚Äôs R¬≤ is 0.84, indicating that the model explains 84% of the variance in log sale prices. The RMSE is , showing that the model‚Äôs predictions are reasonably close to the observed values.\n\nLivable Area of the House matters most for Philadelphia prices.\n\n\n\nRecommendations:\n\nEquity concerns\n\nWhich neighborhoods are hardest to predict?\n\nThe largest prediction errors occur primarily in central Philadelphia, where both overestimation and underestimation coexist within close proximity. This pattern indicates that the model struggles most in areas with high housing heterogeneity- neighborhoods that contain a mix of old row houses, newly renovated apartments, and varying property types within short distances.\nIn contrast, outer neighborhoods such as those in the northwest and northeast tend to have more consistent housing characteristics, leading to smaller residuals. The central tracts‚Äô larger residuals suggest that cultural or historical features have introduced variability that the model‚Äôs current features (mainly physical characteristics) cannot fully explain.\n\nAny data bias?\n\nThe observed spatial pattern of prediction errors reflects inherent data bias. The dataset likely overrepresents mid-range housing conditions and underrepresents both luxury and low-income housing. As shown in the livable area and interior condition maps, data coverage in wealthier areas (especially the northwest) is limited, and these tracts often contain more unique, high-value properties that the model cannot generalize well.\n\nWealthier tracts are more likely to be well-documented, while poorer areas lack records, creating spatial imbalance in model performance.\n\nHigh-priced homes typically have larger negotiation margins, meaning their final sale prices are often lower than the listed prices. In contrast, low-priced homes sell closer to their listing prices. As a result, model tends to overestimate expensive homes and underestimate affordable ones, introducing systematic bias.\n\n\nRecommendations to government\n\n\nImmediate System Calibration: We recommend utilizing our model‚Äôs findings to immediately adjust property assessments in systematically overvalued low-income communities. This action will ensure a fair distribution of the tax burden and address current inequities.\nIntegrate Advanced Spatial Features: We advise the Office of Property Assessment (OPA) to permanently integrate the effective spatial characteristics identified by our study‚Äîspecifically Comparable Sales Proxies (surrounding transaction prices) and Neighborhood Fixed Effects‚Äîinto the next-generation AVM. This will significantly enhance the model‚Äôs responsiveness to rapidly changing market dynamics.\nExtreme high values almost exclusively stem from corporate transactions and require manual review for outliers.\n\n\nLimitations and nest steps\n\nLimitations: Inherent Data Biases\n\nOur predictive accuracy is constrained by inherent data biases which affect equity. We find a dual challenge in data coverage: in affluent areas, high-value, unique properties suffer from data sparsity, making generalization difficult. Conversely, lower-income areas often show data incompleteness, leading to less reliable predictions and higher residual errors. Critically, we observed a price-tier bias: high-priced homes tend to sell below their list price, while low-priced homes transact closer to it. This systemic pattern means the model is prone to over-assessing expensive properties and under-assessing affordable ones, creating a structural risk for vertical inequity in the tax system.\n\nNext Steps: Enhancing Data Quality and Fairness\n\nTo address these limitations, our next steps focus on data enrichment and equitable optimization. The City should partner with us to integrate non-public data, such as detailed appraisal records and permit data, which can account for unobserved renovation quality and close the data gap. Furthermore, to combat the systematic price-tier bias, we recommend integrating fairness metrics directly into the AVM‚Äôs optimization process. This will shift the model‚Äôs objective beyond simple average accuracy (RMSE) to ensure that prediction errors are uniformly low across all price tiers and all Philadelphia neighborhoods, securing both accuracy and equity."
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#the-problem",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#the-problem",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "The Problem",
    "text": "The Problem\nEvery year, thousands of Philadelphians buy or sell homes, and every transaction tells a story about how people value location, convenience, and opportunity.\nYet the city‚Äôs current Automated Valuation Model doesn‚Äôt always capture these stories. Some neighborhoods are undervalued, while others bear unfairly high assessments.\n\n\n\n\n\n\nWhat we are trying to explore?\n\n\n\nPrice Gap: Why do two homes with similar size and design sell for very different prices in Philadelphia?\nSpatial Drivers: Which neighborhood, accessibility, and socioeconomic factors drive these differences?\nFair Valuation: How can understanding them help the city create a fairer, smarter system for property tax assessment?"
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#data-sources",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#data-sources",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Data Sources",
    "text": "Data Sources\n\n\nüè†Property sales\n(n = 34,559 2023-2024)\n\nLivable Area\nBedrooms Counts\nBathrooms Counts\nHouse Age\nInterior Condition\nQuality Grade\nFireplaces\nGarage Spaces\nCentral Air\nMarket Value\nSale Price\n\n\nüî¢Census ACS\n(n = 2023)\n\nMedian Income\nEducation Level\nUnemployment Rate\n\nüèõOpenDataPhilly\n\nTransit\nParks and Recreation center\nHospitals\nCrime(2023)\nPoint of Interest"
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#where-are-expensive-homes",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#where-are-expensive-homes",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Where Are Expensive Homes?",
    "text": "Where Are Expensive Homes?\n\n\n\n\n\n\nResidential sale prices exhibited clear signs of spatial clustering.\nhighest-priced properties: Center City core, University City.\nMiddle & Lower-Price tiers : North Philadelphia, West Philadelphia (outside the university sphere), and South Philadelphia.\nAccount for the specific ‚Äúmicro-market‚Äù in which a property is situated."
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#why-are-they-expensive",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#why-are-they-expensive",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Why Are They Expensive?",
    "text": "Why Are They Expensive?"
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#highlight",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#highlight",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Highlight",
    "text": "Highlight\n\nOur adjustment follows established mass appraisal practices, where implausible sale prices are identified and adjusted based on their ratio to assessed market value (Deaf Smith CAD, Mass Appraisal and Ratio Study Manual, 2023).\nPrincipleÔºöRetain all information that the data tells us (including outliers) rather than deleting it.\nObjectiveÔºöEnsure equity.\nRelationship Formula: Sale Price = -9189.29 + 1.03*Market value\nApproach: Unreliable sale price(low weight)~ Reliable sale price(high weight)"
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#model-building-and-validation",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#model-building-and-validation",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Model Building and Validation",
    "text": "Model Building and Validation\nModel Performance Improves with Each Layer\n\n\n\nModel\nCV RMSE (log)\nR¬≤\nRMSE\n\n\n\n\nStructural Only\n0.5497\n0.5235\n221675.8\n\n\n+ Census\n0.4519\n0.6779\n178383.4\n\n\n+ Spatial\n0.3994\n0.7486\n132547.4\n\n\n+ Interactions/FE\n0.389\n0.7611\n124417.4\n\n\n\n\nR¬≤ = 0.76 =&gt; The model explains 76% of the variation in residential sale prices.\nRMSE = 124,417.4 =&gt; The average error between the model‚Äôs predicted sale price and the actual sale price is approximately \\(\\$124,417.4\\).\n\n\n\n\n\n\n\nImportant\n\n\n\nFor K-fold cross-validation, we use the entire dataset with replaced outliers for training to maintain consistency with the model derivation process. For the final test, however, we use a purified set that excludes all handled data, ensuring a reliable performance evaluation on genuine, trustworthy data."
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#key-factors",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#key-factors",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Key Factors",
    "text": "Key Factors\n\nLivable Area\nComparable Sales\nInterior Condition\nZip Code"
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#identifying-the-hardest-to-predict-neighborhoods",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#identifying-the-hardest-to-predict-neighborhoods",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Identifying the Hardest-to-Predict Neighborhoods",
    "text": "Identifying the Hardest-to-Predict Neighborhoods\n\n\n\n\n\nInterpretation\n\nUnderestimate zip code: 19130, 19123, 19104, 19144‚Ä¶\nOverestimate zip code: 19102, 19107, 19116, 19154‚Ä¶\nMarket is moving faster than the city‚Äôs valuation system can keep up."
  },
  {
    "objectID": "Assignments/midterm/slides/Presentation_slides_final.html#recommendations-and-limitations",
    "href": "Assignments/midterm/slides/Presentation_slides_final.html#recommendations-and-limitations",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Recommendations and Limitations",
    "text": "Recommendations and Limitations\n\nCorrect the assessed values in low-income communities that have been systematically overestimated.\nRoutinely incorporate the effective spatial features‚Äîparticularly surrounding transaction prices and neighborhood fixed effects.\nExtreme high values almost exclusively stem from corporate transactions and require manual review for outliers.\n\n\n\n\n\n\n\nLimitations: Algorithmic Fairness\n\n\n\nSpatial Coverage Bias and Data Quality Bias\nFeature Omission Bias and Spatial Dependence Bias"
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#section",
    "href": "Final/Slides/Slides/Simple_context.html#section",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "",
    "text": "Chapter 1. The Challenge\n\n\nEYES ON THE STREET OR TARGETS FOR CRIME?"
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#the-problem-reactive-policing",
    "href": "Final/Slides/Slides/Simple_context.html#the-problem-reactive-policing",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "THE PROBLEM: REACTIVE POLICING",
    "text": "THE PROBLEM: REACTIVE POLICING\n\nSEPTA carries thousands of passengers daily. Yet for Transit Police, a core question remains unresolved:\n\nDoes high ridership actually create safety (‚ÄúEyes on the Street‚Äù) ‚Äî or does it increase opportunities for crime (‚ÄúPotential Targets‚Äù)?\n\n\n\n\n\n\nüö´ CURRENT ISSUES\n\n\nInefficiency: Patrols often follow intuition instead of evidence.\nSafety Gaps: High-risk stops get overlooked because they aren‚Äôt high-ridership.\nResource Strain: The ‚Äúshotgun approach‚Äù spreads limited personnel too thin.\n\n\n\n\n\nüö® Our Policy Goal\n\n\nHelp SEPTA Transit Police move from reactive responses toward evidence-guided deployment.\nUse ridership‚Äìcrime modeling to identify ‚ÄúHigh-Risk Anomalies‚Äù ‚Äî stops where crime risk is unexpectedly high relative to passenger volume and context."
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#data-sources-the-secret-ingredient",
    "href": "Final/Slides/Slides/Simple_context.html#data-sources-the-secret-ingredient",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "Data Sources: The ‚ÄúSecret Ingredient‚Äù",
    "text": "Data Sources: The ‚ÄúSecret Ingredient‚Äù\n\n\nüöå Operational Data\nSEPTA Ridership (Summer 2025) (OpenDataPhilly)\n\nAggregated by Stop\nKey Feature: Weekday vs.¬†Weekend split\nRole: Measures ‚ÄúExposure‚Äù\n\nüö® Outcome Variable\nCrime Incidents (OpenDataPhilly)\n\nRobbery, Assault, Theft\nMetric: Total Count (Corrected for days)\n\n\nüèôÔ∏è Environment\n(OpenDataPhilly)\n\nAlcohol Outlets: Crime Generators\nStreet Lights: Guardianship (CPTED)\nVacant Land: Broken Windows\nPolice Stations: Response Distance\n\nüë• Demographics\n(ACS Data)\n\nPoverty Rate, Unemployment, Income"
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#section-1",
    "href": "Final/Slides/Slides/Simple_context.html#section-1",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "",
    "text": "Chapter 2. The Technique\n\n\nWhy Negative Binomial?"
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#why-not-linear-regression",
    "href": "Final/Slides/Slides/Simple_context.html#why-not-linear-regression",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\n\n\n\n\n\n\n\nThe Statistical Reality\n\n\n\nThe Nature: Crime data is a discrete Count Variable (0, 1, 2‚Ä¶).\n\n\nThe Problem: It is heavily Right-Skewed. Most stops have zero crime, creating ‚ÄúOverdispersion‚Äù (Variance &gt;&gt; Mean).\n\n\nThe Solution: We used Negative Binomial Regression instead of OLS to mathematically account for this distribution."
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#the-core-hypothesis-guardian-or-target",
    "href": "Final/Slides/Slides/Simple_context.html#the-core-hypothesis-guardian-or-target",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "The Core Hypothesis: Guardian or Target?",
    "text": "The Core Hypothesis: Guardian or Target?\nUnmasking the Dual Nature of Ridership\nWe suspect ridership plays competing roles in public safety. A simple regression averages these effects, potentially hiding the truth.\n\nMethod: Split the data into Weekdays and Weekends, and use the interaction term to isolate the impact of Ridership.\n\n\nScenario A (Eyes on the Street): Commuters create natural surveillance.\n\nExpectation: Higher ridership \\(\\rightarrow\\) Less Crime.\n\nScenario B (Potential Targets): Leisure crowds create opportunities for theft.\n\nExpectation: Higher ridership \\(\\rightarrow\\) More Crime.\n\n\nThe Model Specification:\n\\[Crime = \\beta_0 + \\underbrace{\\beta_1(Ridership)}_{\\text{Guardian Effect}} + \\beta_2(Weekend) + \\underbrace{\\mathbf{\\beta_3(Ridership \\times Weekend)}}_{\\text{Target Effect (The Shift)}}\\]"
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#section-2",
    "href": "Final/Slides/Slides/Simple_context.html#section-2",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "",
    "text": "Chapter 3. The EVIDENCE\n\n\nMODEL PERFORMANCE"
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#what-drives-crime-risk",
    "href": "Final/Slides/Slides/Simple_context.html#what-drives-crime-risk",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "WHAT DRIVES CRIME RISK?",
    "text": "WHAT DRIVES CRIME RISK?\n\n\n\n\n\n\n\n\n\n\n\nüî• RISK AMPLIFIERS\n\n\n\nRidership: More people = More targets.\n\n\nAlcohol Outlets: Strongest environmental predictor.\n\n\nVacancy: ‚ÄúBroken Windows‚Äù effect attracts crime.\n\n\nWeekend Shift: The interaction term confirms risk dynamics intensify on weekdays.\n\n\n\n\n\nüõ°Ô∏è RISK MITIGATORS\n\n\n\nStreet Lights: Validates CPTED theory‚Äîbetter lighting significantly reduces incidents."
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#model-validation-cv-results",
    "href": "Final/Slides/Slides/Simple_context.html#model-validation-cv-results",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "Model Validation (CV Results)",
    "text": "Model Validation (CV Results)\nModel Performance Improves with Each Layer\n\n\n\nModel\nMAE (Error Count)\nRMSE\nImprovement\n\n\n\n\n1. Ridership Only\n17.268\n28.877\n-\n\n\n2. + Interaction\n17.209\n28.868\n+ 0.3%\n\n\n3. + Env & Demo\n12.777\n21.039\n+ 26%\n\n\n4. + Fixed\n11.409\n18.343\n+ 33.9%\n\n\n5. + Temporal Lag\n7.453\n11.629\n+ 56.8%\n\n\n6. Refined\n7.475\n11.662\n+ 56.7%\n\n\n\n\n\n\n\n\n\nConclusion\n\n\nContext Matters. Adding environmental variables and temporal lags reduced prediction error by 56.7%."
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#where-does-the-model-fail-residuals",
    "href": "Final/Slides/Slides/Simple_context.html#where-does-the-model-fail-residuals",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "WHERE DOES THE MODEL FAIL? (RESIDUALS)",
    "text": "WHERE DOES THE MODEL FAIL? (RESIDUALS)\n\n\n\n\n\n\n\n\nRED ZONES(UNDER-PREDICTED)\n\n\n\nModel says ‚ÄúSafe,‚Äù Reality is ‚ÄúDangerous.‚Äù\n\n\nRisk: Public safety gaps due to Insufficient Patrols.\n\n\nAction: Needs Human Intelligence.\n\n\n\n\n\nBLUE ZONES(OVER-PREDICTED)\n\n\n\nModel says ‚ÄúDangerous,‚Äù Reality is ‚ÄúSafe.‚Äù\n\n\nRisk: Potential for Over-policing in minority neighborhoods.\n\n\nAction: Do not deploy without verification."
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#section-3",
    "href": "Final/Slides/Slides/Simple_context.html#section-3",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "",
    "text": "Chapter 4. The solution\n\n\nactionable intelligence"
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#top-50-overunder-policed-map",
    "href": "Final/Slides/Slides/Simple_context.html#top-50-overunder-policed-map",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "Top 50 Over/Under-Policed Map",
    "text": "Top 50 Over/Under-Policed Map\n\n\n\n\n\n\n\n\n\n\nüö´ The ‚ÄúShotgun‚Äù Approach Fails\n\n\nSpreading officers evenly across all high-ridership stops wastes resources on safe stations, while leaving true ‚ÄúHigh-risk Anomalies‚Äù unguarded.  We must distinguish between these scenarios to redeploy effectively."
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#implementation",
    "href": "Final/Slides/Slides/Simple_context.html#implementation",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "Implementation",
    "text": "Implementation\n\n\n\n\nüöÄ IMPLEMENTATION\n\n\n\nDashboard Integration: Embed ‚ÄúTop 50 Risk Map‚Äù into HQ command centers. Weekly refresh.\n\n\nFriday Rostering Guide: Commanders use the model to shift weekend resources from Commuter Lines to Nightlife Districts.\n\n\nLow Cost Feasibility: Uses 100% existing data (SEPTA, Crime, Census). No new sensors required.\n\n\n\n\n\n\nüõ°Ô∏è SAFEGUARDS\n\n\n\n1. Graded Response: Don‚Äôt just send SWAT. Deploy Safety Ambassadors or fix street lights (CPTED) in high-risk zones.\n\n\n2. Human-in-the-Loop: The model is a guide, not a commander. District Captains must validate Red Zones with field intel.\n\n\n3. Bias Auditing: Quarterly checks on Blue Zones to prevent stereotyping low-income areas."
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#section-4",
    "href": "Final/Slides/Slides/Simple_context.html#section-4",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "",
    "text": "Chapter 5. Critical Analysis\n\n\nEquity & Bias"
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#limitations-equity",
    "href": "Final/Slides/Slides/Simple_context.html#limitations-equity",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "LIMITATIONS & EQUITY",
    "text": "LIMITATIONS & EQUITY\n\n\n\n\n‚ö†Ô∏è MODEL LIMITATIONS\n\n\n\nUnobserved Variables: The model cannot capture real-time shocks like temporary gang activity or large events (e.g., concerts).\n\n\nSpatial Lag: Crime is fluid. Our 400m buffer is static, potentially missing cross-boundary crime displacement or spillover effects.\n\n\n\n\n\n\n‚öñÔ∏è ALGORITHMIC BIAS\n\n\n\nThe Risk (Over-Policing): Reliance on Poverty Rate or Vacancy creates a feedback loop, targeting low-income neighborhoods even when no crime occurs.\n\n\nThe Evidence (Blue Zones): Our Residual Analysis confirmed this: The model Over-Predicted risk in specific low-income areas, flagging them as dangerous when they were actually safe."
  },
  {
    "objectID": "Final/Slides/Slides/Simple_context.html#section-5",
    "href": "Final/Slides/Slides/Simple_context.html#section-5",
    "title": "Safe Passage: Optimizing SEPTA Police Deployment",
    "section": "",
    "text": "Thank You\n\n\nQuestions?\n\n\nTeam:\n\n\nXinyuan Cui | Yuqing Yang | Jinyang Xu"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you‚Äôll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr.¬†Delmelle‚Äôs sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let‚Äôs personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (‚úèÔ∏è) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick ‚ÄúCommit changes‚Äù at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (‚úèÔ∏è) to edit\nUpdate the ‚ÄúAbout Me‚Äù section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you‚Äôre taking this course\n\nClick ‚ÄúCommit changes‚Äù\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (‚úèÔ∏è) to edit\nFill in your notes from the first class\nClick ‚ÄúCommit changes‚Äù\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the ‚ÄúSettings‚Äù tab at the top of your repository\nFind Pages: Scroll down and click ‚ÄúPages‚Äù in the left sidebar\nConfigure Source:\n\nSource: Select ‚ÄúDeploy from a branch‚Äù\nBranch: Select ‚Äúmain‚Äù\nFolder: Select ‚Äú/ docs‚Äù\n\nSave: Click ‚ÄúSave‚Äù\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you‚Äôll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you‚Äôll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected ‚Äúmain‚Äù branch and ‚Äú/docs‚Äù folder\n\n\n\n\n\nCheck permissions: Make sure you‚Äôre in YOUR repository, not the template\nSign in: Ensure you‚Äôre signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon‚Äôt panic! Every change is tracked in Git\nSee history: Click the ‚ÄúHistory‚Äù button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you‚Äôve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don‚Äôt struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you‚Äôll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr.¬†Delmelle‚Äôs sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let‚Äôs personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (‚úèÔ∏è) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick ‚ÄúCommit changes‚Äù at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (‚úèÔ∏è) to edit\nUpdate the ‚ÄúAbout Me‚Äù section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you‚Äôre taking this course\n\nClick ‚ÄúCommit changes‚Äù\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (‚úèÔ∏è) to edit\nFill in your notes from the first class\nClick ‚ÄúCommit changes‚Äù\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the ‚ÄúSettings‚Äù tab at the top of your repository\nFind Pages: Scroll down and click ‚ÄúPages‚Äù in the left sidebar\nConfigure Source:\n\nSource: Select ‚ÄúDeploy from a branch‚Äù\nBranch: Select ‚Äúmain‚Äù\nFolder: Select ‚Äú/ docs‚Äù\n\nSave: Click ‚ÄúSave‚Äù\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you‚Äôll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you‚Äôll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected ‚Äúmain‚Äù branch and ‚Äú/docs‚Äù folder\n\n\n\n\n\nCheck permissions: Make sure you‚Äôre in YOUR repository, not the template\nSign in: Ensure you‚Äôre signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon‚Äôt panic! Every change is tracked in Git\nSee history: Click the ‚ÄúHistory‚Äù button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you‚Äôve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don‚Äôt struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes - Data Wrangling with tidyverse",
    "section": "",
    "text": "Data wrangling as a core skill in public policy analytics\nStructure of tidy (long-format) data\nImportance of clear variable definitions and data provenance\nThinking critically about what data represents before analysis"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 Notes - Data Wrangling with tidyverse",
    "section": "",
    "text": "Data wrangling as a core skill in public policy analytics\nStructure of tidy (long-format) data\nImportance of clear variable definitions and data provenance\nThinking critically about what data represents before analysis"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes - Data Wrangling with tidyverse",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nCore dplyr verbs: select(), filter(), mutate(), arrange()\nGrouped operations using group_by() and summarize()\nUsing pipes (%&gt;%) to build readable workflows\nInspecting data with glimpse() and count()"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes - Data Wrangling with tidyverse",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhen to reshape data versus keeping it wide\nChoosing appropriate summary statistics for policy analysis\nAvoiding accidental data loss during filtering and grouping"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes - Data Wrangling with tidyverse",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nData cleaning decisions directly affect policy conclusions\nAggregation choices can hide or reveal inequities\nTransparent data transformations are essential for accountability"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes - Data Wrangling with tidyverse",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: how small data-wrangling choices can change results\nWriting readable pipelines makes analysis easier to audit and explain\nI need more practice chaining multiple transformations confidently"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes - Introduction to Regression for Policy Analysis",
    "section": "",
    "text": "Regression as a tool for understanding relationships between variables\nDifference between descriptive analysis and model-based inference\nInterpretation of coefficients in a policy context\nRole of assumptions in linear regression models"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "title": "Week 4 Notes - Introduction to Regression for Policy Analysis",
    "section": "",
    "text": "Regression as a tool for understanding relationships between variables\nDifference between descriptive analysis and model-based inference\nInterpretation of coefficients in a policy context\nRole of assumptions in linear regression models"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniques",
    "href": "weekly-notes/week-04-notes.html#coding-techniques",
    "title": "Week 4 Notes - Introduction to Regression for Policy Analysis",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nFitting basic linear regression models in R\nUsing formula syntax to specify relationships\nInspecting model output and key summary statistics\nSeparating model estimation from interpretation"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 4 Notes - Introduction to Regression for Policy Analysis",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nHow to interpret coefficients beyond statistical significance\nUnderstanding the implications of violated model assumptions\nDistinguishing correlation from causal claims in regression results"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 4 Notes - Introduction to Regression for Policy Analysis",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nRegression results are often used to justify policy decisions\nModel assumptions can influence policy conclusions if not examined\nCoefficient interpretation must consider real-world context and limitations"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 4 Notes - Introduction to Regression for Policy Analysis",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: regression framed as an analytical aid, not a source of truth\nThis week highlighted the risk of over-interpreting model outputs\nI need to be cautious about how regression results are communicated in policy settings"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 6 Notes - Classification and Logistic Regression",
    "section": "",
    "text": "Difference between regression for continuous outcomes and classification for binary outcomes\nLogistic regression as a commonly used classification model in policy analysis\nInterpreting probabilities rather than predicted values\nRelationship between model output and decision thresholds"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-06-notes.html#key-concepts-learned",
    "title": "Week 6 Notes - Classification and Logistic Regression",
    "section": "",
    "text": "Difference between regression for continuous outcomes and classification for binary outcomes\nLogistic regression as a commonly used classification model in policy analysis\nInterpreting probabilities rather than predicted values\nRelationship between model output and decision thresholds"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#coding-techniques",
    "href": "weekly-notes/week-06-notes.html#coding-techniques",
    "title": "Week 6 Notes - Classification and Logistic Regression",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nFitting logistic regression models in R\nSpecifying binary outcomes using formula syntax\nInterpreting model summaries in terms of direction and magnitude\nGenerating predicted probabilities from fitted models"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#questions-challenges",
    "href": "weekly-notes/week-06-notes.html#questions-challenges",
    "title": "Week 6 Notes - Classification and Logistic Regression",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nHow to choose appropriate probability thresholds for classification\nInterpreting coefficients in a non-linear model\nUnderstanding how classification errors affect conclusions"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#connections-to-policy",
    "href": "weekly-notes/week-06-notes.html#connections-to-policy",
    "title": "Week 6 Notes - Classification and Logistic Regression",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nClassification models are often used for eligibility, risk, or targeting decisions\nThreshold choices can have real consequences for who is included or excluded\nModel uncertainty and error rates are especially important in policy contexts"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#reflection",
    "href": "weekly-notes/week-06-notes.html#reflection",
    "title": "Week 6 Notes - Classification and Logistic Regression",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: framing classification as a decision-support tool rather than a definitive answer\nThis week highlighted the ethical implications of binary policy decisions\nI need to be cautious about how model outputs translate into real-world actions"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html",
    "href": "weekly-notes/week-09-notes.html",
    "title": "Week 9 Notes - Fairness, Bias, and Ethics in Policy Models",
    "section": "",
    "text": "Recognition that models are not neutral tools\nSources of bias in data, modeling choices, and evaluation\nDistinction between technical performance and fairness considerations\nEthical responsibilities in policy-oriented data science"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-09-notes.html#key-concepts-learned",
    "title": "Week 9 Notes - Fairness, Bias, and Ethics in Policy Models",
    "section": "",
    "text": "Recognition that models are not neutral tools\nSources of bias in data, modeling choices, and evaluation\nDistinction between technical performance and fairness considerations\nEthical responsibilities in policy-oriented data science"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#coding-techniques",
    "href": "weekly-notes/week-09-notes.html#coding-techniques",
    "title": "Week 9 Notes - Fairness, Bias, and Ethics in Policy Models",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nExamining model outcomes across different groups\nComparing performance or errors by subgroup\nUsing analytical outputs to surface disparities\nTreating fairness analysis as part of the modeling workflow"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#questions-challenges",
    "href": "weekly-notes/week-09-notes.html#questions-challenges",
    "title": "Week 9 Notes - Fairness, Bias, and Ethics in Policy Models",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nHow to define fairness in different policy contexts\nTension between model accuracy and equity considerations\nDeciding which disparities are acceptable or actionable"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#connections-to-policy",
    "href": "weekly-notes/week-09-notes.html#connections-to-policy",
    "title": "Week 9 Notes - Fairness, Bias, and Ethics in Policy Models",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nPolicy models can reinforce or mitigate existing inequalities\nFairness concerns are central when models affect access, eligibility, or resources\nTransparency around bias is critical for public trust and accountability"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#reflection",
    "href": "weekly-notes/week-09-notes.html#reflection",
    "title": "Week 9 Notes - Fairness, Bias, and Ethics in Policy Models",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: framing fairness as a policy question, not just a technical one\nThis week emphasized that analytical choices carry moral and social weight\nI need to be more explicit about values when interpreting model results"
  },
  {
    "objectID": "weekly-notes/week-11-notes.html",
    "href": "weekly-notes/week-11-notes.html",
    "title": "Week 11 Notes - Model Limitations and Responsible Use",
    "section": "",
    "text": "Recognition of inherent limitations in policy models\nDifference between model usefulness and model completeness\nImportance of stating assumptions, scope, and constraints\nUnderstanding models as partial representations of reality"
  },
  {
    "objectID": "weekly-notes/week-11-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-11-notes.html#key-concepts-learned",
    "title": "Week 11 Notes - Model Limitations and Responsible Use",
    "section": "",
    "text": "Recognition of inherent limitations in policy models\nDifference between model usefulness and model completeness\nImportance of stating assumptions, scope, and constraints\nUnderstanding models as partial representations of reality"
  },
  {
    "objectID": "weekly-notes/week-11-notes.html#coding-techniques",
    "href": "weekly-notes/week-11-notes.html#coding-techniques",
    "title": "Week 11 Notes - Model Limitations and Responsible Use",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nReviewing model outputs with attention to uncertainty\nDocumenting assumptions and known limitations\nUsing sensitivity or alternative specifications to test robustness\nTreating limitations as part of the analytical narrative"
  },
  {
    "objectID": "weekly-notes/week-11-notes.html#questions-challenges",
    "href": "weekly-notes/week-11-notes.html#questions-challenges",
    "title": "Week 11 Notes - Model Limitations and Responsible Use",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nHow much limitation disclosure is sufficient in policy work\nBalancing model complexity with interpretability\nDeciding when a model should not be used for decision-making"
  },
  {
    "objectID": "weekly-notes/week-11-notes.html#connections-to-policy",
    "href": "weekly-notes/week-11-notes.html#connections-to-policy",
    "title": "Week 11 Notes - Model Limitations and Responsible Use",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nPolicy misuse can occur when model limits are ignored or minimized\nExplicitly stating limitations supports responsible governance\nDecision-makers need clarity on what models can and cannot inform"
  },
  {
    "objectID": "weekly-notes/week-11-notes.html#reflection",
    "href": "weekly-notes/week-11-notes.html#reflection",
    "title": "Week 11 Notes - Model Limitations and Responsible Use",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: limits framed as an ethical responsibility, not a weakness\nThis week reinforced the idea that restraint is part of good analysis\nI need to be more explicit about uncertainty and scope in my work"
  }
]