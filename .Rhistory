transit_count+
avg_past_price_density+      #Spatial
sqrt(crime_count) +
log(nearest_hospital_knn3),
data = opa_census_all,
weight=weight_mix
)
summary(model_3)
model_4 <- lm(log(sale_price_predicted) ~
log(total_livable_area)  +   #Structural
number_of_bathrooms +
house_age_c +
house_age_c2 +
interior_condition +
quality_grade_num +
fireplaces +
garage_spaces +
central_air_dummy +
central_air_missing+
income_scaled +              #Census
ba_rate +
unemployment_rate +
transit_count+
avg_past_price_density+      #Spatial
sqrt(crime_count) +
log(nearest_hospital_knn3)+
(interior_condition * income_scaled)+  #FE & Interaction
factor(zip_code),
data = opa_census_all,
weight=weight_mix
)
summary(model_4)
vif(model_4)
opa_census_2_clean <- opa_census_2
models <- list(model_1, model_2, model_3, model_4)
model_names <- c("Model 1", "Model 2", "Model 3", "Model 4")
options(scipen = 999)
all_pred_usd <- c()
for (m in models) {
pred_log_tmp <- predict(m, newdata = opa_census_2_clean)
smearing_tmp <- mean(exp(residuals(m)), na.rm = TRUE)
pred_usd_tmp <- exp(pred_log_tmp) * smearing_tmp
all_pred_usd <- c(all_pred_usd, pred_usd_tmp)
}
actual_usd <- opa_census_2_clean$sale_price_predicted
actual_k <- actual_usd / 1000
pred_all_k <- all_pred_usd / 1000
x_min <- min(actual_k, pred_all_k, na.rm = TRUE)
x_max <- 8000               # manually fix max x at 8000K
y_min <- min(actual_k, pred_all_k, na.rm = TRUE)
y_max <- max(actual_k, pred_all_k, na.rm = TRUE)
x_ticks <- pretty(c(x_min, x_max), n = 6)
y_ticks <- pretty(c(y_min, y_max), n = 6)
# Loop
for (i in seq_along(models)) {
model <- models[[i]]
model_name <- model_names[i]
#  Predict on the same validation dataset
pred_log <- predict(model, newdata = opa_census_2_clean)
# Smearing correction (to restore to USD scale)
smearing_factor <- mean(exp(residuals(model)), na.rm = TRUE)
pred_usd <- exp(pred_log) * smearing_factor
pred_k <- pred_usd / 1000   # Convert to thousand dollars
rmse <- sqrt(mean((pred_usd - opa_census_2_clean$sale_price_predicted)^2, na.rm = TRUE))
rmse_norm <- rmse / mean(opa_census_2_clean$sale_price_predicted, na.rm = TRUE)
cat("\n============================\n")
cat(model_name, "\n")
cat("RMSE (USD):", round(rmse, 2), "\n")
cat("Normalized RMSE:", round(rmse_norm, 4), "\n")
cat("============================\n")
plot(
actual_k, pred_k,
xlab = "Actual Price ($K)",
ylab = "Predicted Price ($K)",
main = paste(model_name, "- Predicted vs Actual Sale Price"),
pch = 19,
col = rgb(0.2, 0.4, 0.6, 0.4),
xlim = c(x_min, x_max),
ylim = c(y_min, y_max),
axes = FALSE
)
axis(1, at = x_ticks, labels = paste0(x_ticks, "K"))
axis(2, at = y_ticks, labels = paste0(y_ticks, "K"), las = 1)
box()
# Add 45-degree line
abline(0, 1, col = "red", lwd = 2)
#Save as PNG with same scale
png_filename <- paste0("pred_actual_", gsub(" ", "_", tolower(model_name)), ".png")
png(png_filename, width = 900, height = 800)
par(mar = c(5, 5, 4, 2))
plot(
actual_k, pred_k,
xlab = "Actual Price ($K)",
ylab = "Predicted Price ($K)",
main = paste(model_name, "- Predicted vs Actual Sale Price"),
pch = 19,
col = rgb(0.2, 0.4, 0.6, 0.4),
xlim = c(x_min, x_max),
ylim = c(y_min, y_max),
axes = FALSE
)
axis(1, at = x_ticks, labels = paste0(x_ticks),cex.axis = 0.8)
axis(2, at = y_ticks, labels = paste0(y_ticks), las = 1,cex.axis = 0.8)
box()
abline(0, 1, col = "red", lwd = 2)
dev.off()
}
#Since data have different weights, we need to define a new 10-fold cross-validation model.
set.seed(123)
k <- 10
# Shuffle dataset rows
opa_census_all <- opa_census_all %>%
slice_sample(prop = 1) %>%  # randomly reorder all rows
mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs
#Initialize result vectors
rmse_usd_vec <- numeric(k)
r2_log_vec <- numeric(k)
rmse_log_vec <- numeric(k)
mae_usd_vec <- numeric(k)
#Perform k-fold cross-validation
for (i in 1:k) {
# Split training / validation sets
train <- opa_census_all %>% filter(fold_id != i)
test_raw <- opa_census_all %>% filter(fold_id == i)
# âœ… Very important! Ensure the 1/10 test data only include real market transactions
test <- test_raw %>% filter(non_market != 1)
if (nrow(test) < 10) {
cat("Skipping fold", i, ": too few validation samples after cleaning.\n")
next
}
# Weighted linear regression
model_i <- lm(log(sale_price_predicted) ~
log(total_livable_area)  +
number_of_bathrooms +
house_age_c +
house_age_c2 +
interior_condition +
quality_grade_num +
fireplaces +
garage_spaces +
central_air_dummy +
central_air_missing,
data = train,
weights = train$weight_mix
)
# Predict in log scale
test$pred_log <- predict(model_i, newdata = test)
# Compute RÂ²
actual_log <- log(test$sale_price_predicted)
ss_res <- sum((actual_log - test$pred_log)^2, na.rm = TRUE)
ss_tot <- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)
r2_log_vec[i] <- 1 - ss_res / ss_tot
# RMSE (log)
rmse_log_vec[i] <- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))
# Compute RMSE in USD (Duan smearing correction)
smearing_factor <- mean(exp(residuals(model_i)), na.rm = TRUE)
test$pred_usd <- exp(test$pred_log) * smearing_factor
rmse_usd_vec[i] <- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))
mae_usd_vec[i] <- mean(abs(test$sale_price_predicted - test$pred_usd), na.rm = TRUE)
}
# Summarize results
cat("\n====================================\n")
cat("MODEL_1\n")
cat("ðŸ’µ Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n")
cat("Average RMSE (log):", round(mean(rmse_log_vec, na.rm = TRUE), 4), "\n")
cat("Average RMSE (USD):", round(mean(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("RMSE Std. Dev (USD):", round(sd(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("Average RÂ²:", round(mean(r2_log_vec, na.rm = TRUE), 4), "\n")
cat("Average MAE (USD):", round(mean(mae_usd_vec, na.rm = TRUE), 2), "\n")
cat("====================================\n")
# Optional: view per-fold results
cv_results <- data.frame(
Fold = 1:k,
RMSE = round(rmse_log_vec, 2),
RMSE_USD = round(rmse_usd_vec, 2),
R2 = round(r2_log_vec, 4),
MAE_USD = round(mae_usd_vec, 2)
)
print(cv_results)
#Since data have different weights, we need to define a new 10-fold cross-validation model.
set.seed(234)
k <- 10
# Shuffle dataset rows
opa_census_all <- opa_census_all %>%
slice_sample(prop = 1) %>%  # randomly reorder all rows
mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs
#Initialize result vectors
rmse_usd_vec <- numeric(k)
r2_log_vec <- numeric(k)
rmse_log_vec <- numeric(k)
mae_usd_vec <- numeric(k)
#Perform k-fold cross-validation
for (i in 1:k) {
# Split training / validation sets
train <- opa_census_all %>% filter(fold_id != i)
test_raw <- opa_census_all %>% filter(fold_id == i)
# âœ… Very important! Ensure the 1/10 test data only include real market transactions
test <- test_raw %>% filter(non_market != 1)
if (nrow(test) < 10) {
cat("Skipping fold", i, ": too few validation samples after cleaning.\n")
next
}
# Weighted linear regression
model_i <- lm(log(sale_price_predicted) ~
log(total_livable_area)  +
number_of_bathrooms +
house_age_c +
house_age_c2 +
interior_condition +
quality_grade_num +
fireplaces +
garage_spaces +
central_air_dummy +
central_air_missing+
income_scaled +
ba_rate +
unemployment_rate,
data = train,
weights = train$weight_mix
)
# Predict in log scale
test$pred_log <- predict(model_i, newdata = test)
# Compute RÂ²
actual_log <- log(test$sale_price_predicted)
ss_res <- sum((actual_log - test$pred_log)^2, na.rm = TRUE)
ss_tot <- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)
r2_log_vec[i] <- 1 - ss_res / ss_tot
# RMSE (log)
rmse_log_vec[i] <- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))
# Compute RMSE in USD (Duan smearing correction)
smearing_factor <- mean(exp(residuals(model_i)), na.rm = TRUE)
test$pred_usd <- exp(test$pred_log) * smearing_factor
rmse_usd_vec[i] <- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))
mae_usd_vec[i] <- mean(abs(test$sale_price_predicted - test$pred_usd), na.rm = TRUE)
}
# Summarize results
cat("\n====================================\n")
cat("MODEL_2\n")
cat("ðŸ’µ Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n")
cat("Average RMSE (log):", round(mean(rmse_log_vec, na.rm = TRUE), 4), "\n")
cat("Average RMSE (USD):", round(mean(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("RMSE Std. Dev (USD):", round(sd(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("Average RÂ²:", round(mean(r2_log_vec, na.rm = TRUE), 4), "\n")
cat("Average MAE (USD):", round(mean(mae_usd_vec, na.rm = TRUE), 2), "\n")
cat("====================================\n")
# Optional: view per-fold results
cv_results <- data.frame(
Fold = 1:k,
RMSE = round(rmse_log_vec, 2),
RMSE_USD = round(rmse_usd_vec, 2),
R2 = round(r2_log_vec, 4),
MAE_USD = round(mae_usd_vec, 2)
)
print(cv_results)
#Since data have different weights, we need to define a new 10-fold cross-validation model.
set.seed(345)
k <- 10
# Shuffle dataset rows
opa_census_all <- opa_census_all %>%
slice_sample(prop = 1) %>%  # randomly reorder all rows
mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs
#Initialize result vectors
rmse_usd_vec <- numeric(k)
r2_log_vec <- numeric(k)
rmse_log_vec <- numeric(k)
mae_usd_vec <- numeric(k)
#Perform k-fold cross-validation
for (i in 1:k) {
# Split training / validation sets
train <- opa_census_all %>% filter(fold_id != i)
test_raw <- opa_census_all %>% filter(fold_id == i)
# âœ… Very important! Ensure the 1/10 test data only include real market transactions
test <- test_raw %>% filter(non_market != 1)
if (nrow(test) < 10) {
cat("Skipping fold", i, ": too few validation samples after cleaning.\n")
next
}
# Weighted linear regression
model_i <- lm(log(sale_price_predicted) ~
log(total_livable_area)  +
number_of_bathrooms +
house_age_c +
house_age_c2 +
interior_condition +
quality_grade_num +
fireplaces +
garage_spaces +
central_air_dummy +
central_air_missing+
income_scaled +
ba_rate +
unemployment_rate+
transit_count+
avg_past_price_density+
sqrt(crime_count) +
log(nearest_hospital_knn3),
data = train,
weights = train$weight_mix
)
# Predict in log scale
test$pred_log <- predict(model_i, newdata = test)
# Compute RÂ²
actual_log <- log(test$sale_price_predicted)
ss_res <- sum((actual_log - test$pred_log)^2, na.rm = TRUE)
ss_tot <- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)
r2_log_vec[i] <- 1 - ss_res / ss_tot
# RMSE (log)
rmse_log_vec[i] <- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))
# Compute RMSE in USD (Duan smearing correction)
smearing_factor <- mean(exp(residuals(model_i)), na.rm = TRUE)
test$pred_usd <- exp(test$pred_log) * smearing_factor
rmse_usd_vec[i] <- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))
mae_usd_vec[i] <- mean(abs(test$sale_price_predicted - test$pred_usd), na.rm = TRUE)
}
# Summarize results
cat("\n====================================\n")
cat("MODEL_3\n")
cat("ðŸ’µ Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n")
cat("Average RMSE (log):", round(mean(rmse_log_vec, na.rm = TRUE), 4), "\n")
cat("Average RMSE (USD):", round(mean(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("RMSE Std. Dev (USD):", round(sd(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("Average RÂ²:", round(mean(r2_log_vec, na.rm = TRUE), 4), "\n")
cat("Average MAE (USD):", round(mean(mae_usd_vec, na.rm = TRUE), 2), "\n")
cat("====================================\n")
# Optional: view per-fold results
cv_results <- data.frame(
Fold = 1:k,
RMSE = round(rmse_log_vec, 2),
RMSE_USD = round(rmse_usd_vec, 2),
R2 = round(r2_log_vec, 4),
MAE_USD = round(mae_usd_vec, 2)
)
print(cv_results)
#Since data have different weights, we need to define a new 10-fold cross-validation model.
set.seed(456)
k <- 10
# Shuffle dataset rows
opa_census_all <- opa_census_all %>%
slice_sample(prop = 1) %>%  # randomly reorder all rows
mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs
#Initialize result vectors
rmse_usd_vec <- numeric(k)
r2_log_vec <- numeric(k)
rmse_log_vec <- numeric(k)
mae_usd_vec <- numeric(k)
#Perform k-fold cross-validation
for (i in 1:k) {
# Split training / validation sets
train <- opa_census_all %>% filter(fold_id != i)
test_raw <- opa_census_all %>% filter(fold_id == i)
# âœ… Very important! Ensure the 1/10 test data only include real market transactions
test <- test_raw %>% filter(non_market != 1)
if (nrow(test) < 10) {
cat("Skipping fold", i, ": too few validation samples after cleaning.\n")
next
}
# Weighted linear regression
model_i <- lm(log(sale_price_predicted) ~
log(total_livable_area)  +
number_of_bathrooms +
house_age_c +
house_age_c2 +
interior_condition +
quality_grade_num +
fireplaces +
garage_spaces +
central_air_dummy +
central_air_missing+
income_scaled +
ba_rate +
unemployment_rate+
transit_count+
avg_past_price_density+
sqrt(crime_count) +
log(nearest_hospital_knn3)+
(interior_condition * income_scaled)+
factor(zip_code),
data = train,
weights = train$weight_mix
)
# Predict in log scale
test$pred_log <- predict(model_i, newdata = test)
# Compute RÂ²
actual_log <- log(test$sale_price_predicted)
ss_res <- sum((actual_log - test$pred_log)^2, na.rm = TRUE)
ss_tot <- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)
r2_log_vec[i] <- 1 - ss_res / ss_tot
# RMSE (log)
rmse_log_vec[i] <- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))
# Compute RMSE in USD (Duan smearing correction)
smearing_factor <- mean(exp(residuals(model_i)), na.rm = TRUE)
test$pred_usd <- exp(test$pred_log) * smearing_factor
rmse_usd_vec[i] <- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))
mae_usd_vec[i] <- mean(abs(test$sale_price_predicted - test$pred_usd), na.rm = TRUE)
}
# Summarize results
cat("\n====================================\n")
cat("MODEL_4\n")
cat("ðŸ’µ Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n")
cat("Average RMSE (log):", round(mean(rmse_log_vec, na.rm = TRUE), 4), "\n")
cat("Average RMSE (USD):", round(mean(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("RMSE Std. Dev (USD):", round(sd(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("Average RÂ²:", round(mean(r2_log_vec, na.rm = TRUE), 4), "\n")
cat("Average MAE (USD):", round(mean(mae_usd_vec, na.rm = TRUE), 2), "\n")
cat("====================================\n")
# Optional: view per-fold results
cv_results <- data.frame(
Fold = 1:k,
RMSE = round(rmse_log_vec, 2),
RMSE_USD = round(rmse_usd_vec, 2),
R2 = round(r2_log_vec, 4),
MAE_USD = round(mae_usd_vec, 2)
)
print(cv_results)
model_data <- data.frame(
Fitted = fitted(model_4),
Residuals = resid(model_4)
)
p_resid_fitted <- ggplot(model_data, aes(x = Fitted, y = Residuals)) +
geom_point(alpha = 0.5, color = "#6A1B9A", size = 2) +
geom_hline(yintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
geom_smooth(method = "loess", color = "black", se = FALSE, linewidth = 0.8) +
labs(
title = "Residuals vs Fitted Values",
subtitle = "Checking linearity and homoscedasticity for Model 4",
x = "Fitted Values (Log(Sale Price))",
y = "Residuals"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(face = "bold", size = 16),
plot.subtitle = element_text(size = 13, color = "gray40"),
axis.title = element_text(face = "bold"),
panel.grid.minor = element_blank()
)
p_resid_fitted
library(dplyr)
library(ggplot2)
resid_full <- rep(NA, nrow(opa_census_all))
resid_full[-as.numeric(model_4$na.action)] <- resid(model_4)
opa_census_all$residuals <- resid_full
tract_resid <- opa_census_all %>%
st_drop_geometry() %>%
group_by(GEOID) %>%
summarise(mean_residual = mean(residuals, na.rm = TRUE))
tract_map <- philly_census %>%
left_join(tract_resid, by = "GEOID")
ggplot() +
geom_sf(data = tract_map, aes(fill = mean_residual), color = "white", size = 0.2) +
scale_fill_gradient2(
low = "#6A1B9A", mid = "white", high = "#FFB300",
midpoint = 0,
limits = c(-0.5, 0.5),
name = "Mean Log Residual",
breaks = c(-0.3, 0, 0.3),
labels = c("Overestimated", "Accurate", "Underestimated"),
na.value = "grey60"
) +
theme_minimal(base_size = 16) +
labs(
title = "Hardest to Predict Neighborhoods in Philadelphia",
subtitle = "Yellow = underestimation | Purple = overestimation"
) +
theme(
panel.grid = element_blank(),
axis.text = element_blank(),
legend.position = "right"
)
p_qq <- ggplot(model_data, aes(sample = Residuals)) +
stat_qq(color = "#6A1B9A", size = 2, alpha = 0.6) +
stat_qq_line(color = "red",linetype = "dashed", linewidth = 1) +
labs(
title = "Normal Q-Q Plot",
subtitle = "Checking normality of residuals for Model 4",
x = "Theoretical Quantiles",
y = "Sample Quantiles"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(face = "bold", size = 16),
plot.subtitle = element_text(size = 13, color = "gray40"),
axis.title = element_text(face = "bold"),
panel.grid.minor = element_blank()
)
p_qq
cooks_d <- cooks.distance(model_4)
model_data <- data.frame(
Index = 1:length(cooks_d),
CooksD = cooks_d
)
threshold <- 4 / nrow(model_4$model)
p_cook <- ggplot(model_data, aes(x = Index, y = CooksD)) +
geom_segment(aes(xend = Index, yend = 0), color = "#6A1B9A", alpha = 0.7) +  # vertical lines
geom_point(color = "#6A1B9A", size = 0.15) +
geom_hline(yintercept = threshold, linetype = "dashed", color = "red", linewidth = 1) +
labs(
title = "Cook's Distance",
subtitle = "Identifying influential observations for Model 4",
x = "Observation Index",
y = "Cook's Distance"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(face = "bold", size = 16),
plot.subtitle = element_text(size = 13, color = "gray40"),
axis.title = element_text(face = "bold"),
panel.grid.minor = element_blank()
)
p_cook
system("git rm -r --cached _quarto _cache _freeze .RData .RDataTmp .Rdatadmp .Rhistory", intern = TRUE)
system("git --version")
system("git --version")
