---
title: "Spatial Predictive Analysis"
subtitle: "Exploring Burglaries Patterns in Chicago and Predictive Modeling with 311 Data"
author: "Jinyang Xu"
date: 11/13/2025
format:
  html:
    code-fold: show
    code-tools: true
    toc: true
    toc-depth: 3
    toc-location: left
    theme: cosmo
    embed-resources: true
editor: visual
execute:
  warning: false
  message: false
  cache: true
---

## Assignment Overview

In this Assignment, I will build a spatial predictive model for burglaries using count regression and spatial features.I will document the process, and interpret results.

# Learning Objectives

In this project, I will:

1.  Build spatial features for predictive modeling
2.  Apply count regression techniques to spatial data
3.  Implement spatial cross-validation
4.  Interpret and communicate model results
5.  Critically evaluate model performance

# Setup

```{r setup}
#| message: false
#| warning: false

# Load required packages
library(tidyverse)      # Data manipulation
library(sf)             # Spatial operations
library(here)           # Relative file paths
library(viridis)        # Color scales
library(terra)          # Raster operations (replaces 'raster')
library(spdep)          # Spatial dependence
library(FNN)            # Fast nearest neighbors
library(MASS)           # Negative binomial regression
library(patchwork)      # Plot composition (replaces grid/gridExtra)
library(knitr)          # Tables
library(kableExtra)     # Table formatting
library(classInt)       # Classification intervals
library(here)

# Spatstat split into sub-packages
library(spatstat.geom)    # Spatial geometries
library(spatstat.explore) # Spatial exploration/KDE

# Set options
options(scipen = 999)  # No scientific notation
set.seed(5080)         # Reproducibility

# Create consistent theme for visualizations
theme_crime <- function(base_size = 11) {
  theme_minimal(base_size = base_size) +
    theme(
      plot.title = element_text(face = "bold", size = base_size + 1),
      plot.subtitle = element_text(color = "gray30", size = base_size - 1),
      legend.position = "right",
      panel.grid.minor = element_blank(),
      axis.text = element_blank(),
      axis.title = element_blank()
    )
}

# Set as default
theme_set(theme_crime())

cat("✓ All packages loaded successfully!\n")
cat("✓ Working directory:", getwd(), "\n")
```

# Part 1: Load and Explore Data

In this part, I load the datasets that will be used in the analysis, including boundaries datasets and burglaries dataset. After that, I will visualize the burglaries on the map to gain an intuitive, preliminary understanding of the spatial distribution of burglaries and where the major hotspots are located.

## Exercise 1.1: Load Chicago Spatial Data

```{r load-boundaries}
#| message: false

# Load police districts (used for spatial cross-validation)
policeDistricts <- 
  st_read("https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(District = dist_num)

# Load police beats (smaller administrative units)
policeBeats <- 
  st_read("https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(Beat = beat_num)

# Load Chicago boundary
chicagoBoundary <- 
  st_read("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson") %>%
  st_transform('ESRI:102271')

cat("✓ Loaded spatial boundaries\n")
cat("  - Police districts:", nrow(policeDistricts), "\n")
cat("  - Police beats:", nrow(policeBeats), "\n")
```

::: callout-note
## Coordinate Reference System

We're using `ESRI:102271` (Illinois State Plane East, NAD83, US Feet). This is appropriate for Chicago because:

-   It minimizes distortion in this region
-   Uses feet (common in US planning)
-   Allows accurate distance calculations
:::

## Exercise 1.2: Load Burglary Data

```{r load-burglaries}
#| message: false

# Load from provided data file (downloaded from Chicago open data portal)
burglaries <- st_read(here("labs/lab4/data", "burglaries.shp")) %>% 
  st_transform('ESRI:102271')

# Check the data
cat("\n✓ Loaded burglary data\n")
cat("  - Number of burglaries:", nrow(burglaries), "\n")
cat("  - CRS:", st_crs(burglaries)$input, "\n")
cat("  - Date range:", min(burglaries$Date, na.rm = TRUE), "to", 
    max(burglaries$Date, na.rm = TRUE), "\n")
```

**Interpretation 1.1:** There are 7482 rows of burglaries in the dataset. The time period is from 2017/01/01 to 2018/01/01, indicates that it contains 366 days of data.

It is essential that all datasets share the same Coordinate Reference System (CRS). If layers are kept in different CRS formats, spatial operations will produce incorrect or misaligned results.

For this project, because we are working in Chicago, we use the specific projected CRS "ESRI: 102271 — Illinois StatePlane East (meters)" that provides accurate distance and area measurements for Chicago.

::: callout-warning
## Critical Pause #1: Data Provenance

Before proceeding, consider where this data came from:

**Who recorded this data?** Chicago Police Department officers and detectives

**What might be missing?**

-   Unreported burglaries (victims didn't call police)
-   Incidents police chose not to record
-   Downgraded offenses (burglary recorded as trespassing)
-   Spatial bias (more patrol = more recorded crime)

**Think About** Was there a Department of Justice investigation of CPD during this period? What did they find about data practices?
:::

## Exercise 1.3: Visualize Point Data

```{r visualize-points}
#| fig-width: 10
#| fig-height: 5

# Simple point map
p1 <- ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
  geom_sf(data = burglaries, color = "#d62828", size = 0.1, alpha = 0.4) +
  labs(
    title = "Burglary Locations",
    subtitle = paste0("Chicago 2017, n = ", nrow(burglaries))
  )

# Density surface using modern syntax
p2 <- ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
  geom_density_2d_filled(
    data = data.frame(st_coordinates(burglaries)),
    aes(X, Y),
    alpha = 0.7,
    bins = 8
  ) +
  scale_fill_viridis_d(
    option = "plasma",
    direction = -1,
    guide = "none"  # Modern ggplot2 syntax (not guide = FALSE)
  ) +
  labs(
    title = "Density Surface",
    subtitle = "Kernel density estimation"
  )

# Combine plots using patchwork (modern approach)
p1 + p2 + 
  plot_annotation(
    title = "Spatial Distribution of Burglaries in Chicago",
    tag_levels = 'A'
  )
```

**Interpretation 1.2:**

The spatial pattern of burglaries in Chicago is clustered rather than evenly distributed. The point map shows that incidents concentrate in several distinct corridors rather than being spread uniformly across the city. The KDE surface reinforces this pattern with three major hotspot zones:

1.Near North / River North area 2.West Side around Humboldt Park and Logan Square 3.South Side corridor extending from Washington Park toward Englewood

These locations share certain environmental features commonly associated with elevated burglary risk. In addition, some hotspots overlap areas with documented social and economic disadvantage, vacant housing, and higher levels of physical disorder—factors linked to burglary opportunity structures in environmental criminology.

# Part 2: Create Fishnet Grid

In Part 2, I convert raw burglary point data into a consistent spatial framework by building a 500m fishnet grid. This allows us to aggregate burglary counts, explore their spatial distribution, and create standardized units for later modeling. By moving from irregular points to a regular grid, we can calculate spatial features, compare areas directly, and prepare the data for regression analysis in the next steps.

## Exercise 2.1: Understanding the Fishnet

A **fishnet grid** converts irregular point data into a regular grid of cells where we can:

-   Aggregate counts
-   Calculate spatial features
-   Apply regression models

```{r create-fishnet}
# Create 500m x 500m grid
fishnet <- st_make_grid(
  chicagoBoundary,
  cellsize = 500,  # 500 meters per cell
  square = TRUE
) %>%
  st_sf() %>%
  mutate(uniqueID = row_number())

# Keep only cells that intersect Chicago
fishnet <- fishnet[chicagoBoundary, ]

# View basic info
cat("✓ Created fishnet grid\n")
cat("  - Number of cells:", nrow(fishnet), "\n")
cat("  - Cell size:", 500, "x", 500, "meters\n")
cat("  - Cell area:", round(st_area(fishnet[1,])), "square meters\n")
```

**Interpretation 2.1:**

I use a regular grid because it gives us equal-sized, neutral spatial units instead of boundaries that were drawn for political or administrative reasons. This makes the analysis cleaner: cell areas are comparable, distance-based features are easier to compute, and we reduce some of the MAUP problems that come from highly uneven, irregular neighborhoods or census tracts.

The trade-off is that grid cells are analytically convenient but socially artificial. They do not correspond to real neighborhoods, police beats, or planning districts, so the results are harder to explain to practitioners and often need to be translated back into those official units.

In short: grids are better for modelling, official boundaries are better for communication.

## Exercise 2.2: Aggregate Burglaries to Grid

```{r aggregate-burglaries}
# Spatial join: which cell contains each burglary?
burglaries_fishnet <- st_join(burglaries, fishnet, join = st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID) %>%
  summarize(countBurglaries = n())

# Join back to fishnet (cells with 0 burglaries will be NA)
fishnet <- fishnet %>%
  left_join(burglaries_fishnet, by = "uniqueID") %>%
  mutate(countBurglaries = replace_na(countBurglaries, 0))

# Summary statistics
cat("\nBurglary count distribution:\n")
summary(fishnet$countBurglaries)
cat("\nCells with zero burglaries:", 
    sum(fishnet$countBurglaries == 0), 
    "/", nrow(fishnet),
    "(", round(100 * sum(fishnet$countBurglaries == 0) / nrow(fishnet), 1), "%)\n")
```

```{r visualize-fishnet}
#| fig-width: 8
#| fig-height: 6

# Visualize aggregated counts
ggplot() +
  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
  geom_sf(data = chicagoBoundary, fill = NA, color = "white", linewidth = 1) +
  scale_fill_viridis_c(
    name = "Burglaries",
    option = "plasma",
    trans = "sqrt",  # Square root for better visualization of skewed data
    breaks = c(0, 1, 5, 10, 20, 40)
  ) +
  labs(
    title = "Burglary Counts by Grid Cell",
    subtitle = "500m x 500m cells, Chicago 2017"
  ) +
  theme_crime()
```

**Interpretation 2.2:**

The burglary counts are skewed: a large number of grid cells have little or zero incidents, while a small number of cells have relatively high counts. This produces a classic zero-inflated and overdispersed distribution, where the variance is much larger than the mean.

So many zeros appear because burglary is a spatially clustered event. Most parts of the city experience little or no burglary activity, while a few hotspots absorb most incidents. This distribution is not well-fit by a standard Poisson model, which assumes that the mean and variance are equal. Instead, the overdispersion suggests that models like the Negative Binomial or a zero-inflated count model are more appropriate.

# Part 3: Create a Kernel Density Baseline

Part 3 builds a KDE hotspot baseline—our simplest prediction model, so I can later test whether complex regression models truly improve on “crime happens where it happened before.

Before building complex models, let's create a simple baseline using **Kernel Density Estimation (KDE)**.

**The KDE baseline asks:** "What if crime just happens where it happened before?" (simple spatial smoothing, no predictors)

```{r kde-baseline}
#| message: false

# Convert burglaries to ppp (point pattern) format for spatstat
burglaries_ppp <- as.ppp(
  st_coordinates(burglaries),
  W = as.owin(st_bbox(chicagoBoundary))
)

# Calculate KDE with 1km bandwidth
kde_burglaries <- density.ppp(
  burglaries_ppp,
  sigma = 1000,  # 1km bandwidth
  edge = TRUE    # Edge correction
)

# Convert to terra raster (modern approach, not raster::raster)
kde_raster <- rast(kde_burglaries)

# Extract KDE values to fishnet cells
fishnet <- fishnet %>%
  mutate(
    kde_value = terra::extract(
      kde_raster,
      vect(fishnet),
      fun = mean,
      na.rm = TRUE
    )[, 2]  # Extract just the values column
  )

cat("✓ Calculated KDE baseline\n")
```

```{r visualize-kde}
#| fig-width: 8
#| fig-height: 6

ggplot() +
  geom_sf(data = fishnet, aes(fill = kde_value), color = NA) +
  geom_sf(data = chicagoBoundary, fill = NA, color = "white", linewidth = 1) +
  scale_fill_viridis_c(
    name = "KDE Value",
    option = "plasma"
  ) +
  labs(
    title = "Kernel Density Estimation Baseline",
    subtitle = "Simple spatial smoothing of burglary locations"
  ) +
  theme_crime()
```

**Question 3.1:**

The KDE map smooths the burglary pattern and highlights the major hotspot corridors more clearly than the raw count map. KDE does a good job capturing the overall spatial structure, showing where clusters of incidents consistently occur even if those events fall on different sides of a grid cell. It is especially effective at revealing continuous “heat bands”.

However, KDE also misses important details. It hides the true variation inside each grid cell, over-smooths sharp boundaries, and can exaggerate hotspots depending on the bandwidth. KDE blurs the data enough that micro-hotspots and sudden transitions disappear, and it no longer reflects the exact number of burglaries in each location.

In short: KDE captures the big-picture hotspot structure very well, but it sacrifices precision and hides fine-scale variations present in the count map.

::: callout-tip
## Why Start with KDE?

The KDE represents our **null hypothesis**: burglaries happen where they happened before, with no other information.

:::

# Part 4: Create Spatial Predictor Variables

Now I'll create features that might help predict burglaries. We'll use "broken windows theory" logic: signs of disorder predict crime.

In Part 4, I create spatial predictors from 311 sanitation code complaints. I map their counts, measure how close each grid cell is to nearby complaints, and identify complaint clusters using Local Moran’s I. These variables let me test whether neighborhood disorder is associated with burglary risk.

## Exercise 4.1: Load 311 Abandoned Vehicle Calls

```{r load-abandoned-cars}
#| message: false

sanitation_coms <- read_csv("data/311_Service_Requests_-_Sanitation_Code_Complaints_-_Historical_20251113.csv")%>%
    filter(
    mdy(`Creation Date`) >= mdy("12/31/2016") &
    mdy(`Creation Date`) <  mdy("01/02/2018")
  ) %>% 
  filter(!is.na(Latitude), !is.na(Longitude)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326) %>%
  st_transform('ESRI:102271')

cat("✓ Loaded Sanitation Code Complaints\n")
cat("  - Number of calls:", nrow(sanitation_coms), "\n")
```

::: callout-note
## Data Loading Note

The data was downloaded from Chicago's Open Data Portal. You can now request an api from the Chicago portal and tap into the data there.

**Consider:** How might the 311 reporting system itself be biased? Who calls 311? What neighborhoods have better 311 awareness?

The 311 system is not a neutral measure of neighborhood conditions. Reporting depends on who knows about 311, who trusts government systems, and who feels empowered to complain. Higher-income and higher-resource neighborhoods tend to have greater 311 awareness, more digital access, and stronger expectations of government responsiveness, which leads to more reporting. In contrast, lower-income communities, immigrant neighborhoods, and historically over-policed areas may underreport due to limited awareness, language barriers, distrust of city services, or simply different norms around what is “worth” reporting.
:::

## Exercise 4.2: Count of Abandoned Cars per Cell

```{r count-abandoned-cars}
# Aggregate Sanitation Code Complaints to fishnet

sanitation_fishnet <- st_join(sanitation_coms, fishnet, join = st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID) %>%
  summarize(sanitation_coms = n())

# Join to fishnet
fishnet <- fishnet %>%
  left_join(sanitation_fishnet, by = "uniqueID") %>%
  mutate(sanitation_coms = replace_na(sanitation_coms, 0))

cat("Sanitation code complaints distribution:\n")
summary(sanitation_coms)
```

```{r visualize-sanitation-complaints}
#| fig-width: 15
#| fig-height: 8

fishnet$san_cont <- pmin(fishnet$sanitation_coms, 60)

fishnet$san_over60 <- fishnet$sanitation_coms > 60

# Sanitation code complaints
p1 <- ggplot() +
  geom_sf(
    data = fishnet,
    aes(fill = san_cont),
    color = NA
  ) +
  scale_fill_viridis_c(
    option = "plasma",     # purple → yellow
    limits = c(0, 60),     
    name = "Sanitation"
  ) +
  geom_sf(
    data = subset(fishnet, san_over60),
    fill = "#f0f921",    
    color = NA
  ) +
  labs(title = "Sanitation Code Complaints (Continuous Gradient, 60+ Highlighted)") +
  theme_crime()


# Burglaries
p2 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
  scale_fill_viridis_c(name = "Count", option = "plasma") +
  labs(title = "Burglaries") +
  theme_crime()

p1 + p2 +
  plot_annotation(
    title = "Comparison of Sanitation Complaints and Burglaries (Fixed Bins)"
  )

```

**Interpretation 4.1:**

Both maps show broadly overlapping hotspot corridors, especially in the south side. This visual overlap suggests that physical disorder and burglary risk cluster in similar environments—places with poor maintenance, dumping, or abandoned properties also show elevated burglary activity. This aligns with environmental criminology and “broken windows” dynamics: neighborhoods with more visible disorder may signal lower guardianship and create more opportunities for property crime.

In short: the two patterns are not identical, but they cluster in many of the same spaces, indicating a potential relationship worth modeling statistically.

## Exercise 4.3: Nearest Neighbor Features

Count in a cell is one measure. Distance to the nearest 3 sanitation complaints captures local context.

```{r nn-feature}
#| message: false

# Calculate mean distance to 3 nearest sanitation complaints
# (Do this OUTSIDE of mutate to avoid sf conflicts)

# Get coordinates
fishnet_coords <- st_coordinates(st_centroid(fishnet))
sanitation_coords <- st_coordinates(sanitation_coms)

# Calculate k nearest neighbors and distances
nn_result <- get.knnx(sanitation_coords, fishnet_coords, k = 3)

# Add to fishnet
fishnet <- fishnet %>%
  mutate(
    sanitation_coms.nn = rowMeans(nn_result$nn.dist)
  )

cat("✓ Calculated nearest neighbor distances\n")
summary(fishnet$sanitation_coms.nn)
```

**Interpretation 4.2:**

A low value of `sanitation_coms.nn` means that the cell is very close to several sanitation code complaints.This indicates the cell is surrounded by visible physical disorder.

A high value means the nearest sanitation complaints are far away, suggesting that the surrounding area has few sanitation-related problems or lower reporting activity.

This measure is informative because distance to sanitation complaints captures the spatial context of disorder, not just what happens inside a single grid cell. Even if a cell has zero complaints itself, being close to areas with repeated sanitation violations may still signal neighborhood sanitation problem —condition may associated with higher burglary risk.

## Exercise 4.4: Distance to Hot Spots

Let's identify clusters of sanitation complaints using Local Moran's I, then calculate distance to these hot spots.

```{r local-morans-complaints}
# Function to calculate Local Moran's I
calculate_local_morans <- function(data, variable, k = 5) {
  
  # Create spatial weights
  coords <- st_coordinates(st_centroid(data))
  neighbors <- knn2nb(knearneigh(coords, k = k))
  weights <- nb2listw(neighbors, style = "W", zero.policy = TRUE)
  
  # Calculate Local Moran's I
  local_moran <- localmoran(data[[variable]], weights)
  
  # Classify clusters
  mean_val <- mean(data[[variable]], na.rm = TRUE)
  
  data %>%
    mutate(
      local_i = local_moran[, 1],
      p_value = local_moran[, 5],
      is_significant = p_value < 0.05,
      
      moran_class = case_when(
        !is_significant ~ "Not Significant",
        local_i > 0 & .data[[variable]] > mean_val ~ "High-High",
        local_i > 0 & .data[[variable]] <= mean_val ~ "Low-Low",
        local_i < 0 & .data[[variable]] > mean_val ~ "High-Low",
        local_i < 0 & .data[[variable]] <= mean_val ~ "Low-High",
        TRUE ~ "Not Significant"
      )
    )
}

# Apply to abandoned cars
fishnet <- calculate_local_morans(fishnet, "sanitation_coms", k = 5)
```

```{r visualize-morans}
#| fig-width: 8
#| fig-height: 6

# Visualize hot spots
ggplot() +
  geom_sf(
    data = fishnet, 
    aes(fill = moran_class), 
    color = NA
  ) +
  scale_fill_manual(
    values = c(
      "High-High" = "#d7191c",
      "High-Low" = "#fdae61",
      "Low-High" = "#abd9e9",
      "Low-Low" = "#2c7bb6",
      "Not Significant" = "gray90"
    ),
    name = "Cluster Type"
  ) +
  labs(
    title = "Local Moran's I: sanitation code complatins Clusters",
    subtitle = "High-High = Hot spots of disorder"
  ) +
  theme_crime()
```

```{r distance-to-hotspots}
# Get centroids of "High-High" cells (hot spots)
hotspots <- fishnet %>%
  filter(moran_class == "High-High") %>%
  st_centroid()

# Calculate distance from each cell to nearest hot spot
if (nrow(hotspots) > 0) {
  fishnet <- fishnet %>%
    mutate(
      dist_to_hotspot = as.numeric(
        st_distance(st_centroid(fishnet), hotspots %>% st_union())
      )
    )
  
  cat("✓ Calculated distance to sanitation code comlaints hot spots\n")
  cat("  - Number of hot spot cells:", nrow(hotspots), "\n")
} else {
  fishnet <- fishnet %>%
    mutate(dist_to_hotspot = 0)
  cat("⚠ No significant hot spots found\n")
}
```

**Interpretation 4.3:**

Distance to a cluster of sanitation complaints is more informative than distance to a single complaint because clusters capture persistent, spatially concentrated disorder, not just isolated events. A single sanitation complaint could be random, one-time, or caused by a single resident. But when many nearby cells all show high sanitation complaints, it signals a structural problem in the poor sanitation maintenance.

Local Moran’s I identifies exactly these patterns by detecting spatial autocorrelation. High-High clusters indicate areas where a cell with high sanitation complaints is surrounded by other high-value cells. These are true hotspots of disorder, not random noise. Low-High areas reveal local outliers, where a relatively clean cell sits next to a disorder hotspot.

::: callout-note
**Local Moran's I** identifies:

-   **High-High**: Hot spots (high values surrounded by high values)
-   **Low-Low**: Cold spots (low values surrounded by low values)
-   **High-Low / Low-High**: Spatial outliers

This helps us understand spatial clustering patterns.
:::

------------------------------------------------------------------------

# Part 5: Join Police Districts for Cross-Validation

We'll use police districts for our spatial cross-validation.

```{r join-districts}
# Join district information to fishnet
fishnet <- st_join(
  fishnet,
  policeDistricts,
  join = st_within,
  left = TRUE
) %>%
  filter(!is.na(District))  # Remove cells outside districts！！！！！！！！！

cat("✓ Joined police districts\n")
cat("  - Districts:", length(unique(fishnet$District)), "\n")
cat("  - Cells:", nrow(fishnet), "\n")
```

# Part 6: Model Fitting

In Part 6, I fit Poisson and Negative Binomial models to explain burglary counts using disorder-related predictors. I check for overdispersion, compare AIC values, and confirm that the Negative Binomial model provides a better fit for the skewed, zero-inflated burglary data.

## Exercise 6.1: Poisson Regression

Burglary counts are count data (0, 1, 2, 3...). We'll use **Poisson regression**.

```{r prepare-data}
# Create clean modeling dataset
fishnet_model <- fishnet %>%
  st_drop_geometry() %>%
  dplyr::select(
    uniqueID,
    District,
    countBurglaries,
    sanitation_coms,
    sanitation_coms.nn,
    dist_to_hotspot
  ) %>%
  na.omit()  # Remove any remaining NAs

cat("✓ Prepared modeling data\n")
cat("  - Observations:", nrow(fishnet_model), "\n")
cat("  - Variables:", ncol(fishnet_model), "\n")
```

```{r fit-poisson}
# Fit Poisson regression
model_poisson <- glm(
  countBurglaries ~ sanitation_coms + sanitation_coms.nn + 
    dist_to_hotspot,
  data = fishnet_model,
  family = "poisson"
)

# Summary
summary(model_poisson)
```

**Interpretation 6.1:**

Interpret the coefficients.

-   sanitation_coms (positive coefficient) Estimate: +0.00265 Cells with more sanitation code complaints tend to have slightly higher burglary counts. This suggests that areas with more visible disorder (trash issues, sanitation violations) also experience more burglaries, consistent with environmental criminology.

-   sanitation_coms.nn (negative coefficient) Estimate: –0.00450 This variable measures the average distance to nearby sanitation complaints. A negative sign means: Closer to sanitation complaint clusters → more burglaries Farther away → fewer burglaries This strengthens the idea that being near clusters of disorder is more predictive of burglary risk than just having a complaint inside the cell.

-   dist_to_hotspot (negative coefficient) Estimate: –0.00015 Cells closer to the burglary hotspot have higher burglary counts. As distance increases, burglaries decline—typical spatial decay around a known hotspot.

## Exercise 6.2: Check for Overdispersion

Poisson regression assumes mean = variance. Real count data often violates this (overdispersion).

```{r check-overdispersion}
# Calculate dispersion parameter
dispersion <- sum(residuals(model_poisson, type = "pearson")^2) / 
              model_poisson$df.residual

cat("Dispersion parameter:", round(dispersion, 2), "\n")
cat("Rule of thumb: >1.5 suggests overdispersion\n")

if (dispersion > 1.5) {
  cat("⚠ Overdispersion detected! Consider Negative Binomial model.\n")
} else {
  cat("✓ Dispersion looks okay for Poisson model.\n")
}
```

## Exercise 6.3: Negative Binomial Regression

If overdispersed, use **Negative Binomial regression** (more flexible).

```{r fit-negbin}
# Fit Negative Binomial model
model_nb <- glm.nb(
  countBurglaries ~ sanitation_coms + sanitation_coms.nn + 
    dist_to_hotspot,
  data = fishnet_model
)

# Summary
summary(model_nb)

# Compare AIC (lower is better)
cat("\nModel Comparison:\n")
cat("Poisson AIC:", round(AIC(model_poisson), 1), "\n")
cat("Negative Binomial AIC:", round(AIC(model_nb), 1), "\n")
```

**Interpretation 6.2:**

model comparison

The Negative Binomial model fits better, with an AIC of 7152.6, compared to the Poisson model’s AIC of 8128.6. Lower AIC indicates a superior model, so the Negative Binomial clearly provides a better description of the burglary counts.

This tells us again that the data exhibits overdispersion: the variance is much larger than the mean, which violates the core assumption of the Poisson model. The Negative Binomial handles this extra variability by adding a dispersion parameter (theta), allowing it to model the heavy-tailed, zero-inflated structure of the burglary data more accurately.

# Part 7: Spatial Cross-Validation

Standard cross-validation randomly splits data. But with spatial data, this means training on cells right next to test cells (information leakage!). So here, I'll use Leave-One-Group-Out (LOGO) Cross-Validation to train the model.

**Leave-One-Group-Out (LOGO) Cross-Validation** trains on all districts except one, then tests on the held-out district.

```{r spatial-cv}
# Get unique districts
districts <- unique(fishnet_model$District)
cv_results <- tibble()

cat("Running LOGO Cross-Validation...\n")

for (i in seq_along(districts)) {
  
  test_district <- districts[i]
  
  # Split data
  train_data <- fishnet_model %>% filter(District != test_district)
  test_data <- fishnet_model %>% filter(District == test_district)
  
  # Fit model on training data
  model_cv <- glm.nb(
    countBurglaries ~ sanitation_coms + sanitation_coms.nn + 
      dist_to_hotspot,
    data = train_data
  )
  
  # Predict on test data
  test_data <- test_data %>%
    mutate(
      prediction = predict(model_cv, test_data, type = "response")
    )
  
  # Calculate metrics
  mae <- mean(abs(test_data$countBurglaries - test_data$prediction))
  rmse <- sqrt(mean((test_data$countBurglaries - test_data$prediction)^2))
  
  # Store results
  cv_results <- bind_rows(
    cv_results,
    tibble(
      fold = i,
      test_district = test_district,
      n_test = nrow(test_data),
      mae = mae,
      rmse = rmse
    )
  )
  
  cat("  Fold", i, "/", length(districts), "- District", test_district, 
      "- MAE:", round(mae, 2), "\n")
}

# Overall results
cat("\n✓ Cross-Validation Complete\n")
cat("Mean MAE:", round(mean(cv_results$mae), 2), "\n")
cat("Mean RMSE:", round(mean(cv_results$rmse), 2), "\n")
```

```{r cv-results-table}
# Show results
cv_results %>%
  arrange(desc(mae)) %>%
  kable(
    digits = 2,
    caption = "LOGO CV Results by District"
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

**Interpretation 7.1:**

Spatial cross-validation is more appropriate because nearby grid cells are not independent. Crime and environmental features exhibit strong spatial autocorrelation, meaning locations close to each other look more similar than locations far apart.

Random CV would mix neighboring cells into both the training and testing folds, causing information leakage, which means that the model would “cheat” by learning from cells right next to the test area, leading to overly optimistic accuracy.

Spatial CV forces the model to predict entire districts it has never seen before, which better reflects the real forecasting challenge:

District 3,8 and 11 are the 3 districts hardest to predict due to the highest MAE of 5.21, 3.57 and 2.99. These districts likely contain complex, unstable, or highly heterogeneous burglary patterns, making them harder for the model to generalize to.

::: callout-note
## Connection to previous work

This is a spatial version of the train/test splits and cross-validation used in the previous projects.

**Why it matters:** If we can only predict well in areas we've already heavily policed, what does that tell us about the model's utility?
:::

# Part 8: Model Predictions and Comparison

In Part 8, I generate final burglary predictions using the Negative Binomial model and compare them to the KDE baseline. I evaluate performance using MAE and RMSE, map the predicted surfaces, and examine where the model works well or struggles. This section shows how the statistical model and KDE differ in accuracy, behavior, and purpose—one aimed at explanation, the other at spatial smoothing.

## Exercise 8.1: Generate Final Predictions

```{r final-predictions}
# Fit final model on all data
final_model <- glm.nb(
  countBurglaries ~ sanitation_coms + sanitation_coms.nn + 
    dist_to_hotspot,
  data = fishnet_model
)

# Add predictions back to fishnet
fishnet <- fishnet %>%
  mutate(
    prediction_nb = predict(final_model, fishnet_model, type = "response")[match(uniqueID, fishnet_model$uniqueID)]
  )

# Also add KDE predictions (normalize to same scale as counts)
kde_sum <- sum(fishnet$kde_value, na.rm = TRUE)
count_sum <- sum(fishnet$countBurglaries, na.rm = TRUE)
fishnet <- fishnet %>%
  mutate(
    prediction_kde = (kde_value / kde_sum) * count_sum
  )
```

## Exercise 8.2: Compare Model vs. KDE Baseline

```{r compare-models}
#| fig-width: 12
#| fig-height: 4

# Create three maps
p1 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
  scale_fill_viridis_c(name = "Count", option = "plasma", limits = c(0, 15)) +
  labs(title = "Actual Burglaries") +
  theme_crime()

p2 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +
  scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
  labs(title = "Model Predictions (Neg. Binomial)") +
  theme_crime()

p3 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +
  scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
  labs(title = "KDE Baseline Predictions") +
  theme_crime()

p1 + p2 + p3 +
  plot_annotation(
    title = "Actual vs. Predicted Burglaries",
    subtitle = "Does our complex model outperform simple KDE?"
  )
```

```{r model-comparison-metrics}
# Calculate performance metrics
comparison <- fishnet %>%
  st_drop_geometry() %>%
  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %>%
  summarize(
    model_mae = mean(abs(countBurglaries - prediction_nb)),
    model_rmse = sqrt(mean((countBurglaries - prediction_nb)^2)),
    kde_mae = mean(abs(countBurglaries - prediction_kde)),
    kde_rmse = sqrt(mean((countBurglaries - prediction_kde)^2))
  )

comparison %>%
  pivot_longer(everything(), names_to = "metric", values_to = "value") %>%
  separate(metric, into = c("approach", "metric"), sep = "_") %>%
  pivot_wider(names_from = metric, values_from = value) %>%
  kable(
    digits = 2,
    caption = "Model Performance Comparison"
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

**Interpretation 8.1:** Does the complex model outperform the simple KDE baseline? By how much? Is the added complexity worth it?

The KDE baseline achieves slightly better predictive accuracy (MAE 2.06 vs. 2.21), but this does not mean the regression model is unnecessary or “not worth it.” The two approaches serve fundamentally different purposes.

KDE is a purely spatial smoother: it captures clustering extremely well because burglary is highly spatially dependent. Its strength lies in interpolating hotspots, not in explaining why those hotspots exist. In contrast, the regression model provides something KDE cannot: insight into the underlying mechanisms that correlate with burglary patterns. These findings are meaningful because they speak to environmental criminology and “broken windows” dynamics—relationships that KDE, by design, cannot reveal.

So while KDE edges out the model in raw predictive accuracy, the regression model is still valuable because it helps us understand the social and spatial processes behind crime, not just map it.

## Exercise 8.3: Where Does the Model Work Well?

```{r prediction-errors}
#| fig-width: 10
#| fig-height: 5

# Calculate errors
fishnet <- fishnet %>%
  mutate(
    error_nb = countBurglaries - prediction_nb,
    error_kde = countBurglaries - prediction_kde,
    abs_error_nb = abs(error_nb),
    abs_error_kde = abs(error_kde)
  )

# Map errors
p1 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +
  scale_fill_gradient2(
    name = "Error",
    low = "#2166ac", mid = "white", high = "#b2182b",
    midpoint = 0,
    limits = c(-10, 10)
  ) +
  labs(title = "Model Errors (Actual - Predicted)") +
  theme_crime()

p2 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +
  scale_fill_viridis_c(name = "Abs. Error", option = "magma") +
  labs(title = "Absolute Model Errors") +
  theme_crime()

p1 + p2
```

**Interpretation 8.2:** Where does the model make the biggest errors? Are there spatial patterns in the errors? What might this reveal?

The largest model errors appear in a few concentrated spatial areas, especially in the strong burglary hotspots, and several zones where burglary levels change sharply from one grid cell to the next.

The absolute-error map shows that the largest absolute errors are spatially clustered, not randomly scattered. This suggests the model struggles most in places with very high burglary levels, and places with abrupt local variation, where neighboring cells differ strongly.

The model’s predictors capture broad trends but cannot fully account for the highly localized, uneven nature of burglary hotspots, which leads to systematic errors in those areas.

# Part 9: Summary Statistics and Tables

## Exercise 9.1: Model Summary Table

```{r model-summary-table}
# Create nice summary table
model_summary <- broom::tidy(final_model, exponentiate = TRUE) %>%
  mutate(
    across(where(is.numeric), ~round(., 3))
  )

model_summary %>%
  kable(
    caption = "Final Negative Binomial Model Coefficients (Exponentiated)",
    col.names = c("Variable", "Rate Ratio", "Std. Error", "Z", "P-Value")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  footnote(
    general = "Rate ratios > 1 indicate positive association with burglary counts."
  )
```

## Exercise 9.2: Key Findings Summary

**Technical Performance:**

-   Cross-validation MAE: `r round(mean(cv_results$mae), 2)`
-   Model vs. KDE: \[KDE performs slightly better (MAE 2.21 vs. 2.06)\]
-   Most predictive variable: \[Distance to sanitation complaint clusters (sanitation_coms.nn)\]

**Spatial Patterns:**

-   Burglaries are \[clustered\]
-   Hot spots are located in \[South Side, the West Side, and parts of the Near North\]
-   Model errors show \[systematic\] patterns

**Model Limitations:**

-   Overdispersion: \[Yes\]
-   Spatial autocorrelation in residuals: \[Haven't tested yet\]
-   Cells with zero counts: \[781 / 2458 ( 31.8 %)\]

# Conclusion and Next Steps

::: callout-important
## What We've Accomplished

We built a spatial predictive model for burglaries using:

✓ Fishnet aggregation\
✓ Spatial features (counts, distances, nearest neighbors)\
✓ Spatial autocorrelation diagnostics (Local Moran's I)\
✓ Count regression (Poisson and Negative Binomial)\
✓ Spatial cross-validation (LOGO)\
✓ Comparison to baseline (KDE)
:::

::::::::::
