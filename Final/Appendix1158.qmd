---
title: "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops"
author: "Xinyuan Cui, Yuqing Yang, Jinyang Xu"
format: 
  html:
    code-fold: show
    toc: true
    toc-location: left
    theme: cosmo
execute:
  warning: false
  message: false
---

é¢„æµ‹â€œ**å®¢æµå¸¦æ¥çš„ç¤¾ä¼šå½±å“ (å…¬å…±å®‰å…¨)**â€ï¼Œâ€œ**å¸®åŠ©è­¦å¯Ÿéƒ¨é—¨æ•‘å‘½**â€ã€‚Shark Tank çš„è¯„å§”ï¼ˆé€šå¸¸æ˜¯æ”¿åºœå®˜å‘˜ï¼‰ä¼šå¯¹â€œå®‰å…¨â€è¯é¢˜æå…¶æ•æ„Ÿã€‚

æ ¸å¿ƒé—®é¢˜ï¼š å®¢æµå¤§åˆ°åº•æ˜¯å¸¦æ¥äº†â€œè¡—é“çœ¼ (Eyes on the Street)â€ä»è€ŒæŠ‘åˆ¶äº†çŠ¯ç½ªï¼Œè¿˜æ˜¯å¸¦æ¥äº†æ›´å¤šçš„â€œæ½œåœ¨å—å®³è€… (Targets)â€ä»è€Œå¸å¼•äº†æ‰’çªƒï¼Ÿ

æ”¿ç­–ç›®æ ‡ï¼š å¸®åŠ© SEPTA è­¦å¯Ÿéƒ¨é—¨ (Transit Police) é¢„æµ‹å“ªäº›ç«™ç‚¹å‘¨è¾¹æ˜¯çŠ¯ç½ªé«˜é£é™©åŒºï¼Œä»è€Œä¼˜åŒ–å·¡é€»è­¦åŠ›éƒ¨ç½²ã€‚

é€‰é¢˜ï¼šåš **â€œBus Ridership -\> Crimeâ€** æ¨¡å‹

**å› å˜é‡** (é¢„æµ‹ç›®æ ‡): ç«™ç‚¹å‘¨è¾¹ 400ç±³å†…çš„çŠ¯ç½ªæ•°é‡ (Crime Incidents Count)

**è‡ªå˜é‡**: Regional Rail Ridership: ä½¿ç”¨ä½ çš„ Regional_Rail_Station_Summary.csvã€‚

å…¶ä»–è‡ªå˜é‡: äººå£ç»Ÿè®¡ (Census): è´«å›°ç‡ã€å¹´è½»ç”·æ€§æ¯”ä¾‹ã€‚å»ºæˆç¯å¢ƒ (Built Env): è·¯ç¯å¯†åº¦ (Street Light Locations)ã€ç©ºç½®åœ°å— (Vacant Land)ã€‚æ—¶é—´ç‰¹å¾: å¦‚æœä½ åšé¢æ¿æ•°æ®ï¼Œå¯ä»¥åŠ å…¥å‘¨æœ«/å·¥ä½œæ—¥ã€‚â€œè¯±å‘å› å­â€ ï¼š**é…’ç±»é”€å”®ç‚¹ (Alcohol Outlets):***æ•°æ®æº:* OpenDataPhilly (Liquor Licenses).+**å•†ä¸šå¯†åº¦ (Commercial Density):***æ•°æ®æº:* Land Use (Zoning) æˆ– OSMã€‚**æˆ¿å±‹ç©ºç½®ç‡ (Vacancy Rate) / 311 æŠ•è¯‰:***æ•°æ®æº:* OpenDataPhillyã€‚**è·ç¦»æœ€è¿‘è­¦å±€çš„è·ç¦» (Dist to Police Station):** *æ•°æ®æº:* OpenDataPhilly (Police Stations)ã€‚

æ¨¡å‹æ–¹æ³•: Negative Binomial Regression (è´ŸäºŒé¡¹å›å½’): çŠ¯ç½ªæ•°æ®ä¹Ÿæ˜¯è®¡æ•°æ•°æ®ï¼Œä¸”ç¦»æ•£åº¦æé«˜ï¼ˆå¾ˆå¤šåœ°æ–¹æ²¡çŠ¯ç½ªï¼Œå°‘æ•°åœ°æ–¹ç‰¹åˆ«å¤šï¼‰ã€‚

Local Spatial Regression (GWR): æ¢ç´¢å®¢æµå¯¹çŠ¯ç½ªçš„å½±å“åœ¨ä¸åŒç¤¾åŒºæ˜¯å¦ä¸åŒï¼Ÿ

Shark Tank Pitch: "SEPTA çš„é¦–è¦ä»»åŠ¡æ˜¯å®‰å…¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¸é¢„æµ‹æœ‰å¤šå°‘äººåè½¦ï¼Œè€Œæ˜¯é¢„æµ‹å“ªé‡Œæœ€éœ€è¦è­¦å¯Ÿã€‚æˆ‘ä»¬åˆ©ç”¨å®¢æµæ•°æ®ä½œä¸ºæ ¸å¿ƒæŒ‡æ ‡ï¼Œè¯†åˆ«å‡ºé‚£äº›â€˜é«˜å®¢æµä½†é«˜çŠ¯ç½ªâ€™çš„å¼‚å¸¸ç«™ç‚¹ï¼Œå»ºè®®åœ¨æ­¤å¢åŠ ç›‘æ§å’Œå·¡é€»ã€‚"

# Phase 1: Data Preparation

## 1.0 Complete data cleaning code

***Load necessary libraries***

```{r}
library(tidyverse)
library(sf)
library(tidycensus)
library(tigris)
options(tigris_use_cache = TRUE, tigris_class = "sf")
library(MASS)
library(spdep)
library(dplyr)
library(scales)
library(ggplot2)
library(caret)
library(nngeo)
library(car)
library(knitr)
library(readr)
library(patchwork)
library(kableExtra)
library(lubridate)
# Add this near the top of your .qmd after loading libraries
options(tigris_use_cache = TRUE)
options(tigris_progress = FALSE)  
```

***Define themes***

```{r themes}
plotTheme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  plot.subtitle = element_text(size = 10),
  plot.caption = element_text(size = 8),
  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
  axis.text.y = element_text(size = 10),
  axis.title = element_text(size = 11, face = "bold"),
  panel.background = element_blank(),
  panel.grid.major = element_line(colour = "#D0D0D0", size = 0.2),
  panel.grid.minor = element_blank(),
  axis.ticks = element_blank(),
  legend.position = "right"
)

mapTheme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  plot.subtitle = element_text(size = 10),
  plot.caption = element_text(size = 8),
  axis.line = element_blank(),
  axis.text = element_blank(),
  axis.ticks = element_blank(),
  axis.title = element_blank(),
  panel.background = element_blank(),
  panel.border = element_blank(),
  panel.grid.major = element_line(colour = 'transparent'),
  panel.grid.minor = element_blank(),
  legend.position = "right",
  plot.margin = margin(1, 1, 1, 1, 'cm'),
  legend.key.height = unit(1, "cm"),
  legend.key.width = unit(0.2, "cm")
)

palette5 <- c("#eff3ff", "#bdd7e7", "#6baed6", "#3182bd", "#08519c")
```

## **1.1 Load and clean bus stop ridership data:**

### 1.1.1 Load bus stop ridership data

```{r}
# 1. Load Bus Data
bus_raw <- read_csv("data/Summer_2025_Stop_Summary_(Bus).csv")
```

### 1.1.2 Clean

```{r}
# 2. Get Philadelphia Boundary
philly_boundary <- counties(state = "PA", cb = TRUE, class = "sf") %>%
  filter(NAME == "Philadelphia") %>%
  st_transform(2272)

```

```{r}
# 3. Process Ridership: Create Long Format (Aggregated + Weekday vs Weekend)
bus_long <- bus_raw %>%
  # A. åŸºç¡€æ¸…æ´—
  filter(!is.na(Lat) & !is.na(Lon)) %>%
  
  # B. é¢„è®¡ç®—æ¯ä¸ªç‰©ç†ç«™ç‰Œçš„å®¢æµ (Pre-calculate per stop_code)
  mutate(
    Raw_Weekday = Weekdays_O + Weekdays_1,
    # Weekend average = (Sat total + Sun total) / 2
    Raw_Weekend = (Saturdays_ + Saturdays1 + Sundays_On + Sundays_Of) / 2
  ) %>%
  
  # C. [æ–°å¢æ­¥éª¤] èšåˆåŒå‘ç«™ç‚¹ (Aggregation by Stop Name)
  # è¿™ä¸€æ­¥æŠŠåŒåçš„ç«™ç‚¹ï¼ˆæ¯”å¦‚ "Broad St & Walnut St" çš„åŒå‘ï¼‰åˆäºŒä¸ºä¸€
  group_by(Stop) %>% 
  summarise(
    # ä¸¤ä¸ªæ–¹å‘çš„å®¢æµç›¸åŠ 
    Ridership_Weekday = sum(Raw_Weekday, na.rm = TRUE),
    Ridership_Weekend = sum(Raw_Weekend, na.rm = TRUE),
    # åæ ‡å–å¹³å‡å€¼
    Lat = mean(Lat, na.rm = TRUE),
    Lon = mean(Lon, na.rm = TRUE),
    # ä¿ç•™ä¸€ä¸ª Stop_Code ä½œä¸º ID
    Stop_Code = first(Stop_Code),
    .groups = "drop"
  ) %>%
  
  # D. Pivot to Long Format (æŠŠå®½æ•°æ®å˜é•¿æ•°æ®)
  # ç°åœ¨æ¯ä¸ªç«™ç‚¹ä¼šå˜æˆä¸¤è¡Œï¼šä¸€è¡Œ Weekdayï¼Œä¸€è¡Œ Weekend
  pivot_longer(
    cols = c(Ridership_Weekday, Ridership_Weekend),
    names_to = "Time_Type",
    values_to = "Ridership"
  ) %>%
  
  # E. Create Dummy Variable & Clean up
  mutate(
    is_weekend = if_else(Time_Type == "Ridership_Weekend", 1, 0),
    Time_Category = if_else(is_weekend == 1, "Weekend", "Weekday")
  ) %>%
  
  # F. Convert to SF & Clip
  st_as_sf(coords = c("Lon", "Lat"), crs = 4326) %>%
  st_transform(2272) %>%
  st_intersection(philly_boundary) %>%
  dplyr::select(Stop_Code, Stop, Ridership, is_weekend, Time_Category, geometry)

# Check results
cat("Total Aggregated Stops (Unique Locations):", nrow(bus_long) / 2, "\n")
cat("Total Observation Rows (Panel Data):", nrow(bus_long), "\n")
```

### 1.1.3 Visualization

```{r map_stations}
# 1. Prepare Base Map (Philly Boundary)
# ç¡®ä¿ä¹‹å‰å·²ç»åŠ è½½äº† tigris åŒ…å¹¶è·å–äº†è¾¹ç•Œ
# philly_boundary <- counties(state = "PA", cb = TRUE, class = "sf") %>%
#   filter(NAME == "Philadelphia") %>%
#   st_transform(2272)

# 2. Plot the Map
ggplot() +
  # A. Base Layer: Philadelphia County Background
  geom_sf(data = philly_boundary, 
          fill = "grey98", 
          color = "grey50", 
          size = 0.5) +
  
  # B. Station Layer: Simple Points
  # [ä¿®æ”¹ç‚¹] åªç­›é€‰ Weekday çš„è¡Œæ¥ç”»å›¾ï¼Œé¿å…æ¯ä¸ªç‚¹ç”»ä¸¤æ¬¡å¯¼è‡´é‡å 
  geom_sf(data = bus_long %>% filter(is_weekend == 0), 
          color = "#3182bd",  # SEPTA Blue
          size = 0.5,         # Small dots to avoid clutter
          alpha = 0.6) +      # Slight transparency
  
  # C. Styling
  labs(
    title = "Spatial Coverage of SEPTA Bus Network",
    # [ä¿®æ”¹ç‚¹] ä¿®æ­£ nrow é™¤æ³•çš„è¯­æ³•
    subtitle = paste0("Total Aggregated Stops: ", nrow(bus_long) / 2),
    caption = "Source: SEPTA Summer 2025 Stop Summary"
  ) +
  theme_void() + # Clean look
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 10, color = "grey40", hjust = 0.5),
    plot.margin = margin(1, 1, 1, 1, "cm")
  )
```

## **1.2 Load and clean secondary dataset:**

### 1.2.1 Crime data:

```{r}
crime_raw <- read_csv("data/crime_sel.csv")

# Filter & Transform
crime_sf <- crime_raw %>%
  filter(!is.na(lat) & !is.na(lng)) %>%
  # 1. Parse Date and Create Time Features
  # å‡è®¾æ—¥æœŸåˆ—åä¸º 'dispatch_date' æˆ– 'date'ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
  mutate(
    crime_date = as.Date(dispatch_date), 
    day_of_week = wday(crime_date), # 1 is Sunday, 7 is Saturday
    # å®šä¹‰å‘¨æœ«: å‘¨å…­(7) å’Œ å‘¨æ—¥(1)
    is_crime_weekend = if_else(day_of_week %in% c(1, 7), 1, 0)
  ) %>%
  
  # Optional: Filter for specific crime types if needed (e.g., Theft, Assault)
  # For now, we take all crimes to see general safety risk
  st_as_sf(coords = c("lng", "lat"), crs = 4326) %>%
  st_transform(2272)

cat("Total Crime Incidents:", nrow(crime_sf))
```

### 1.2.2 Census data (tidycensus):

```{r census_key, eval=FALSE}

census_api_key("20068788c6e79d5716fbceb0dcd562ab23f74ca1", overwrite = TRUE, install = TRUE)
```

```{r census_key_hidden, include=FALSE}
# Hidden key for rendering
census_api_key("20068788c6e79d5716fbceb0dcd562ab23f74ca1")
```

```{r}
# Load Census data for Philadelphia tracts
philly_census <- get_acs(
  geography = "tract",
  variables = c(
    total_pop = "B01003_001",
    poverty_pop = "B17001_002",
    med_income = "B19013_001",
    ba_degree = "B15003_022",
    total_edu = "B15003_001",
    labor_force = "B23025_003",
    unemployed = "B23025_005",
    total_housing = "B25002_001",
    vacant_housing = "B25002_003"
  ),
  year = 2023, 
  state = "PA",
  county = "Philadelphia",
  geometry = TRUE,
  output = "wide"
) %>%
  st_transform(2272) %>%
  mutate(
    # æ³¨æ„ï¼šæ‰€æœ‰å˜é‡ååé¢éƒ½è¦åŠ  "E"
    Poverty_Rate = poverty_popE / total_popE,
    Med_Income = med_incomeE,
    
    # ä¿®æ­£éƒ¨åˆ†ï¼šåŠ ä¸Š E
    ba_rate = 100 * ba_degreeE / total_eduE,
    unemployment_rate = 100 * unemployedE / labor_forceE,
    vacancy_rate = 100 * vacant_housingE / total_housingE
  ) %>%
  dplyr::select(GEOID, Poverty_Rate, Med_Income, ba_rate, unemployment_rate, vacancy_rate)
```

### 1.2.3 Spatial amenities (OpenDataPhilly)

-   è·¯ç¯å¯†åº¦
-   é…’ç±»é”€å”®ç‚¹
-   è­¦å±€

```{r}
# A. Alcohol Outlets (Crime Generators)
alcohol_sf <- read_csv("data/PHL_PLCB_geocoded.csv") %>%
  filter(!is.na(lon) & !is.na(lat)) %>%
  st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%
  st_transform(2272)

```

```{r}
# B. Street Lights (Guardianship)
lights_sf <- read_csv("data/Street_Poles.csv") %>%
  filter(!is.na(X) & !is.na(Y)) %>%
  # ä¿®æ­£ï¼šåŸå§‹åæ ‡æ˜¯ EPSG:3857 (Web Mercator)
  st_as_sf(coords = c("X", "Y"), crs = 3857) %>%
  # ç„¶åå†è½¬ä¸ºè´¹åŸæŠ•å½±
  st_transform(2272)
```

```{r}
# C. Police Stations (Guardianship) 
# ç›´æ¥è¯»å–åŒ…å«åæ ‡çš„ GeoJSON æ–‡ä»¶
police_sf <- st_read("data/Police_Stations.geojson", quiet = TRUE) %>%
  st_transform(2272) # ç»Ÿä¸€è½¬ä¸º PA State Plane

cat("Total Police Stations:", nrow(police_sf))
```

------------------------------------------------------------------------

# Phase 2: Feature Engineering

## **2.1 Buffer creation:**

### 2.1.1 å…¬äº¤ç«™400m buffer

```{r}
# Create Buffer(400m)
bus_buffer <- st_buffer(bus_long, 1312)
```

### 2.1.2 Crime numbers

```{r}
# 1. Count Crime (Y)
crime_agg <- st_join(bus_buffer, crime_sf, join = st_intersects) %>%
  # [é‡ç‚¹] åªæœ‰å½“ å…¬äº¤çš„æ—¶é—´å±æ€§ == çŠ¯ç½ªçš„æ—¶é—´å±æ€§ æ—¶æ‰ä¿ç•™
  filter(is_weekend == is_crime_weekend) %>%
  # å¿…é¡»æŒ‰ Stop_Code å’Œ is_weekend åˆ†ç»„ï¼Œç¡®ä¿ä¸€ä¸€å¯¹åº”
  group_by(Stop_Code, is_weekend) %>%
  summarise(Crime_Count = n()) %>% # è¿™é‡Œä¸éœ€è¦ n()-1ï¼Œå› ä¸º filter ä¹‹åæ²¡æœ‰ match çš„ä¼šè¢«æ»¤æ‰ï¼Œæˆ–è€…æˆ‘ä»¬åœ¨è¿™ä¸€æ­¥ä¹‹å fill 0
  st_drop_geometry()
```

### 2.1.3 POI Numbersï¼ˆé…’ç±»é”€å”®ç‚¹ï¼‰

```{r}
# 2. Count Alcohol Outlets
alcohol_agg <- st_join(bus_buffer, alcohol_sf, join = st_intersects) %>%
  # æŒ‰ Stop å’Œ æ—¶é—´ç±»å‹åˆ†ç»„ï¼Œè¿™æ ·æ¯ä¸€è¡Œéƒ½ä¼šå¾—åˆ°è®¡æ•°
  group_by(Stop_Code, is_weekend) %>%
  summarise(Alcohol_Count = n() - 1) %>% # Subtract 1 because st_join is left join (self-intersection NA check)
  st_drop_geometry()
```

### 2.1.4 Infrastructure Numbersï¼ˆè·¯ç¯å¯†åº¦ï¼‰

```{r}
# 3. Count Street Lights
light_agg <- st_join(bus_buffer, lights_sf, join = st_intersects) %>%
  group_by(Stop_Code, is_weekend) %>%
  summarise(Light_Count = n() - 1) %>%
  st_drop_geometry()
```

### 2.1.5 Census Demographics

```{r}
# 4. Average Census Demographics
census_agg <- st_join(bus_buffer, philly_census, join = st_intersects) %>%
  group_by(Stop_Code, is_weekend) %>%
  summarise(
    Avg_Poverty = mean(Poverty_Rate, na.rm = TRUE),
    Avg_Income = mean(Med_Income, na.rm = TRUE),
    Avg_BA = mean(ba_rate, na.rm = TRUE),
    Avg_Unemployment = mean(unemployment_rate, na.rm = TRUE),
    Avg_Vacancy = mean(vacancy_rate, na.rm = TRUE)
  ) %>%
  st_drop_geometry()
```

## **2.2 k-Nearest Neighbor features:**

### 2.2.1 Police station (KNN-1)

```{r}
# Calculate distance to nearest police station
dist_matrix <- st_distance(bus_long, police_sf)

# å–æ¯ä¸€è¡Œçš„æœ€å°å€¼ï¼Œå¹¶æ¢ç®—æˆè‹±é‡Œ
bus_long$Dist_Police <- apply(dist_matrix, 1, min) / 5280
```

## 2.3 Merge Features into Master Dataset

```{r}
final_data <- bus_long %>%
  # Join all aggregated tables using BOTH Stop_Code and is_weekend
  left_join(crime_agg, by = c("Stop_Code", "is_weekend")) %>%
  left_join(alcohol_agg, by = c("Stop_Code", "is_weekend")) %>%
  left_join(light_agg, by = c("Stop_Code", "is_weekend")) %>%
  left_join(census_agg, by = c("Stop_Code", "is_weekend")) %>%
  
  mutate(
    # Handle NAs (important for count data)
    Crime_Count = replace_na(Crime_Count, 0),
    Alcohol_Count = replace_na(Alcohol_Count, 0),
    Light_Count = replace_na(Light_Count, 0),
    
    # Log transform Ridership
    Log_Ridership = log(Ridership + 1),
    
    # [IMPORTANT] Create Factor for Interaction Term
    # Ensure 0 is Weekday (Reference) and 1 is Weekend
    is_weekend_factor = factor(is_weekend, levels = c(0, 1), labels = c("Weekday", "Weekend"))
  ) %>%
  # Remove stops that fell outside census tracts or had bad data
  na.omit() 

cat("Final Panel Dataset Rows:", nrow(final_data))
```

------------------------------------------------------------------------

# Phase 3: Exploratory Data Analysis

## 3.1 Distribution of Crime and Bus Stop Ridership (histogram)

Does ridership follow a normal distribution? No. That's why we need Negative Binomial.

```{r}
library(gridExtra)
p1 <- ggplot(final_data, aes(x = Ridership)) +
  geom_histogram(fill = "#3182bd", bins = 50) +
  labs(title = "Distribution of Ridership", x = "Daily Boardings") + plotTheme

p2 <- ggplot(final_data, aes(x = Crime_Count)) +
  geom_histogram(fill = "#de2d26", bins = 50) +
  labs(title = "Distribution of Crime", x = "Crime Count (400m)") + plotTheme

grid.arrange(p1, p2, ncol = 2)
```

**Key Findings:**

```{r}
# æ£€æŸ¥å‡å€¼å’Œæ–¹å·®
mean_crime <- mean(final_data$Crime_Count, na.rm = TRUE)
var_crime <- var(final_data$Crime_Count, na.rm = TRUE)

cat("Mean:", mean_crime, "\n")
cat("Variance:", var_crime, "\n")
cat("Ratio (Var/Mean):", var_crime / mean_crime, "\n")
```

## 3.2 Spatial distribution of crime and Bus Stop Ridership(map)

```{r}

```

**Key Findings:**

## 3.3 Crime vs. Bus Stop Ridership (scatter plots)

```{r eda_interaction}
# Crime vs. Ridership (Interaction Plot)
# è¿™æ˜¯ä¸€ä¸ªéå¸¸å…³é”®çš„å›¾ï¼Œç”¨äºéªŒè¯ä½ çš„æ ¸å¿ƒå‡è®¾ï¼šå‘¨æœ«çš„å®¢æµå½±å“æ˜¯å¦ä¸åŒï¼Ÿ

ggplot(final_data, aes(x = Log_Ridership, y = Crime_Count, color = is_weekend_factor)) +
  geom_point(alpha = 0.1, size = 1) +
  # æ·»åŠ å¹³æ»‘æ‹Ÿåˆçº¿ (Poisson/NB é£æ ¼)
  geom_smooth(method = "glm", method.args = list(family = "quasipoisson"), se = TRUE) +
  scale_color_manual(values = c("Weekday" = "#3182bd", "Weekend" = "#de2d26")) +
  labs(
    title = "Does Ridership impact Crime differently on Weekends?",
    subtitle = "Interaction Effect of Time Category",
    x = "Log(Daily Ridership)",
    y = "Crime Count (400m Buffer)",
    color = "Time Period"
  ) +
  plotTheme
```

**Key Findings:**

-   

## 3.4 Crime vs. Spatial & Social features (scatter plots)

```{r 3.4_scatter_plots}
library(patchwork)
library(scales)

# ä¸ºäº†å¯è§†åŒ–æ¸…æ™°ï¼Œæˆ‘ä»¬å…ˆåˆ›å»ºä¸€ä¸ªå¸¦æœ‰ log(crime) çš„ä¸´æ—¶ç»˜å›¾æ•°æ®
plot_data <- final_data %>%
  mutate(
    log_crime = log(Crime_Count + 1), # +1 é¿å… log(0)
    # ç»™ Income å–ä¸ª log çœ‹çœ‹æ˜¯å¦æ›´çº¿æ€§ï¼Œå¾ˆå¤šç»æµå­¦å˜é‡ log åæ•ˆæœæ›´å¥½
    log_income = log(Avg_Income + 1)
  )

# --- 1. POI & Infrastructure (Built Environment) ---

# A. Alcohol Outlets (çº¿æ€§è¿˜æ˜¯æŒ‡æ•°å¢åŠ ï¼Ÿ)
p1 <- ggplot(plot_data, aes(x = Alcohol_Count, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#6A1B9A") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Log(Crime) vs. Alcohol Outlets",
       subtitle = "Check for diminishing returns",
       x = "Count of Alcohol Outlets", y = "Log(Crime Count)") +
  plotTheme

# B. Street Lights (è·¯ç¯è¶Šå¤šè¶Šå®‰å…¨ï¼Ÿè¿˜æ˜¯è·¯ç¯åªå‡ºç°åœ¨ç¹ååŒºï¼Ÿ)
p2 <- ggplot(plot_data, aes(x = Light_Count, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#6A1B9A") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Log(Crime) vs. Street Lights",
       subtitle = "Is the relationship linear?",
       x = "Count of Street Lights", y = "") +
  plotTheme

# C. Distance to Police (ç¦»è­¦å±€è¶Šè¿œè¶Šå±é™©ï¼Ÿ)
p3 <- ggplot(plot_data, aes(x = Dist_Police, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#6A1B9A") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Log(Crime) vs. Dist to Police",
       subtitle = "Check for U-shape or threshold",
       x = "Distance to Station (Miles)", y = "") +
  plotTheme

# --- 2. Demographics (Census) ---

# D. Poverty Rate (è´«å›°ç‡ä¸çŠ¯ç½ª)
p4 <- ggplot(plot_data, aes(x = Avg_Poverty, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#3182bd") +
  geom_smooth(method = "loess", color = "orange", se = FALSE) +
  scale_x_continuous(labels = scales::percent) +
  labs(title = "Log(Crime) vs. Poverty Rate",
       x = "Poverty Rate", y = "Log(Crime Count)") +
  plotTheme

# E. Median Income (æ”¶å…¥ä¸çŠ¯ç½ª - è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„å¯èƒ½éœ€è¦ log çš„å˜é‡)
p5 <- ggplot(plot_data, aes(x = Avg_Income, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#3182bd") +
  geom_smooth(method = "loess", color = "orange", se = FALSE) +
  scale_x_continuous(labels = scales::dollar) +
  labs(title = "Log(Crime) vs. Median Income",
       subtitle = "Does wealth shield against crime?",
       x = "Median Household Income", y = "") +
  plotTheme

# F. Vacancy Rate (ç©ºç½®ç‡/ç ´çª—ç†è®º)
p6 <- ggplot(plot_data, aes(x = Avg_Vacancy, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#3182bd") +
  geom_smooth(method = "loess", color = "orange", se = FALSE) +
  scale_x_continuous(labels = scales::percent) +
  labs(title = "Log(Crime) vs. Vacancy Rate",
       x = "Housing Vacancy Rate", y = "") +
  plotTheme

# --- Combine Plots ---
(p1 | p2 | p3) / (p4 | p5 | p6) +
  plot_annotation(
    title = "Exploratory Analysis: Variable Functional Forms",
    subtitle = "Red Line = Loess Smoother (Non-linear trend)",
    theme = theme(plot.title = element_text(size = 16, face = "bold"))
  )
```

**Key Findings:**

-   **Strong Socioeconomic Influence:** Housing prices show clear positive correlations with key socioeconomic indicators. Both bachelor's degree rate and median household income exhibit strong positive relationships with sale prices, indicating that neighborhoods with higher educational attainment and income levels command substantially higher property values.

-   **Negative Impact of Crime and Unemployment:** There are evident negative relationships between housing prices and both crime levels (measured by square root of crime count) and unemployment rates. This demonstrates that public safety and local economic vitality are significant determinants of property values in Philadelphia.

-   **Positive Effects of Historical Prices and Transit Access:** Sale prices maintain a positive relationship with both historical price density and accessibility to public transportation. This suggests that areas with established high-value characteristics and better transit infrastructure maintain their premium in the housing market, reflecting path dependence in neighborhood valuation and the value of transportation accessibility.

## 3.5 Other visualization

```{r}

```

```{r}

```

**Key Findings:**

-   **Spatial Variation in Property Values:** The average sale price per square foot shows significant geographic clustering across census tracts, with distinct high-value areas concentrated in specific neighborhoods. This indicates strong spatial autocorrelation in housing prices, where adjacent tracts tend to have similar price levels.

-   **Correlation Between Property Condition and Location:** Better average interior conditions are systematically concentrated in particular geographic areas, suggesting that housing maintenance and quality are not randomly distributed but follow spatial patterns that may correlate with neighborhood characteristics and property values.

-   **Heterogeneous Distribution of Housing Size:** The average livable area varies substantially across census tracts, with larger properties clustered in specific regions. This spatial patterning of housing size complements the price distribution, indicating that both property characteristics and location factors contribute to the overall housing market structure in Philadelphia.

------------------------------------------------------------------------

# Phase 4: Model Building(è´ŸäºŒé¡¹å¼)

## **4.1 Ridership only:**

```{r Model_1+Summary}

model_1 <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  +  #Structural
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing, 
              
              data = opa_census_all,
              weight=weight_mix
               )
summary(model_1)
```

**Coefficient Interpretation:**

-   `log(total_livable_area) (0.752)`: An elasticity coefficient. A 1% increase in livable area is associated with a 0.752% increase in price. This is a strong positive driver.

-   `number_of_bathrooms (0.046)`: Each additional bathroom is associated with a 4.6% increase in price.

-   `house_age_c (-0.00001)`: The linear term for house age is statistically insignificant (p=0.929).

-   `house_age_c2 (0.000047)`: The squared term for age is positive and significant. Combined with the insignificant linear term, this suggests a slight U-shaped relationship, where new homes and very old homes (perhaps with historical value) command a premium over middle-aged homes.

-   `interior_condition (-0.114)`: Assuming a higher value means worse condition, each one-unit worsening in condition is associated with an 11.4% decrease in price.\

-   `quality_grade_num (0.070)`: Each one-unit increase in the quality grade is associated with a 7.0% increase in price.

-   `fireplaces (0.117)`: Each additional fireplace is associated with a 11.7% increase in price.

-   `garage_spaces (0.143)`: Each additional garage space is associated with a 14.3% increase in price.

-   `central_air_dummy (0.458)`: Homes with central air are estimated to be 45.8% more expensive than the baseline (e.g., no AC). This is a very significant amenity premium.

-   `central_air_missing (-0.230)`: Homes where central air data is missing are 23.0% cheaper than the baseline.

## **4.2 Census variables:**

```{r Model_2+Summary}
model_2 <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  +   #Structural
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing+
                
                  income_scaled +              #Census
                  ba_rate +
                  unemployment_rate,
  
              data = opa_census_all,
              weight=weight_mix
               )
summary(model_2)
```

**Coefficient Interpretation:**

-   Coefficient Evolution (vs. Model 1):

    -   `log(total_livable_area) (0.710 vs 0.752)`: The elasticity of area decreased. This suggests Model 1 overestimated the impact of area. Why? Because larger homes are often located in wealthier neighborhoods. Model 1 incorrectly attributed some of the "wealthy neighborhood" premium to "large area."

    -   `quality_grade_num (0.0015 vs 0.070)`: The coefficient for quality grade became statistically insignificant (p=0.520). This is a key finding: home quality is highly correlated with neighborhood income. Once we directly control for income (income_scaled), the independent effect of quality grade disappears.

    -   `central_air_dummy (0.219 vs 0.458)`: The premium for central air was halved. This also indicates that central air is more common in affluent areas, and Model 1 suffered from significant Omitted Variable Bias (OVB).

-   New Variable (Census) Interpretation:

    -   `income_scaled (0.455)`: A one-unit increase in standardized census tract income is associated with a 45.5% increase in price. A very strong positive effect.

    -   `ba_rate (0.0129)`: A 1 percentage point increase in the neighborhood's bachelor's degree attainment rate is associated with a 1.3% price increase.

    -   `unemployment_rate (-0.0066)`: A 1 percentage point increase in the unemployment rate is associated with a 0.66% decrease in price.

## **4.3 Spatial features:**

```{r Model_3+Summary}
model_3 <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  +   #Structural
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing+
                
                  income_scaled +              #Census 
                  ba_rate +
                  unemployment_rate +
                
                  transit_count+
                  avg_past_price_density+      #Spatial 
                  sqrt(crime_count) +
                  log(nearest_hospital_knn3),
              
              data = opa_census_all,
              weight=weight_mix
               )
summary(model_3)
```

**Coefficient Interpretation:**

-   Coefficient Evolution (vs. Model 2):

    -   `income_scaled (0.146 vs 0.455)`: The effect of income dropped sharply (by \~2/3). This again reveals OVB in Model 2. The large "income" effect in Model 2 was confounded with "spatial amenities"â€”high-income individuals tend to live in low-crime, accessible areas.

    -   `ba_rate (0.0027 vs 0.0129)`: The education premium also dropped significantly for the same reason.

    -   `garage_spaces (0.095 vs 0.170)`: The garage premium decreased, likely because spatial variables (like density or transit access) have captured related information.

-   New Variable (Spatial) Interpretation:

    -   `transit_count (0.00029)`: Each additional nearby public transit stop is associated with a 0.029% increase in price.

    -   `avg_past_price_density (0.0032)`: As a proxy for local market heat or locational value, each unit increase is associated with a 0.32% price increase.

    -   `sqrt(crime_count) (-0.040)`: A one-unit increase in the square root of the crime count is associated with a 4.0% decrease in price.

    -   `log(nearest_hospital_knn3) (0.087)`: A 1% increase in the distance from the nearest hospital is associated with a 0.087% increase in price. This suggests people prefer to live further away from hospitals (perhaps to avoid noise, traffic, or sirens), not closer.

## **4.4 Interactions and fixed effects:**

```{r Model_4+Summary}
model_4 <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  +   #Structural
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing+
                
                  income_scaled +              #Census 
                  ba_rate +
                  unemployment_rate +
                
                  transit_count+
                  avg_past_price_density+      #Spatial 
                  sqrt(crime_count) +
                  log(nearest_hospital_knn3)+
                
                  (interior_condition * income_scaled)+  #FE & Interaction
                  factor(zip_code),
              
              data = opa_census_all,
              weight=weight_mix
               )
summary(model_4)
```

```{r 4.4.3 VIF Validation}
vif(model_4)
```

**Coefficient Interpretation:**

-   Fixed Effects Interpretation:

    -   These coefficients represent the price difference for each zip code relative to the "reference zip code" (which is omitted from the list, e.g. 19102).

    -   Example: `factor(zip_code)19106 (-0.107)`: A home in zip code 19106 is, on average, 10.7% less expensive than a home in the reference zip code, holding all other variables constant.

    -   Example: `factor(zip_code)19149 (0.183)`: A home in zip code 19149 is, on average, 18.3% more expensive.

-   interior_condition: `income_scaled (0.117)` (Interaction Term):

    -   This is one of the most interesting findings. It shows that the impact of interior_condition depends on income_scaled.

    -   The total marginal effect of interior_condition is:$$= -0.1695 + 0.1165 \times \text{income\_scaled}$$

    -   At the baseline income level (income_scaled = 0), each one-unit worsening in condition is associated with a 17.0% price decrease (-0.1695). However, this penalty is mitigated (lessened) in higher-income areas. For each one-unit increase in income_scaled, the negative penalty of poor condition is reduced by 11.7 percentage points. This may imply that in high-income neighborhoods, "fixer-uppers" (homes in poor condition) are seen as investment opportunities with high renovation potential. Therefore, the market penalty for "poor condition" is smaller.

# Phase 5: Model Validation

## **5.1 Compare all 4 models:**

### 5.1.1 Create predicted vs. actual plot

```{r}
opa_census_2_clean <- opa_census_2

models <- list(model_1, model_2, model_3, model_4)
model_names <- c("Model 1", "Model 2", "Model 3", "Model 4")


options(scipen = 999)

all_pred_usd <- c()
for (m in models) {
  pred_log_tmp <- predict(m, newdata = opa_census_2_clean)
  smearing_tmp <- mean(exp(residuals(m)), na.rm = TRUE)
  pred_usd_tmp <- exp(pred_log_tmp) * smearing_tmp
  all_pred_usd <- c(all_pred_usd, pred_usd_tmp)
}
actual_usd <- opa_census_2_clean$sale_price_predicted

actual_k <- actual_usd / 1000
pred_all_k <- all_pred_usd / 1000

x_min <- min(actual_k, pred_all_k, na.rm = TRUE)
x_max <- 8000               # manually fix max x at 8000K
y_min <- min(actual_k, pred_all_k, na.rm = TRUE)
y_max <- max(actual_k, pred_all_k, na.rm = TRUE)

x_ticks <- pretty(c(x_min, x_max), n = 6)
y_ticks <- pretty(c(y_min, y_max), n = 6)

# Loop
for (i in seq_along(models)) {
  model <- models[[i]]
  model_name <- model_names[i]
  
  #  Predict on the same validation dataset 
  pred_log <- predict(model, newdata = opa_census_2_clean)
  
  # Smearing correction (to restore to USD scale) 
  smearing_factor <- mean(exp(residuals(model)), na.rm = TRUE)
  pred_usd <- exp(pred_log) * smearing_factor
  pred_k <- pred_usd / 1000   # Convert to thousand dollars
  

  rmse <- sqrt(mean((pred_usd - opa_census_2_clean$sale_price_predicted)^2, na.rm = TRUE))
  rmse_norm <- rmse / mean(opa_census_2_clean$sale_price_predicted, na.rm = TRUE)
  

  cat("\n============================\n")
  cat(model_name, "\n")
  cat("RMSE (USD):", round(rmse, 2), "\n")
  cat("Normalized RMSE:", round(rmse_norm, 4), "\n")
  cat("============================\n")
  
  
  plot(
    actual_k, pred_k,
    xlab = "Actual Price ($K)",
    ylab = "Predicted Price ($K)",
    main = paste(model_name, "- Predicted vs Actual Sale Price"),
    pch = 19,
    col = rgb(0.2, 0.4, 0.6, 0.4),
    xlim = c(x_min, x_max),
    ylim = c(y_min, y_max),
    axes = FALSE
  )

  axis(1, at = x_ticks, labels = paste0(x_ticks, "K"))
  axis(2, at = y_ticks, labels = paste0(y_ticks, "K"), las = 1)
  box()
  
  # Add 45-degree line
  abline(0, 1, col = "red", lwd = 2)
  
  #Save as PNG with same scale
  png_filename <- paste0("pred_actual_", gsub(" ", "_", tolower(model_name)), ".png")
  png(png_filename, width = 900, height = 800)
  par(mar = c(5, 5, 4, 2))
  
  plot(
    actual_k, pred_k,
    xlab = "Actual Price ($K)",
    ylab = "Predicted Price ($K)",
    main = paste(model_name, "- Predicted vs Actual Sale Price"),
    pch = 19,
    col = rgb(0.2, 0.4, 0.6, 0.4),
    xlim = c(x_min, x_max),
    ylim = c(y_min, y_max),
    axes = FALSE
  )
  axis(1, at = x_ticks, labels = paste0(x_ticks),cex.axis = 0.8)
  axis(2, at = y_ticks, labels = paste0(y_ticks), las = 1,cex.axis = 0.8)
  box()
  abline(0, 1, col = "red", lwd = 2)
  dev.off()
}

```

### 5.1.2 Report and Compare RMSE, MAE, RÂ²

```{r 5.1.1 Model_1 Validation}
#Since data have different weights, we need to define a new 10-fold cross-validation model.

set.seed(123)   
k <- 10

# Shuffle dataset rows
opa_census_all <- opa_census_all %>%
  slice_sample(prop = 1) %>%  # randomly reorder all rows
  mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs

#Initialize result vectors
rmse_usd_vec <- numeric(k)
r2_log_vec <- numeric(k)
rmse_log_vec <- numeric(k)
mae_usd_vec <- numeric(k)

#Perform k-fold cross-validation
for (i in 1:k) {
  
  # Split training / validation sets
  train <- opa_census_all %>% filter(fold_id != i)
  test_raw <- opa_census_all %>% filter(fold_id == i)
  
  # âœ… Very important! Ensure the 1/10 test data only include real market transactions

  test <- test_raw %>% filter(non_market != 1)  
  
  if (nrow(test) < 10) {
    cat("Skipping fold", i, ": too few validation samples after cleaning.\n")
    next
  }
  
  # Weighted linear regression
  model_i <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  + 
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing, 
    data = train,
    weights = train$weight_mix
  )
  
  # Predict in log scale
  test$pred_log <- predict(model_i, newdata = test)
  
  # Compute RÂ² 
  actual_log <- log(test$sale_price_predicted)
  ss_res <- sum((actual_log - test$pred_log)^2, na.rm = TRUE)
  ss_tot <- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)
  r2_log_vec[i] <- 1 - ss_res / ss_tot
  
  # RMSE (log)
  rmse_log_vec[i] <- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))
  
  # Compute RMSE in USD (Duan smearing correction)
  smearing_factor <- mean(exp(residuals(model_i)), na.rm = TRUE)
  test$pred_usd <- exp(test$pred_log) * smearing_factor
  rmse_usd_vec[i] <- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))
  
  mae_usd_vec[i] <- mean(abs(test$sale_price_predicted - test$pred_usd), na.rm = TRUE)
}
```

```{r, echo=FALSE}
# Summarize results
cat("\n====================================\n")
cat("MODEL_1\n")
cat("ğŸ’µ Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n")
cat("Average RMSE (log):", round(mean(rmse_log_vec, na.rm = TRUE), 4), "\n")
cat("Average RMSE (USD):", round(mean(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("RMSE Std. Dev (USD):", round(sd(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("Average RÂ²:", round(mean(r2_log_vec, na.rm = TRUE), 4), "\n")
cat("Average MAE (USD):", round(mean(mae_usd_vec, na.rm = TRUE), 2), "\n")
cat("====================================\n")

# Optional: view per-fold results
cv_results <- data.frame(
  Fold = 1:k,
  RMSE = round(rmse_log_vec, 2),
  RMSE_USD = round(rmse_usd_vec, 2),
  R2 = round(r2_log_vec, 4),
  MAE_USD = round(mae_usd_vec, 2)
)
print(cv_results)
```

```{r 5.1.2 Model_2 Validation}
#Since data have different weights, we need to define a new 10-fold cross-validation model.

set.seed(234)   
k <- 10

# Shuffle dataset rows
opa_census_all <- opa_census_all %>%
  slice_sample(prop = 1) %>%  # randomly reorder all rows
  mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs

#Initialize result vectors
rmse_usd_vec <- numeric(k)
r2_log_vec <- numeric(k)
rmse_log_vec <- numeric(k)
mae_usd_vec <- numeric(k)

#Perform k-fold cross-validation
for (i in 1:k) {
  
  # Split training / validation sets
  train <- opa_census_all %>% filter(fold_id != i)
  test_raw <- opa_census_all %>% filter(fold_id == i)
  
  # âœ… Very important! Ensure the 1/10 test data only include real market transactions

  test <- test_raw %>% filter(non_market != 1)  
  
  if (nrow(test) < 10) {
    cat("Skipping fold", i, ": too few validation samples after cleaning.\n")
    next
  }
  
  # Weighted linear regression
  model_i <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  + 
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing+
                  
                  income_scaled +             
                  ba_rate +
                  unemployment_rate,
    data = train,
    weights = train$weight_mix
  )
  
  # Predict in log scale
  test$pred_log <- predict(model_i, newdata = test)
  
  # Compute RÂ² 
  actual_log <- log(test$sale_price_predicted)
  ss_res <- sum((actual_log - test$pred_log)^2, na.rm = TRUE)
  ss_tot <- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)
  r2_log_vec[i] <- 1 - ss_res / ss_tot
  
  # RMSE (log)
  rmse_log_vec[i] <- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))
  
  # Compute RMSE in USD (Duan smearing correction)
  smearing_factor <- mean(exp(residuals(model_i)), na.rm = TRUE)
  test$pred_usd <- exp(test$pred_log) * smearing_factor
  rmse_usd_vec[i] <- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))
  
  mae_usd_vec[i] <- mean(abs(test$sale_price_predicted - test$pred_usd), na.rm = TRUE)
}

```

```{r, echo=FALSE}
# Summarize results
cat("\n====================================\n")
cat("MODEL_2\n")
cat("ğŸ’µ Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n")
cat("Average RMSE (log):", round(mean(rmse_log_vec, na.rm = TRUE), 4), "\n")
cat("Average RMSE (USD):", round(mean(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("RMSE Std. Dev (USD):", round(sd(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("Average RÂ²:", round(mean(r2_log_vec, na.rm = TRUE), 4), "\n")
cat("Average MAE (USD):", round(mean(mae_usd_vec, na.rm = TRUE), 2), "\n")
cat("====================================\n")

# Optional: view per-fold results
cv_results <- data.frame(
  Fold = 1:k,
  RMSE = round(rmse_log_vec, 2),
  RMSE_USD = round(rmse_usd_vec, 2),
  R2 = round(r2_log_vec, 4),
  MAE_USD = round(mae_usd_vec, 2)
)
print(cv_results)
```

```{r 5.1.3 Model_3 Validation}
#Since data have different weights, we need to define a new 10-fold cross-validation model.

set.seed(345)   
k <- 10

# Shuffle dataset rows
opa_census_all <- opa_census_all %>%
  slice_sample(prop = 1) %>%  # randomly reorder all rows
  mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs

#Initialize result vectors
rmse_usd_vec <- numeric(k)
r2_log_vec <- numeric(k)
rmse_log_vec <- numeric(k)
mae_usd_vec <- numeric(k)

#Perform k-fold cross-validation
for (i in 1:k) {
  
  # Split training / validation sets
  train <- opa_census_all %>% filter(fold_id != i)
  test_raw <- opa_census_all %>% filter(fold_id == i)
  
  # âœ… Very important! Ensure the 1/10 test data only include real market transactions

  test <- test_raw %>% filter(non_market != 1)  
  
  if (nrow(test) < 10) {
    cat("Skipping fold", i, ": too few validation samples after cleaning.\n")
    next
  }
  
  # Weighted linear regression
  model_i <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  + 
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing+
                  
                  income_scaled +             
                  ba_rate +
                  unemployment_rate+
                  
                  transit_count+
                  avg_past_price_density+ 
                  sqrt(crime_count) +
                  log(nearest_hospital_knn3),
    data = train,
    weights = train$weight_mix
  )
  
  # Predict in log scale
  test$pred_log <- predict(model_i, newdata = test)
  
  # Compute RÂ² 
  actual_log <- log(test$sale_price_predicted)
  ss_res <- sum((actual_log - test$pred_log)^2, na.rm = TRUE)
  ss_tot <- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)
  r2_log_vec[i] <- 1 - ss_res / ss_tot
  
  # RMSE (log)
  rmse_log_vec[i] <- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))
  
  # Compute RMSE in USD (Duan smearing correction)
  smearing_factor <- mean(exp(residuals(model_i)), na.rm = TRUE)
  test$pred_usd <- exp(test$pred_log) * smearing_factor
  rmse_usd_vec[i] <- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))
  
  mae_usd_vec[i] <- mean(abs(test$sale_price_predicted - test$pred_usd), na.rm = TRUE)
}

```

```{r, echo=FALSE}
# Summarize results
cat("\n====================================\n")
cat("MODEL_3\n")
cat("ğŸ’µ Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n")
cat("Average RMSE (log):", round(mean(rmse_log_vec, na.rm = TRUE), 4), "\n")
cat("Average RMSE (USD):", round(mean(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("RMSE Std. Dev (USD):", round(sd(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("Average RÂ²:", round(mean(r2_log_vec, na.rm = TRUE), 4), "\n")
cat("Average MAE (USD):", round(mean(mae_usd_vec, na.rm = TRUE), 2), "\n")
cat("====================================\n")

# Optional: view per-fold results
cv_results <- data.frame(
  Fold = 1:k,
  RMSE = round(rmse_log_vec, 2),
  RMSE_USD = round(rmse_usd_vec, 2),
  R2 = round(r2_log_vec, 4),
  MAE_USD = round(mae_usd_vec, 2)
)
print(cv_results)
```

```{r 5.1.4 Model_4 Validation}
#Since data have different weights, we need to define a new 10-fold cross-validation model.

set.seed(456)   
k <- 10

# Shuffle dataset rows
opa_census_all <- opa_census_all %>%
  slice_sample(prop = 1) %>%  # randomly reorder all rows
  mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs

#Initialize result vectors
rmse_usd_vec <- numeric(k)
r2_log_vec <- numeric(k)
rmse_log_vec <- numeric(k)
mae_usd_vec <- numeric(k)

#Perform k-fold cross-validation
for (i in 1:k) {
  
  # Split training / validation sets
  train <- opa_census_all %>% filter(fold_id != i)
  test_raw <- opa_census_all %>% filter(fold_id == i)
  
  # âœ… Very important! Ensure the 1/10 test data only include real market transactions

  test <- test_raw %>% filter(non_market != 1)  
  
  if (nrow(test) < 10) {
    cat("Skipping fold", i, ": too few validation samples after cleaning.\n")
    next
  }
  
  # Weighted linear regression
  model_i <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  + 
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing+
                  
                  income_scaled +             
                  ba_rate +
                  unemployment_rate+
                  
                  transit_count+
                  avg_past_price_density+ 
                  sqrt(crime_count) +
                  log(nearest_hospital_knn3)+
                  
                  (interior_condition * income_scaled)+
                  factor(zip_code),
    data = train,
    weights = train$weight_mix
  )
  
  # Predict in log scale
  test$pred_log <- predict(model_i, newdata = test)
  
  # Compute RÂ² 
  actual_log <- log(test$sale_price_predicted)
  ss_res <- sum((actual_log - test$pred_log)^2, na.rm = TRUE)
  ss_tot <- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)
  r2_log_vec[i] <- 1 - ss_res / ss_tot
  # RMSE (log)
  rmse_log_vec[i] <- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))
  
  # Compute RMSE in USD (Duan smearing correction)
  smearing_factor <- mean(exp(residuals(model_i)), na.rm = TRUE)
  test$pred_usd <- exp(test$pred_log) * smearing_factor
  rmse_usd_vec[i] <- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))
  
  mae_usd_vec[i] <- mean(abs(test$sale_price_predicted - test$pred_usd), na.rm = TRUE)
}

```

```{r, echo=FALSE}
# Summarize results
cat("\n====================================\n")
cat("MODEL_4\n")
cat("ğŸ’µ Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n")
cat("Average RMSE (log):", round(mean(rmse_log_vec, na.rm = TRUE), 4), "\n")
cat("Average RMSE (USD):", round(mean(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("RMSE Std. Dev (USD):", round(sd(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("Average RÂ²:", round(mean(r2_log_vec, na.rm = TRUE), 4), "\n")
cat("Average MAE (USD):", round(mean(mae_usd_vec, na.rm = TRUE), 2), "\n")
cat("====================================\n")

# Optional: view per-fold results
cv_results <- data.frame(
  Fold = 1:k,
  RMSE = round(rmse_log_vec, 2),
  RMSE_USD = round(rmse_usd_vec, 2),
  R2 = round(r2_log_vec, 4),
  MAE_USD = round(mae_usd_vec, 2)
)
print(cv_results)
```

**Discussion of the most mattered features**:

-   

# Phase 6: Model Diagnostics

### Check assumptions for best model:

## **6.1 Residual plot:ï¼ˆæ‰¾å®é™…çŠ¯ç½ª\>é¢„æµ‹ï¼‰**

```{r}
model_data <- data.frame(
  Fitted = fitted(model_4),
  Residuals = resid(model_4)
)

p_resid_fitted <- ggplot(model_data, aes(x = Fitted, y = Residuals)) +
  geom_point(alpha = 0.5, color = "#6A1B9A", size = 2) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
  geom_smooth(method = "loess", color = "black", se = FALSE, linewidth = 0.8) +
  labs(
    title = "Residuals vs Fitted Values",
    subtitle = "Checking linearity and homoscedasticity for Model 4",
    x = "Fitted Values (Log(Sale Price))",
    y = "Residuals"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 13, color = "gray40"),
    axis.title = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )
p_resid_fitted
```

```{r}
library(dplyr)
library(ggplot2)

resid_full <- rep(NA, nrow(opa_census_all))
resid_full[-as.numeric(model_4$na.action)] <- resid(model_4)

opa_census_all$residuals <- resid_full

tract_resid <- opa_census_all %>%
  st_drop_geometry() %>%
  group_by(GEOID) %>%
  summarise(mean_residual = mean(residuals, na.rm = TRUE))

tract_map <- philly_census %>%
  left_join(tract_resid, by = "GEOID")

ggplot() +
  geom_sf(data = tract_map, aes(fill = mean_residual), color = "white", size = 0.2) +
  scale_fill_gradient2(
    low = "#6A1B9A", mid = "white", high = "#FFB300",
    midpoint = 0,
    limits = c(-0.5, 0.5),
    name = "Mean Log Residual",
    breaks = c(-0.3, 0, 0.3),
    labels = c("Overestimated", "Accurate", "Underestimated"),
    na.value = "grey60"
  ) +
  theme_minimal(base_size = 16) +
  labs(
    title = "Hardest to Predict Neighborhoods in Philadelphia",
    subtitle = "Yellow = underestimation | Purple = overestimation"
  ) +
  theme(
    panel.grid = element_blank(),
    axis.text = element_blank(),
    legend.position = "right"
  )

```

**Interpretation**:

-   

## **6.2 Q-Q plot:**

```{r}
p_qq <- ggplot(model_data, aes(sample = Residuals)) +
  stat_qq(color = "#6A1B9A", size = 2, alpha = 0.6) +
  stat_qq_line(color = "red",linetype = "dashed", linewidth = 1) +
  labs(
    title = "Normal Q-Q Plot",
    subtitle = "Checking normality of residuals for Model 4",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 13, color = "gray40"),
    axis.title = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )
p_qq
```

**Interpretation**:

-   

## **6.3** Top 50 é£é™©ç«™ç‚¹åœ°å›¾**:**

```{r}

```

**Interpretation**:

-   **Overall**

------------------------------------------------------------------------

# Phase 7: Conclusions

## 7.1 Results:

**ç³»æ•°è§£è¯»:**

-   å¦‚æœå®¢æµç³»æ•°æ˜¯ **è´Ÿ**çš„ -\> **æ”¯æŒâ€œè¡—é“çœ¼â€ç†è®º**ï¼ˆäººå¤šåè€Œå®‰å…¨ï¼‰ã€‚

-   å¦‚æœå®¢æµç³»æ•°æ˜¯ **æ­£**çš„ -\> **æ”¯æŒâ€œæ½œåœ¨å—å®³è€…â€ç†è®º**ï¼ˆäººå¤šæ‰’çªƒå¤šï¼‰ã€‚

## 7.2 The Action Plan:

-   **è­¦åŠ›éƒ¨ç½²:** æ ¹æ®é¢„æµ‹çš„é£é™©å€¼ï¼Œç”Ÿæˆâ€œæ¯æ—¥å·¡é€»çƒ­ç‚¹å›¾â€ã€‚

-   **ç¯å¢ƒè®¾è®¡ (CPTED):** å¯¹äºé‚£äº›é«˜çŠ¯ç½ªç«™ç‚¹ï¼Œå¦‚æœæ˜¯è·¯ç¯ä¸å¤Ÿï¼Œè£…ç¯ï¼›å¦‚æœæ˜¯ç©ºç½®æˆ¿å¤ªå¤šï¼Œæ¸…ç†ç¤¾åŒºã€‚

## **7.3 Equity concerns**

-   **Which neighborhoods are hardest to predict?**

-   **Any data bias?**
