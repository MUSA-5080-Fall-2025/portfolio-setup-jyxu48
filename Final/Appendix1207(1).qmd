---
title: "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops"
author: "Xinyuan Cui, Yuqing Yang, Jinyang Xu"
format: 
  html:
    code-fold: show
    toc: true
    toc-location: left
    theme: cosmo
execute:
  warning: false
  message: false
---

预测“**客流带来的社会影响 (公共安全)**”，“**帮助警察部门救命**”。Shark Tank 的评委（通常是政府官员）会对“安全”话题极其敏感。

核心问题： 客流大到底是带来了“街道眼 (Eyes on the Street)”从而抑制了犯罪，还是带来了更多的“潜在受害者 (Targets)”从而吸引了扒窃？

政策目标： 帮助 SEPTA 警察部门 (Transit Police) 预测哪些站点周边是犯罪高风险区，从而优化巡逻警力部署。

选题：做 **“Bus Ridership -\> Crime”** 模型

**因变量** (预测目标): 站点周边 400米内的犯罪数量 (Crime Incidents Count)

**自变量**: Regional Rail Ridership: 使用你的 Regional_Rail_Station_Summary.csv。

其他自变量: 人口统计 (Census): 贫困率、年轻男性比例。建成环境 (Built Env): 路灯密度 (Street Light Locations)、空置地块 (Vacant Land)。时间特征: 如果你做面板数据，可以加入周末/工作日。“诱发因子” ：**酒类销售点 (Alcohol Outlets):***数据源:* OpenDataPhilly (Liquor Licenses).+**商业密度 (Commercial Density):***数据源:* Land Use (Zoning) 或 OSM。**房屋空置率 (Vacancy Rate) / 311 投诉:***数据源:* OpenDataPhilly。**距离最近警局的距离 (Dist to Police Station):** *数据源:* OpenDataPhilly (Police Stations)。

模型方法: Negative Binomial Regression (负二项回归): 犯罪数据也是计数数据，且离散度极高（很多地方没犯罪，少数地方特别多）。

Local Spatial Regression (GWR): 探索客流对犯罪的影响在不同社区是否不同？

Shark Tank Pitch: "SEPTA 的首要任务是安全。我们的模型不预测有多少人坐车，而是预测哪里最需要警察。我们利用客流数据作为核心指标，识别出那些‘高客流但高犯罪’的异常站点，建议在此增加监控和巡逻。"

# Phase 1: Data Preparation

In this part, I load the datasets that will be used in the analysis. our main dataset is : Philadelphia Septa bus stop summaryboundaries datasets and burglaries dataset. After that, I will visualize the burglaries on the map to gain an intuitive, preliminary understanding of the spatial distribution of burglaries and where the major hotspots are located.

## 1.0 Complete data cleaning code

***Load necessary libraries***

```{r}
library(tidyverse)
library(sf)
library(tidycensus)
library(tigris)
options(tigris_use_cache = TRUE, tigris_class = "sf")
library(MASS)
library(spdep)
library(dplyr)
library(scales)
library(ggplot2)
library(caret)
library(nngeo)
library(car)
library(knitr)
library(readr)
library(patchwork)
library(kableExtra)
library(lubridate)
# Add this near the top of your .qmd after loading libraries
options(tigris_use_cache = TRUE)
options(tigris_progress = FALSE)  
```

***Define themes***

```{r themes}
plotTheme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  plot.subtitle = element_text(size = 10),
  plot.caption = element_text(size = 8),
  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
  axis.text.y = element_text(size = 10),
  axis.title = element_text(size = 11, face = "bold"),
  panel.background = element_blank(),
  panel.grid.major = element_line(colour = "#D0D0D0", size = 0.2),
  panel.grid.minor = element_blank(),
  axis.ticks = element_blank(),
  legend.position = "right"
)

mapTheme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  plot.subtitle = element_text(size = 10),
  plot.caption = element_text(size = 8),
  axis.line = element_blank(),
  axis.text = element_blank(),
  axis.ticks = element_blank(),
  axis.title = element_blank(),
  panel.background = element_blank(),
  panel.border = element_blank(),
  panel.grid.major = element_line(colour = 'transparent'),
  panel.grid.minor = element_blank(),
  legend.position = "right",
  plot.margin = margin(1, 1, 1, 1, 'cm'),
  legend.key.height = unit(1, "cm"),
  legend.key.width = unit(0.2, "cm")
)

palette5 <- c("#eff3ff", "#bdd7e7", "#6baed6", "#3182bd", "#08519c")
```

## **1.1 Load and clean bus stop ridership data:**

### 1.1.1 Load bus stop ridership data

```{r}
# 1. Load Bus Data
bus_raw <- read_csv("data/Summer_2025_Stop_Summary_(Bus).csv")
```

### 1.1.2 Clean

```{r}
# 2. Get Philadelphia Boundary
philly_boundary <- counties(state = "PA", cb = TRUE, class = "sf") %>%
  filter(NAME == "Philadelphia") %>%
  st_transform(2272)

```

```{r}
# 3. Process Ridership: Create Long Format (Aggregated + Weekday vs Weekend)
bus_long <- bus_raw %>%
  # A. 基础清洗
  filter(!is.na(Lat) & !is.na(Lon)) %>%
  
  # B. 预计算每个物理站牌的客流 (Pre-calculate per stop_code)
  mutate(
    Raw_Weekday = Weekdays_O + Weekdays_1,
    # Weekend average = (Sat total + Sun total) / 2
    Raw_Weekend = (Saturdays_ + Saturdays1 + Sundays_On + Sundays_Of) / 2
  ) %>%
  
  # C. [新增步骤] 聚合双向站点 (Aggregation by Stop Name)
  # 这一步把同名的站点（比如 "Broad St & Walnut St" 的双向）合二为一
  group_by(Stop) %>% 
  summarise(
    # 两个方向的客流相加
    Ridership_Weekday = sum(Raw_Weekday, na.rm = TRUE),
    Ridership_Weekend = sum(Raw_Weekend, na.rm = TRUE),
    # 坐标取平均值
    Lat = mean(Lat, na.rm = TRUE),
    Lon = mean(Lon, na.rm = TRUE),
    # 保留一个 Stop_Code 作为 ID
    Stop_Code = first(Stop_Code),
    .groups = "drop"
  ) %>%
  
  # D. Pivot to Long Format (把宽数据变长数据)
  # 现在每个站点会变成两行：一行 Weekday，一行 Weekend
  pivot_longer(
    cols = c(Ridership_Weekday, Ridership_Weekend),
    names_to = "Time_Type",
    values_to = "Ridership"
  ) %>%
  
  # E. Create Dummy Variable & Clean up
  mutate(
    is_weekend = if_else(Time_Type == "Ridership_Weekend", 1, 0),
    Time_Category = if_else(is_weekend == 1, "Weekend", "Weekday")
  ) %>%
  
  # F. Convert to SF & Clip
  st_as_sf(coords = c("Lon", "Lat"), crs = 4326) %>%
  st_transform(2272) %>%
  st_intersection(philly_boundary) %>%
  dplyr::select(Stop_Code, Stop, Ridership, is_weekend, Time_Category, geometry)

# Check results
cat("Total Aggregated Stops (Unique Locations):", nrow(bus_long) / 2, "\n")
cat("Total Observation Rows (Panel Data):", nrow(bus_long), "\n")
```

### 1.1.3 Visualization

```{r map_stations}
# 1. Prepare Base Map (Philly Boundary)
# 确保之前已经加载了 tigris 包并获取了边界
# philly_boundary <- counties(state = "PA", cb = TRUE, class = "sf") %>%
#   filter(NAME == "Philadelphia") %>%
#   st_transform(2272)

# 2. Plot the Map
ggplot() +
  # A. Base Layer: Philadelphia County Background
  geom_sf(data = philly_boundary, 
          fill = "grey98", 
          color = "grey50", 
          size = 0.5) +
  
  # B. Station Layer: Simple Points
  # [修改点] 只筛选 Weekday 的行来画图，避免每个点画两次导致重叠
  geom_sf(data = bus_long %>% filter(is_weekend == 0), 
          color = "#3182bd",  # SEPTA Blue
          size = 0.1,         # Small dots to avoid clutter
          alpha = 0.6) +      # Slight transparency
  
  # C. Styling
  labs(
    title = "Spatial Coverage of SEPTA Bus Network",
    # [修改点] 修正 nrow 除法的语法
    subtitle = paste0("Total Aggregated Stops: ", nrow(bus_long) / 2),
    caption = "Source: SEPTA Summer 2025 Stop Summary"
  ) +
  theme_void() + # Clean look
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 10, color = "grey40", hjust = 0.5),
    plot.margin = margin(1, 1, 1, 1, "cm")
  )
```

## **1.2 Load and clean secondary dataset:**

### 1.2.1 Crime data:

```{r}
### 1.2.1 Crime data:

# 定义“街头犯罪”列表 (排除室内/家庭纠纷/非蓄意犯罪)
# target_crime_types <- c(
#   "Robbery Firearm",
#   "Robbery No Firearm",
#   "Aggravated Assault Firearm",
#   "Aggravated Assault No Firearm",
#   "Homicide - Criminal",
#   "Rape",
#   "Other Sex Offenses (Not Commercialized)",
#   "Weapon Violations",
#   "Narcotic / Drug Law Violations",
#   "Vandalism/Criminal Mischief",
#   "Prostitution and Commercialized Vice",
#   "Public Drunkenness"
# )

crime_raw <- read_csv("data/crime2024.csv")

# Filter & Transform
crime_raw <- crime_raw %>%
  filter(!is.na(lat) & !is.na(lng))

# 2. 单独保存 1–3 月的数据
crime_q1 <- crime_raw %>%
  filter(lubridate::month(dispatch_date) %in% 1:3) %>%
  mutate(
    crime_date = as.Date(dispatch_date), 
    day_of_week = wday(crime_date), # 1 is Sunday, 7 is Saturday
    is_crime_weekend = if_else(day_of_week %in% c(1, 7), 1, 0)
  ) %>%
    
  st_as_sf(coords = c("lng", "lat"), crs = 4326) %>%
  st_transform(2272)  

 # 1. 筛选 4–6 月
crime_sf <- crime_raw %>%
  filter(lubridate::month(dispatch_date) %in% 4:6)%>%

  # 1. 筛选：只保留列表中的犯罪类型
  #filter(text_general_code %in% target_crime_types) %>%
  
  # 2. 时间特征处理
  mutate(
    crime_date = as.Date(dispatch_date), 
    day_of_week = wday(crime_date), # 1 is Sunday, 7 is Saturday
    is_crime_weekend = if_else(day_of_week %in% c(1, 7), 1, 0)
  ) %>%
  
  st_as_sf(coords = c("lng", "lat"), crs = 4326) %>%
  st_transform(2272)

# 打印检查
cat("Total Selected Crimes (Count):", nrow(crime_sf), "\n")
```

### 1.2.2 Census data (tidycensus):

```{r census_key, eval=FALSE}

census_api_key("20068788c6e79d5716fbceb0dcd562ab23f74ca1", overwrite = TRUE, install = TRUE)
```

```{r census_key_hidden, include=FALSE}
# Hidden key for rendering
census_api_key("20068788c6e79d5716fbceb0dcd562ab23f74ca1")
```


```{r}
# Load Census data for Philadelphia tracts
philly_census <- get_acs(
  geography = "tract",
  variables = c(
    total_pop = "B01003_001",
    poverty_pop = "B17001_002",
    med_income = "B19013_001",
    ba_degree = "B15003_022",
    total_edu = "B15003_001",
    labor_force = "B23025_003",
    unemployed = "B23025_005",
    total_housing = "B25002_001",
    vacant_housing = "B25002_003"
  ),
  year = 2023, 
  state = "PA",
  county = "Philadelphia",
  geometry = TRUE,
  output = "wide"
) %>%
  st_transform(2272) %>%
  mutate(
    # 注意：所有变量名后面都要加 "E"
    Poverty_Rate = poverty_popE / total_popE,
    Med_Income = med_incomeE,
    
    # 修正部分：加上 E
    ba_rate = 100 * ba_degreeE / total_eduE,
    unemployment_rate = 100 * unemployedE / labor_forceE,
    vacancy_rate = 100 * vacant_housingE / total_housingE
  ) %>%
  dplyr::select(GEOID, Poverty_Rate, Med_Income, ba_rate, unemployment_rate, vacancy_rate)
```

### 1.2.3 Spatial amenities (OpenDataPhilly)

-   路灯密度
-   酒类销售点
-   警局

```{r}
# A. Alcohol Outlets (Crime Generators)
alcohol_sf <- read_csv("data/PHL_PLCB_geocoded.csv") %>%
  filter(!is.na(lon) & !is.na(lat)) %>%
  st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%
  st_transform(2272)

```

```{r}
# B. Street Lights (Guardianship)
lights_sf <- read_csv("data/Street_Poles.csv") %>%
  filter(!is.na(X) & !is.na(Y)) %>%
  # 修正：原始坐标是 EPSG:3857 (Web Mercator)
  st_as_sf(coords = c("X", "Y"), crs = 3857) %>%
  # 然后再转为费城投影
  st_transform(2272)
```

```{r}
# C. Police Stations (Guardianship) 
# 直接读取包含坐标的 GeoJSON 文件
police_sf <- st_read("data/Police_Stations.geojson", quiet = TRUE) %>%
  st_transform(2272) # 统一转为 PA State Plane

cat("Total Police Stations:", nrow(police_sf))
```

------------------------------------------------------------------------

# Phase 2: Feature Engineering

## **2.1 Buffer creation:**

### 2.1.1 公交站400m buffer

```{r}
# Create Buffer(400m)
bus_buffer <- st_buffer(bus_long, 1312)
```

### 2.1.2 Crime numbers

```{r}
### 2.1.2 Crime numbers (Corrected with Normalization)

# 1. 计算数据集中包含的“总天数” (Exposure)
# 这一步会自动计算你的 crime_sf 数据里涵盖了多少个工作日，多少个周末
day_counts <- crime_sf %>%
  st_drop_geometry() %>%
  group_by(is_crime_weekend) %>%
  summarise(n_days = n_distinct(crime_date)) # 统计有多少个不重复的日期

# 打印天数检查 (你会发现 Weekday 天数大约是 Weekend 的 2.5 倍)
print(day_counts)

# 2. 关联并计算日均犯罪 (Rate)
crime_agg <- st_join(bus_buffer, crime_sf, join = st_intersects) %>%
  filter(is_weekend == is_crime_weekend) %>%
  group_by(Stop_Code, is_weekend) %>%
  summarise(
    Crime_Total_Count = n() # 原始总数 (用于模型)
  ) %>%
  st_drop_geometry() %>%
  # 把天数信息 join 进来
  left_join(day_counts, by = c("is_weekend" = "is_crime_weekend")) %>%
  mutate(
    # 关键修正：计算日均犯罪量
    Crime_Daily_Rate = Crime_Total_Count / n_days,
    
    # 同时也保留 n_days，因为负二项回归模型需要它作为 offset
    Exposure_Days = n_days
  )

# 检查结果
head(crime_agg)

```
### 2.1.3 Spatial Feature

In this part 我们加入了犯罪的时间滞后变量，将上一季度的站点周边犯罪数量纳入考虑
```{r}
### 2.1.2 Lag Crime numbers (Corrected with Normalization)

# 1. 计算 Q1 数据集中包含的“总天数” (Exposure)
day_counts_q1 <- crime_q1 %>%
  st_drop_geometry() %>%
  group_by(is_crime_weekend) %>%
  summarise(n_days_lag = n_distinct(crime_date)) 

# 打印天数检查
print(day_counts_q1)

# 2. 关联并计算日均犯罪 (Rate) —— 完全复制你的 Q2 逻辑
crime_lag_q1 <- st_join(bus_buffer, crime_q1, join = st_intersects) %>%
  filter(is_weekend == is_crime_weekend) %>%
  group_by(Stop_Code, is_weekend) %>%
  summarise(
    Crime_Total_Count_lag = n()   # lag 原始总数
  ) %>%
  st_drop_geometry() %>%
  left_join(day_counts_q1, by = c("is_weekend" = "is_crime_weekend")) %>%
  mutate(
    Crime_Daily_Rate_lag = Crime_Total_Count_lag / n_days_lag,
    Exposure_Days_lag    = n_days_lag
  )

# 检查结果
head(crime_lag_q1)
```


### 2.1.3 POI Numbers（酒类销售点）

```{r}
# 2. Count Alcohol Outlets
alcohol_agg <- st_join(bus_buffer, alcohol_sf, join = st_intersects) %>%
  # 按 Stop 和 时间类型分组，这样每一行都会得到计数
  group_by(Stop_Code, is_weekend) %>%
  summarise(Alcohol_Count = n() - 1) %>% # Subtract 1 because st_join is left join (self-intersection NA check)
  st_drop_geometry()
```

### 2.1.4 Infrastructure Numbers（路灯密度）

```{r}
# 3. Count Street Lights
light_agg <- st_join(bus_buffer, lights_sf, join = st_intersects) %>%
  group_by(Stop_Code, is_weekend) %>%
  summarise(Light_Count = n() - 1) %>%
  st_drop_geometry()
```

### 2.1.5 Census Demographics

```{r}
# 4. Average Census Demographics
census_agg <- st_join(bus_buffer, philly_census, join = st_intersects) %>%
  group_by(Stop_Code, is_weekend) %>%
  summarise(
    Avg_Poverty = mean(Poverty_Rate, na.rm = TRUE),
    Avg_Income = mean(Med_Income, na.rm = TRUE),
    Avg_BA = mean(ba_rate, na.rm = TRUE),
    Avg_Unemployment = mean(unemployment_rate, na.rm = TRUE),
    Avg_Vacancy = mean(vacancy_rate, na.rm = TRUE)
  ) %>%
  st_drop_geometry()
```

## **2.2 k-Nearest Neighbor features:**

### 2.2.1 Police station (KNN-1)

```{r}
# Calculate distance to nearest police station
dist_matrix <- st_distance(bus_long, police_sf)

# 取每一行的最小值，并换算成英里
bus_long$Dist_Police <- apply(dist_matrix, 1, min) / 5280
```

## 2.3 Get Census Tract ID (GEOID) for Fixed Effects

```{r}
# 我们需要知道每个站点物理上位于哪个普查区 (Point in Polygon)
# 注意：只取 weekday=0 的数据来做空间匹配，避免重复计算
stop_geoid_mapping <- bus_long %>%
  filter(is_weekend == 0) %>% 
  st_join(philly_census %>% dplyr::select(GEOID)) %>% # 空间连接：点找面
  st_drop_geometry() %>%
  dplyr::select(Stop_Code, GEOID) %>%
  # 以防万一某个点在边界上匹配到了两个区，去重保留第一个
  distinct(Stop_Code, .keep_all = TRUE)

cat("GEOID mapping created for", nrow(stop_geoid_mapping), "stops.\n")
```

## 2.4 Merge Features into Master Dataset

```{r}
## 2.4 Merge Features into Master Dataset (Final Version)

final_data <- bus_long %>%
  # Join all aggregated tables
  left_join(crime_agg,     by = c("Stop_Code", "is_weekend")) %>%      # Q2 crime
  left_join(crime_lag_q1,  by = c("Stop_Code", "is_weekend")) %>%      # ⭐ 新增：Q1 lag crime
  left_join(alcohol_agg,   by = c("Stop_Code", "is_weekend")) %>%
  left_join(light_agg,     by = c("Stop_Code", "is_weekend")) %>%
  left_join(census_agg,    by = c("Stop_Code", "is_weekend")) %>% 
  
  # --- [新增] Join GEOID ---
  left_join(stop_geoid_mapping, by = "Stop_Code") %>%
  # -----------------------
  
  mutate(
    # 1. Handle NAs for Counts
    Crime_Total_Count = replace_na(Crime_Total_Count, 0),
    Crime_Daily_Rate  = replace_na(Crime_Daily_Rate, 0),

    # ⭐ 处理 lag crime 的 NA（如果某站点 Q1 没犯罪）
    Crime_Total_Count_lag = replace_na(Crime_Total_Count_lag, 0),
    Crime_Daily_Rate_lag  = replace_na(Crime_Daily_Rate_lag, 0),

    Alcohol_Count = replace_na(Alcohol_Count, 0),
    Light_Count   = replace_na(Light_Count, 0),
    
    # 2. Log transform Ridership
    Log_Ridership = log(Ridership + 1),
    
    # 3. Create Factor for Interaction Term
    is_weekend_factor = factor(is_weekend, levels = c(0, 1), 
                               labels = c("Weekday", "Weekend")),

  ) %>%
  
  # 5. Clean up
  filter(!is.na(GEOID)) %>%
  na.omit() 

cat("Final Panel Dataset Rows:", nrow(final_data))

```

------------------------------------------------------------------------

# Phase 3: Exploratory Data Analysis

## 3.1 Distribution of Crime and Bus Stop Ridership (histogram)

Does ridership follow a normal distribution? No. That's why we need Negative Binomial.

```{r}
library(gridExtra)
p1 <- ggplot(final_data, aes(x = Ridership)) +
  geom_histogram(fill = "#3182bd", bins = 50) +
  labs(title = "Distribution of Ridership", x = "Daily Boardings") + plotTheme

p2 <- ggplot(final_data, aes(x = Crime_Total_Count)) +
  geom_histogram(fill = "#de2d26", bins = 50) +
  labs(title = "Distribution of Crime", x = "Crime Count (400m)") + plotTheme

grid.arrange(p1, p2, ncol = 2)
```

**Key Findings:**

```{r}
# 检查均值和方差
mean_crime <- mean(final_data$Crime_Total_Count, na.rm = TRUE)
var_crime <- var(final_data$Crime_Total_Count, na.rm = TRUE)

cat("Mean:", mean_crime, "\n")
cat("Variance:", var_crime, "\n")
cat("Ratio (Var/Mean):", var_crime / mean_crime, "\n")
```

## 3.2 Spatial distribution of crime and Bus Stop Ridership(map)

```{r}
# 需要先加载 patchwork 包，如果前面没加载，这里补充一下
# library(patchwork) 

# --- 1. 准备绘图数据 (Extract Coordinates) ---

# A. 处理公交数据：只取工作日数据以避免重复，并提取坐标
bus_plot_data <- bus_long %>%
  filter(is_weekend == 0) %>%   # 仅使用工作日数据代表典型客流
  mutate(
    X = st_coordinates(geometry)[,1],
    Y = st_coordinates(geometry)[,2]
  ) %>%
  st_drop_geometry() # 移除几何列，方便 stat_density_2d 使用

# B. 处理犯罪数据：提取坐标
crime_plot_data <- crime_sf %>%
  mutate(
    X = st_coordinates(geometry)[,1],
    Y = st_coordinates(geometry)[,2]
  ) %>%
  st_drop_geometry()

# --- 2. 绘制图形 (Create Maps) ---

# 左图：客流热力图 (Ridership Density)
# 注意：aes(weight = Ridership) 是关键，让热力图基于客流量而不是站点数量
p_ridership <- ggplot() +
  # 底图：费城轮廓
  geom_sf(data = philly_boundary, fill = "#f5f5f5", color = "grey80") +
  # 热力层
  stat_density_2d(
    data = bus_plot_data, 
    aes(x = X, y = Y, fill = ..level.., weight = Ridership), 
    geom = "polygon", 
    alpha = 0.75
  ) +
  # 配色：蓝色系 (代表正常活动)
  scale_fill_distiller(palette = "Blues", direction = 1, guide = "none") +
  labs(
    title = "Weekday Ridership Hotspots", 
    subtitle = "High Transit Activity Zones"
  ) +
  mapTheme

# 右图：犯罪热力图 (Crime Density)
p_crime_map <- ggplot() +
  # 底图
  geom_sf(data = philly_boundary, fill = "#f5f5f5", color = "grey80") +
  
  # 热力层 (修改部分)
  stat_density_2d(
    data = crime_plot_data, 
    aes(x = X, y = Y, fill = ..level..), 
    geom = "polygon", 
    alpha = 0.4,       # 1. 调低透明度，减少"硬边"的感觉
    bins = 30,         # 2. 增加层数，让过渡更平滑 (原来默认很少)
    adjust = 0.5       # 3. 调低带宽 (0.5), 让热力点收缩，更聚焦局部热点
  ) +
  
  # 配色
  scale_fill_distiller(palette = "Reds", direction = 1, guide = "none") +
  labs(
    title = "Crime Hotspots", 
    subtitle = "High Incident Zones"
  ) +
  mapTheme

# --- 3. 组合展示 (Combine Side-by-Side) ---

# 使用 patchwork 进行拼接
combined_map <- p_ridership + p_crime_map +
  plot_annotation(
    title = "Spatial Mismatch Analysis: Eyes on the Street vs. Targets?",
    subtitle = "Left: Where people are (Ridership) | Right: Where crimes happen",
    theme = theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      plot.subtitle = element_text(size = 12, color = "grey40", hjust = 0.5)
    )
  )

# 输出图形
combined_map

```

**Key Findings:**

## 3.3 Crime vs. Bus Stop Ridership (scatter plots)

```{r eda_interaction}
# Crime vs. Ridership (Interaction Plot)
# 这是一个非常关键的图，用于验证你的核心假设：周末的客流影响是否不同？

ggplot(final_data, aes(x = Log_Ridership, y = Crime_Daily_Rate, color = is_weekend_factor)) +
  geom_point(alpha = 0.1, size = 1) +
  geom_smooth(method = "glm", method.args = list(family = "quasipoisson"), se = TRUE) +
  scale_color_manual(values = c("Weekday" = "#3182bd", "Weekend" = "#de2d26")) +
  labs(
    title = "Does Ridership impact Crime differently on Weekends?",
    subtitle = "Interaction Effect (Normalized by Number of Days)",
    x = "Log(Daily Ridership)",
    y = "Average Daily Crime Count (per 400m)", # 修改 Y 轴标签
    color = "Time Period"
  ) +
  plotTheme
```

坏了。。。。。。

**Key Findings:**

-   

## 3.4 Crime vs. Spatial & Social features (scatter plots)

```{r 3.4_scatter_plots}
library(patchwork)
library(scales)

# 为了可视化清晰，我们先创建一个带有 log(crime) 的临时绘图数据
plot_data <- final_data %>%
  mutate(
    log_crime = log(Crime_Daily_Rate + 0.01), # +1 避免 log(0)
    # 给 Income 取个 log 看看是否更线性，很多经济学变量 log 后效果更好
    log_income = log(Avg_Income + 1)
  )

# --- 1. POI & Infrastructure (Built Environment) ---

# A. Alcohol Outlets (线性还是指数增加？)
p1 <- ggplot(plot_data, aes(x = Alcohol_Count, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#6A1B9A") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Log(Crime) vs. Alcohol Outlets",
       subtitle = "Check for diminishing returns",
       x = "Count of Alcohol Outlets", y = "Log(Crime Count)") +
  plotTheme

# B. Street Lights (路灯越多越安全？还是路灯只出现在繁华区？)
p2 <- ggplot(plot_data, aes(x = Light_Count, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#6A1B9A") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Log(Crime) vs. Street Lights",
       subtitle = "Is the relationship linear?",
       x = "Count of Street Lights", y = "") +
  plotTheme

# C. Distance to Police (离警局越远越危险？)
p3 <- ggplot(plot_data, aes(x = Dist_Police, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#6A1B9A") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Log(Crime) vs. Dist to Police",
       subtitle = "Check for U-shape or threshold",
       x = "Distance to Station (Miles)", y = "") +
  plotTheme

# --- 2. Demographics (Census) ---

# D. Poverty Rate (贫困率与犯罪)
p4 <- ggplot(plot_data, aes(x = Avg_Poverty, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#3182bd") +
  geom_smooth(method = "loess", color = "orange", se = FALSE) +
  scale_x_continuous(labels = scales::percent) +
  labs(title = "Log(Crime) vs. Poverty Rate",
       x = "Poverty Rate", y = "Log(Crime Count)") +
  plotTheme

# E. Median Income (收入与犯罪 - 这是一个典型的可能需要 log 的变量)
p5 <- ggplot(plot_data, aes(x = Avg_Income, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#3182bd") +
  geom_smooth(method = "loess", color = "orange", se = FALSE) +
  scale_x_continuous(labels = scales::dollar) +
  labs(title = "Log(Crime) vs. Median Income",
       subtitle = "Does wealth shield against crime?",
       x = "Median Household Income", y = "") +
  plotTheme

# F. Vacancy Rate (空置率/破窗理论)
p6 <- ggplot(plot_data, aes(x = Avg_Vacancy, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#3182bd") +
  geom_smooth(method = "loess", color = "orange", se = FALSE) +
  scale_x_continuous(labels = scales::percent) +
  labs(title = "Log(Crime) vs. Vacancy Rate",
       x = "Housing Vacancy Rate", y = "") +
  plotTheme

# --- Combine Plots ---
(p1 | p2 | p3) / (p4 | p5 | p6) +
  plot_annotation(
    title = "Exploratory Analysis: Variable Functional Forms",
    subtitle = "Red Line = Loess Smoother (Non-linear trend)",
    theme = theme(plot.title = element_text(size = 16, face = "bold"))
  )
```

**Key Findings:**

-   

## 3.5 Correlation Matrix of Features for all Features

```{r}

library(ggcorrplot)

# 1. 选择数值型变量
numeric_vars <- final_data %>%
  st_drop_geometry() %>%
  dplyr::select(
    Crime_Daily_Rate, 
    Ridership, 
    Alcohol_Count, 
    Light_Count, 
    Dist_Police, 
    Avg_Poverty, 
    Avg_Income, 
    Avg_Unemployment,
    Avg_Vacancy
  )

# 2. 计算相关系数矩阵
corr_matrix <- cor(numeric_vars, use = "complete.obs")

# 3. 绘图
ggcorrplot(
  corr_matrix, 
  method = "square", 
  type = "lower", 
  lab = TRUE, 
  lab_size = 3, 
  colors = c("#6D9EC1", "white", "#E46726"), 
  title = "Correlation Matrix of Features",
  ggtheme = theme_minimal()
)

```

如果矩阵显示它们的相关系数超过 0.7，不要把它们都放进模型！所以可以说这里没有超过0.7的，可以都放？ 另外如果后面监测到多重共线性，可以用这个矩阵决定怎么处理数据

## 3.6 Crime distribution in weekdays and weekends

```{r}

### 3.5 Weekend Effect Boxplot

ggplot(final_data, aes(x = is_weekend_factor, y = Crime_Daily_Rate, fill = is_weekend_factor)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) + 
  
  # 加上均值点
  stat_summary(fun = mean, geom = "point", shape = 23, size = 3, fill = "white") +
  
  # 限制 Y 轴范围 (注意：现在的单位是“每天”，数值会很小，比如 0.05)
  # 使用 quantile 自动截断极端值
  coord_cartesian(ylim = c(0, quantile(final_data$Crime_Daily_Rate, 0.95))) +
  
  scale_fill_manual(values = c("Weekday" = "#3182bd", "Weekend" = "#de2d26")) +
  
  labs(
    title = "Daily Crime Risk: Weekday vs. Weekend",
    subtitle = "Comparison of Average Daily Crime Counts per Stop",
    x = "Time Period",
    y = "Average Daily Crime Count (per 400m)", # 修改标签
    caption = "Note: Values represent daily averages to account for fewer weekend days per year."
  ) +
  plotTheme
```

坏了。。。。

**Key Findings:**

-   **Spatial Variation in Property Values:** The average sale price per square foot shows significant geographic clustering across census tracts, with distinct high-value areas concentrated in specific neighborhoods. This indicates strong spatial autocorrelation in housing prices, where adjacent tracts tend to have similar price levels.

-   **Correlation Between Property Condition and Location:** Better average interior conditions are systematically concentrated in particular geographic areas, suggesting that housing maintenance and quality are not randomly distributed but follow spatial patterns that may correlate with neighborhood characteristics and property values.

-   **Heterogeneous Distribution of Housing Size:** The average livable area varies substantially across census tracts, with larger properties clustered in specific regions. This spatial patterning of housing size complements the price distribution, indicating that both property characteristics and location factors contribute to the overall housing market structure in Philadelphia.

------------------------------------------------------------------------

# Phase 4: Model Building(负二项式)

## **4.1 Ridership only:**

```{r Model_1+Summary}

model_1 <- glm.nb(Crime_Total_Count ~ 
                    Log_Ridership + 
                    offset(log(Exposure_Days)), 
                  data = final_data)
summary(model_1)

```

**Coefficient Interpretation:**

## \*\*4.2 The Interaction (核心假设)

```{r Model_2+Summary}
model_2 <- glm.nb(Crime_Total_Count ~ 
                    Log_Ridership * is_weekend_factor + 
                    offset(log(Exposure_Days)), 
                  data = final_data)
summary(model_2)
```

**Coefficient Interpretation:**

-   Coefficient Evolution (vs. Model 1):

-   New Variable Interpretation:

## **4.3 Built Environment and Demographics features:**(加入环境控制)

```{r Model_3+Summary}
model_3 <- glm.nb(Crime_Total_Count ~ 
                    Log_Ridership * is_weekend_factor + 
                    log(Alcohol_Count + 1) +        # 诱发因子
                    poly(Light_Count, 2) +          # 预防因子
                    Avg_Poverty +          # 社会结构
                    Avg_Vacancy + I(Avg_Vacancy^2) +          # 破窗效应
                    Avg_Income + I(Avg_Income^2)  +
                    Avg_Unemployment +
                    Dist_Police +
                    offset(log(Exposure_Days)), 
                  data = final_data)
summary(model_3)
```

**Coefficient Interpretation:**

-   Coefficient Evolution (vs. Model 2):

-   New Variable (Spatial) Interpretation:

## **4.4 空间固定效应:**

```{r Model_4+Summary}
# 加入 'GEOID' (Census Tract) 作为固定效应
# 这相当于为费城每一个普查区都加了一个“基准拦截”，控制了所有观测不到的社区特征
model_4 <- glm.nb(Crime_Total_Count ~ 
                    Log_Ridership * is_weekend_factor + 
                    log(Alcohol_Count + 1) +        # 诱发因子
                    poly(Light_Count, 2) +          # 预防因子
                    Avg_Poverty +          # 社会结构
                    Avg_Vacancy + I(Avg_Vacancy^2) +          # 破窗效应
                    Avg_Income + I(Avg_Income^2)  +
                    Avg_Unemployment +
                    Dist_Police +          # 政策变量：警力可达性
                    factor(GEOID) +      # <--- 如果模型跑不动(不收敛)，注释掉这一行
                    offset(log(Exposure_Days)), 
                  data = final_data,
                  control = glm.control(maxit = 100)) # 增加迭代次数以防不收敛
summary(model_4)
```

## Model 5 crime temporal lag
```{r Model_5+Summary}
# 加入警局距离，并加入 'GEOID' (Census Tract) 作为固定效应
# 这相当于为费城每一个普查区都加了一个“基准拦截”，控制了所有观测不到的社区特征
model_5 <- glm.nb(Crime_Total_Count ~ 
                    Log_Ridership * is_weekend_factor + 
                    log(Alcohol_Count + 1) +        # 诱发因子
                    poly(Light_Count, 2) +          # 预防因子
                    Avg_Poverty +          # 社会结构
                    Avg_Vacancy + I(Avg_Vacancy^2) +          # 破窗效应
                    Avg_Income + I(Avg_Income^2)  +
                    Avg_Unemployment +
                    Dist_Police +          # 政策变量：警力可达性
                    factor(GEOID) + 
                    log(Crime_Daily_Rate_lag + 0.001) +
                    offset(log(Exposure_Days)), 
                  data = final_data,
                  control = glm.control(maxit = 100)) # 增加迭代次数以防不收敛
summary(model_5)
```

在加入geoid前，aic等于77655，morans i约等于0.5；加入geoid之后，aic等于74009，morans i 约等于0.29，并且只有其他一个其他变量Avg_Poverty从显著变成了不显著。
GEOID 的作用： 当你放入factor(GEOID)，模型就已经知道了“这个区是贫困区”。
如果变量变得不显著： 说明这个变量对犯罪的影响，已经被 GEOID 这个“大标签”完全解释掉了。
如果其他变量依然显著： 这才是最牛的地方！说明像 客流 (Ridership)、路灯、酒铺这些变量，即使在控制了社区背景后，依然对犯罪有独立的、显著的影响。

而且我觉得用census tract可能是最能降morans i的了，其他空间区分方式（比如警区）更是很大一块

总体看来很需要加入geoid作为固定项。AIC 下降 3000 是硬道理，说明模型拟合度极佳。变量显著性稳定，说明模型结构健康。Moran's I 的0.29虽然不完美，但在实际应用类项目中是可以容忍的（你可以说这是 limitations）。

如何解释那个不显著的变量？
在结果表格里，如实汇报它变得不显著。
解释为：“该变量的影响已被社区固定效应所包含（Captured by neighborhood fixed effects）。”


```{r VIF Validation}
vif(model_5)
```

其实在加入geoid做固定效应之后，vif变得很屎（看第二列的平方值），但是log ridership还行（1.53^2=2.3）。
多重共线性（High VIF）的主要后果是让相关变量的系数变得不稳定（方差变大，导致显著性消失）。在你的模型里，Light_Count 和 GEOID 打架，导致路灯的系数废了。但是，Log_Ridership 没有参与这场架（它的 VIF 很低）。

只要核心变量不共线，且模型设定正确（AIC 降低证明了这一点），你对 Log_Ridership 的系数估计依然是无偏且有效的。

以前的解释（不加 GEOID）：
“客流高的站点犯罪率高，但我们不确定是因为客流本身，还是因为这些站点恰好都在贫民窟或治安差的区域。”

现在的解释（加了 GEOID）：
“我们在同一个普查区（Census Tract）内部进行了比较。即便是在同一个社区、面临同样的贫困水平和同样的警力管辖下，那个客流更大的站点，依然比它隔壁客流小的站点有更高的犯罪风险。”

对于不显著的变量，就简单报告：加入了建成环境（路灯、酒铺）作为控制变量，并加入了社区固定效应以消除偏差。虽然这些变量吸收了空间变异，但我们的核心关注点依然是客流带来的稳健影响……


## **4.5 输出对比表格**

```{r}
# 1. 加载必要的包
library(modelsummary)
library(ggplot2)
library(dplyr)

# 2. 定义变量名的“美化映射” (跟 Stargazer 的 label 对应)
coef_map_list <- c(
  "Log_Ridership" = "Ridership (Log)",
  "is_weekend_factorWeekend" = "Weekend Effect",
  "Log_Ridership:is_weekend_factorWeekend" = "Interaction: Ridership × Weekend",
  "Alcohol_Count" = "Alcohol Outlets",
  "Light_Count" = "Street Lights",
  "Avg_Poverty" = "Poverty Rate",
  "Avg_Vacancy" = "Vacancy Rate",
  "Dist_Police" = "Dist to Police Station",
  "Crime_Total_Count_lag" = "Temporal lag: Crime count(quarter)"
)

# 3. 绘制系数图
# 我们把四个模型放进去，它会自动用颜色区分
p_models <- modelplot(
  list(
    "1. Naive" = model_1, 
    "2. Weekend" = model_2, 
    "3. Context" = model_3, 
    "4. Robust" = model_4,
    "5. Temporal" = model_5
  ),
  coef_map = coef_map_list, # 只画这些变量，自动过滤掉 Intercept 和 GEOID
  conf_level = 0.95         # 95% 置信区间
) +
  # --- 以下是标准的 ggplot 美化代码 ---
  geom_vline(xintercept = 0, color = "red", linetype = "dashed") + # 添加 0 刻度线
  labs(
    title = "What Drives Crime Risk?",
    subtitle = "Comparison of Coefficients across 4 Models",
    x = "Effect Size (Coefficient Estimate)",
    y = "",
    caption = "Points represent estimates; bars represent 95% confidence intervals.\nIf the bar does not cross the red line, the effect is statistically significant."
  ) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") + # 使用对比度高的配色
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 14),
    axis.text.y = element_text(size = 11, face = "bold")
  )

# 展示图形
p_models
```

**Coefficient Interpretation:**

-   Fixed Effects Interpretation:

# Phase 5: Model Validation

## **5.1 Compare all 5 models:**

### 5.1.1 Create predicted vs. actual plot

```{r}
# 1. 重新定义五个模型的公式 (确保与 Phase 4 一致)

f1 <- Crime_Total_Count ~ Log_Ridership + offset(log(Exposure_Days))
f2 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + offset(log(Exposure_Days))
f3 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + log(Alcohol_Count + 1) + poly(Light_Count, 2) + Avg_Poverty + Avg_Vacancy + I(Avg_Vacancy^2) + Avg_Income + I(Avg_Income^2) + Avg_Unemployment + Dist_Police + offset(log(Exposure_Days))

f4 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + log(Alcohol_Count + 1) + poly(Light_Count, 2) + Avg_Poverty + Avg_Vacancy + I(Avg_Vacancy^2) + Avg_Income + I(Avg_Income^2) + Avg_Unemployment + Dist_Police + factor(GEOID) + offset(log(Exposure_Days)) 

f5 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + log(Alcohol_Count + 1) + poly(Light_Count, 2) + Avg_Poverty + Avg_Vacancy + I(Avg_Vacancy^2) + Avg_Income + I(Avg_Income^2) + Avg_Unemployment + Dist_Police + factor(GEOID) + log(Crime_Total_Count_lag) + offset(log(Exposure_Days)) 

# 2. 训练五个模型
m1 <- glm.nb(f1, data = final_data)
m2 <- glm.nb(f2, data = final_data)
m3 <- glm.nb(f3, data = final_data)
m4 <- glm.nb(f4, data = final_data)
m5 <- glm.nb(f5, data = final_data)

# 3. 生成预测数据 (Gather Predictions)
# type = "response" 会直接返回预测的犯罪数量 (Count)，不需要再 exp
plot_data <- final_data %>%
  dplyr::select(Crime_Total_Count) %>%
  mutate(
    Model_1_Pred = predict(m1, type = "response"),
    Model_2_Pred = predict(m2, type = "response"),
    Model_3_Pred = predict(m3, type = "response"),
    Model_4_Pred = predict(m4, type = "response"),
    Model_5_Pred = predict(m5, type = "response")
  ) %>%
  pivot_longer(
    cols = starts_with("Model"), 
    names_to = "Model", 
    values_to = "Predicted_Count"
  )

# 4. 绘图：Facet Wrap 对比五个模型
# 因为犯罪是计数数据，点会重叠，我们使用 alpha 和 45度线
ggplot(plot_data, aes(x = Predicted_Count, y = Crime_Total_Count)) +
  geom_point(alpha = 0.2, color = "#3182bd", size = 0.8) +
  
  # 添加完美预测线 (y=x)
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  
  facet_wrap(~Model, ncol = 2) +
  
  labs(
    title = "Predicted vs. Actual Crime Counts",
    subtitle = "Comparing Goodness-of-Fit Across Models",
    x = "Predicted Crime Count",
    y = "Actual Crime Count"
  ) +
  theme_bw() +
  # 限制坐标轴范围以便看清核心区域 (去掉极值干扰)
  coord_cartesian(xlim = c(0, 50), ylim = c(0, 50))
```

### 5.1.2 Report and Compare RMSE, MAE, R²

```{r}
library(caret)

# 1. 设置 5-Fold Cross Validation
set.seed(999)
folds <- createFolds(final_data$Crime_Total_Count, k = 5, list = TRUE)

# 定义一个辅助函数：跑 CV 并计算指标
run_cv <- function(formula, data, folds) {
  mae_list <- c()
  rmse_list <- c()
  
  for (i in 1:length(folds)) {
    # Split Data
    test_idx <- folds[[i]]
    train_set <- data[-test_idx, ]
    test_set  <- data[test_idx, ]
    
    # Train Model (tryCatch 防止不收敛报错)
    model <- tryCatch({
      glm.nb(formula, data = train_set)
    }, error = function(e) return(NULL))
    
    if(!is.null(model)) {
      # Predict (type = "response" 返回预测的数量)
      preds <- predict(model, newdata = test_set, type = "response")
      
      # Calculate Errors
      actuals <- test_set$Crime_Total_Count
      mae_list <- c(mae_list, mean(abs(actuals - preds)))
      rmse_list <- c(rmse_list, sqrt(mean((actuals - preds)^2)))
    }
  }
  return(c(mean(mae_list), mean(rmse_list)))
}

# 2. 对五个模型分别跑 CV
# 这里的公式需要跟上面定义的一致
results_m1 <- run_cv(f1, final_data, folds)
results_m2 <- run_cv(f2, final_data, folds)
results_m3 <- run_cv(f3, final_data, folds)
results_m4 <- run_cv(f4, final_data, folds)
results_m5 <- run_cv(f5, final_data, folds)

# 3. 汇总表格
validation_summary <- data.frame(
  Model = c("1. Ridership Only", "2. Interaction", "3. Env & Demo", "4. Policy & Dist", "5. Temporal lag"),
  MAE  = c(results_m1[1], results_m2[1], results_m3[1], results_m4[1], results_m5[1]),
  RMSE = c(results_m1[2], results_m2[2], results_m3[2], results_m4[2], results_m5[2])
) %>%
  mutate(
    # 计算相对于 Model 1 的提升百分比
    Improvement_MAE = (MAE[1] - MAE) / MAE[1]
  )

# 4. 展示漂亮的表格
kable(validation_summary, digits = 3, caption = "5-Fold Cross-Validation Metrics") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE) %>%
  column_spec(4, color = "green", bold = TRUE)
```

**Discussion of the most mattered features**:

Result Interpretation (Interpretation Guide for Report):

MAE (Mean Absolute Error):看 Model 3 或 Model 4 的 MAE。假设 MAE = 2.5，这意味着你的模型对每个站点整个夏天的犯罪预测，平均误差在 2.5 起案件左右。

Improvement:对比 Model 1 (Baseline) 和 Model 2 (Interaction)。如果 Model 2 的误差更低，说明把周末和工作日分开预测是必要的（支持了你的 Shark Tank Pitch）。对比 Model 3/4。如果加入了环境变量（酒类、路灯）后误差大幅下降，说明环境因素比单纯的客流更重要。

# Phase 6: Model Diagnostics

### Check assumptions for best model:

## **6.1 Spatial Autocorrelation of Residuals (Moran's I)**

```{r}

# 1. 明确指定 Model 5 为最佳模型
best_model <- model_5 

# 2. 提取残差
# Pearson 残差用于统计检验（因为它是标准化的）
# Deviance 残差用于绘图（因为视觉上更能反映拟合优度）
final_data$resid_pearson  <- residuals(best_model, type = "pearson")
final_data$resid_deviance <- residuals(best_model, type = "deviance")

# 3. 构建空间权重矩阵 (Spatial Weights Matrix)
# 由于公交站点是离散点，使用 k-Nearest Neighbors (KNN) 比距离阈值更稳健
# 这里选取 k=8，意味着每个站点对比它最近的 8 个邻居
coords <- st_coordinates(final_data)
neighbor_nb <- knn2nb(knearneigh(coords, k = 8))
spatial_weights <- nb2listw(neighbor_nb, style = "W")

# 4. 运行 Moran's I 检验
moran_result <- moran.test(final_data$resid_pearson, spatial_weights)

# 打印结果
print(moran_result)

# 解释逻辑：
# 如果 p-value > 0.05: 恭喜！残差是随机分布的，模型非常完美，没有遗漏空间变量。
# 如果 p-value < 0.05 但 Moran's I 很小 (如 < 0.1): 模型还可以，只有轻微的空间依赖。

```


```{r}

# Spatial Distribution of Residuals
# 1. 将残差添加回空间数据
# type = "deviance" residuals are often better for mapping goodness-of-fit
final_data$spatial_resid <- residuals(best_model, type = "deviance")

# 2. 绘制地图
ggplot() +
  # 底图
  geom_sf(data = philly_boundary, fill = "grey95", color = NA) +
  
  # 站点残差图
  geom_sf(data = final_data, 
          aes(color = spatial_resid), 
          size = 0.8, alpha = 0.7) +
  
  # 颜色比例尺
  scale_color_gradient2(
    low = "blue", mid = "grey90", high = "red",
    midpoint = 0,
    name = "Deviance\nResidual",
    # 限制范围，防止极个别离群值破坏颜色分布
    limits = c(-3, 3), 
    oob = scales::squish 
  ) +
  
  labs(
    title = "Map of Model Residuals",
    subtitle = "Red = Unexpectedly High Crime (Under-predicted)\nBlue = Unexpectedly Low Crime (Over-predicted)"
  ) +
  mapTheme

```

**Interpretation**:

-   Interpretation:

Red Spots (Positive Residuals): These are "Anomalies". The model (based on income, lighting, ridership) predicted low crime, but actual crime was high. These are targets for specific police intervention or CPTED audits.

Blue Spots (Negative Residuals): These areas are safer than their environment suggests. What are they doing right?

morans I还是很高，但是空间负二项式回归很难做，我们只能保留这个高Morans i作为limitation：
C. 数学上的噩梦：“非线性链接” (The Link Function Problem)这是最硬核的技术原因。线性模型 (OLS): $Y = X\beta + \text{Spatial Effects} + \epsilon$。加法运算，很容易解。广义线性模型 (GLM/NegBin): $\ln(Y) = X\beta$。如果你想把空间效应加进去，模型就变成了 $\ln(Y) = \rho W Y + X\beta$。问题来了： $Y$ 在左边取了对数，在右边又是原始值。这导致似然函数（Likelihood Function）变得极其复杂，无法像普通回归那样求出解析解，必须用非常复杂的马尔可夫链蒙特卡洛（MCMC/Bayesian）方法去“猜”参数。

可以说：我们使用负二项模型，因为标准空间模型无法处理离散的计数数据。但为了修正空间自相关，我们引入了固定效应/空间滤波。这是一种“两全其美”的策略：既尊重了数据的计数属性，又控制了空间聚集风险。

## **6.1 Residual plot:（找实际犯罪\>预测）**

```{r}
best_model <- model_5

# 1. 提取拟合值和残差
# 注意：对于 glm.nb，必须指定 type = "pearson" 来标准化残差
model_data <- data.frame(
  Fitted = fitted(best_model),
  Residuals = residuals(best_model, type = "pearson")
)

# 2. 绘制残差 vs. 拟合值图
p_resid_fitted <- ggplot(model_data, aes(x = log(Fitted), y = Residuals)) +
  geom_point(alpha = 0.3, color = "#6A1B9A", size = 1.5) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
  geom_smooth(method = "loess", color = "black", se = FALSE, linewidth = 0.8) +
  labs(
    title = "Residuals vs Fitted Values",
    subtitle = "Checking for systematic bias in the Count Model",
    x = "Log(Fitted Values) - Predicted Crime Count",
    y = "Pearson Residuals"
  ) +
  plotTheme

p_resid_fitted
```
Interpretation:

Ideal Plot: The points should be scattered randomly around the horizontal red line (0).

Curvature? If the black line curves significantly, it suggests we might be missing a non-linear variable (like a squared term).

Fan Shape? Ideally, the spread should be roughly constant. Since we used Negative Binomial (which handles overdispersion), we hope to see a relatively even spread compared to a Poisson model.

"The Integer Effect" (整数效应) 或 "Discreteness" (离散性)

不要惊慌，这些线条不是模型错误的标志，反而证明了你使用 负二项回归 (Count Model) 来处理犯罪数据是正确的选择。

没看懂啥原理，对就完了

## **6.2 Q-Q plot:**

```{r}
p_qq <- ggplot(model_data, aes(sample = Residuals)) +
  stat_qq(color = "#6A1B9A", size = 1.5, alpha = 0.5) +
  stat_qq_line(color = "red", linetype = "dashed", linewidth = 1) +
  labs(
    title = "Q-Q Plot of Pearson Residuals",
    subtitle = "Checking for extreme outliers in count data",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  ) +
  plotTheme

p_qq
```

**Interpretation**:

-   Deviation at tails: It is common for count data models to deviate at the extreme ends.

Interpretation: If the points mostly follow the line in the middle range, the model is acceptable. If there is a massive curve, the Negative Binomial assumption might still be struggling with extreme overdispersion.


## **6.3** Top 50 风险站点地图**:**

```{r}
# 1. 提取预测值 (Predicted Counts)
final_data <- final_data %>%
  mutate(Predicted_Crime = predict(best_model, type = "response"))

# 2. 筛选前 50 个高风险站点
top_50_risky <- final_data %>%
  arrange(desc(Predicted_Crime)) %>%
  slice(1:50)

# 3. 绘制行动地图
ggplot() +
  # 背景底图
  geom_sf(data = philly_boundary, fill = "grey95", color = "white") +
  
  # 所有站点 (灰色背景)
  geom_sf(data = final_data, color = "grey80", size = 0.5, alpha = 0.3) +
  
  # 高风险站点 (红色醒目)
  geom_sf(data = top_50_risky, color = "red", size = 2, alpha = 0.9) +
  
  # (可选) 加上文字标签
  # geom_sf_text(data = head(top_50_risky, 5), aes(label = Stop), dy = 100, size = 3) +

  labs(
    title = "Top 50 High-Risk Bus Stops",
    subtitle = "Priority Zones for Police Patrol Deployment (Based on Model Prediction)",
    caption = "Red dots represent the stops with the highest predicted crime counts."
  ) +
  mapTheme
```

**Interpretation**:

-   **Overall**These 50 stops represent the "Hotspots" identified by our algorithm.

Actionable Insight: "We recommend SEPTA Police allocate dedicated patrol units to these specific locations during peak hours. By focusing resources on these top 1% of stops, we can potentially address a significant portion of the total crime risk."

这很好，加了geoid之后，除了市中心还有另一坨红色

------------------------------------------------------------------------

# Phase 7: Conclusions

## 7.1 Results:

**系数解读:**

-   如果客流系数是 **负**的 -\> **支持“街道眼”理论**（人多反而安全）。

-   如果客流系数是 **正**的 -\> **支持“潜在受害者”理论**（人多扒窃多）。

## 7.2 The Action Plan:

-   **警力部署:** 根据预测的风险值，生成“每日巡逻热点图”。

-   **环境设计 (CPTED):** 对于那些高犯罪站点，如果是路灯不够，装灯；如果是空置房太多，清理社区。

## **7.3 Equity concerns**

-   **Which neighborhoods are hardest to predict?**

-   **Any data bias?**
