---
title: "Safe Passage: Modeling Crime Risk around SEPTA Bus Stops"
author: "Xinyuan Cui, Yuqing Yang, Jinyang Xu"
format: 
  html:
    code-fold: show
    toc: true
    toc-location: left
    theme: cosmo
execute:
  warning: false
  message: false
---

预测“**客流带来的社会影响 (公共安全)**”，“**帮助警察部门救命**”。Shark Tank 的评委（通常是政府官员）会对“安全”话题极其敏感。

核心问题： 客流大到底是带来了“街道眼 (Eyes on the Street)”从而抑制了犯罪，还是带来了更多的“潜在受害者 (Targets)”从而吸引了扒窃？

政策目标： 帮助 SEPTA 警察部门 (Transit Police) 预测哪些站点周边是犯罪高风险区，从而优化巡逻警力部署。

选题：做 **“Bus Ridership -\> Crime”** 模型

**因变量** (预测目标): 站点周边 400米内的犯罪数量 (Crime Incidents Count)

**自变量**: Regional Rail Ridership: 使用你的 Regional_Rail_Station_Summary.csv。

其他自变量: 人口统计 (Census): 贫困率、年轻男性比例。建成环境 (Built Env): 路灯密度 (Street Light Locations)、空置地块 (Vacant Land)。时间特征: 如果你做面板数据，可以加入周末/工作日。“诱发因子” ：**酒类销售点 (Alcohol Outlets):***数据源:* OpenDataPhilly (Liquor Licenses).+**商业密度 (Commercial Density):***数据源:* Land Use (Zoning) 或 OSM。**房屋空置率 (Vacancy Rate) / 311 投诉:***数据源:* OpenDataPhilly。**距离最近警局的距离 (Dist to Police Station):** *数据源:* OpenDataPhilly (Police Stations)。

模型方法: Negative Binomial Regression (负二项回归): 犯罪数据也是计数数据，且离散度极高（很多地方没犯罪，少数地方特别多）。

Local Spatial Regression (GWR): 探索客流对犯罪的影响在不同社区是否不同？

Shark Tank Pitch: "SEPTA 的首要任务是安全。我们的模型不预测有多少人坐车，而是预测哪里最需要警察。我们利用客流数据作为核心指标，识别出那些‘高客流但高犯罪’的异常站点，建议在此增加监控和巡逻。"

# Phase 1: Data Preparation

In this part, I load the datasets that will be used in the analysis. our main dataset is : Philadelphia Septa bus stop summaryboundaries datasets and burglaries dataset. After that, I will visualize the burglaries on the map to gain an intuitive, preliminary understanding of the spatial distribution of burglaries and where the major hotspots are located.

## 1.0 Complete data cleaning code

***Load necessary libraries***

```{r}
library(tidyverse)
library(sf)
library(tidycensus)
library(tigris)
options(tigris_use_cache = TRUE, tigris_class = "sf")
library(MASS)
library(spdep)
library(dplyr)
library(scales)
library(ggplot2)
library(caret)
library(nngeo)
library(car)
library(knitr)
library(readr)
library(patchwork)
library(kableExtra)
library(lubridate)
# Add this near the top of your .qmd after loading libraries
options(tigris_use_cache = TRUE)
options(tigris_progress = FALSE)  
```

***Define themes***

```{r themes}
plotTheme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  plot.subtitle = element_text(size = 10),
  plot.caption = element_text(size = 8),
  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
  axis.text.y = element_text(size = 10),
  axis.title = element_text(size = 11, face = "bold"),
  panel.background = element_blank(),
  panel.grid.major = element_line(colour = "#D0D0D0", size = 0.2),
  panel.grid.minor = element_blank(),
  axis.ticks = element_blank(),
  legend.position = "right"
)

mapTheme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  plot.subtitle = element_text(size = 10),
  plot.caption = element_text(size = 8),
  axis.line = element_blank(),
  axis.text = element_blank(),
  axis.ticks = element_blank(),
  axis.title = element_blank(),
  panel.background = element_blank(),
  panel.border = element_blank(),
  panel.grid.major = element_line(colour = 'transparent'),
  panel.grid.minor = element_blank(),
  legend.position = "right",
  plot.margin = margin(1, 1, 1, 1, 'cm'),
  legend.key.height = unit(1, "cm"),
  legend.key.width = unit(0.2, "cm")
)

palette5 <- c("#eff3ff", "#bdd7e7", "#6baed6", "#3182bd", "#08519c")
```

## **1.1 Load and clean bus stop ridership data:**

### 1.1.1 Load bus stop ridership data

```{r}
# 1. Load Bus Data
bus_raw <- read_csv("data/Summer_2025_Stop_Summary_(Bus).csv")
```

### 1.1.2 Clean

```{r}
# 2. Get Philadelphia Boundary
philly_boundary <- counties(state = "PA", cb = TRUE, class = "sf") %>%
  filter(NAME == "Philadelphia") %>%
  st_transform(2272)

```

```{r}
# 3. Process Ridership: Create Long Format (Aggregated + Weekday vs Weekend)
bus_long <- bus_raw %>%
  # A. 基础清洗
  filter(!is.na(Lat) & !is.na(Lon)) %>%
  
  # B. 预计算每个物理站牌的客流 (Pre-calculate per stop_code)
  mutate(
    Raw_Weekday = Weekdays_O + Weekdays_1,
    # Weekend average = (Sat total + Sun total) / 2
    Raw_Weekend = (Saturdays_ + Saturdays1 + Sundays_On + Sundays_Of) / 2
  ) %>%
  
  # C. [新增步骤] 聚合双向站点 (Aggregation by Stop Name)
  # 这一步把同名的站点（比如 "Broad St & Walnut St" 的双向）合二为一
  group_by(Stop) %>% 
  summarise(
    # 两个方向的客流相加
    Ridership_Weekday = sum(Raw_Weekday, na.rm = TRUE),
    Ridership_Weekend = sum(Raw_Weekend, na.rm = TRUE),
    # 坐标取平均值
    Lat = mean(Lat, na.rm = TRUE),
    Lon = mean(Lon, na.rm = TRUE),
    # 保留一个 Stop_Code 作为 ID
    Stop_Code = first(Stop_Code),
    .groups = "drop"
  ) %>%
  
  # D. Pivot to Long Format (把宽数据变长数据)
  # 现在每个站点会变成两行：一行 Weekday，一行 Weekend
  pivot_longer(
    cols = c(Ridership_Weekday, Ridership_Weekend),
    names_to = "Time_Type",
    values_to = "Ridership"
  ) %>%
  
  # E. Create Dummy Variable & Clean up
  mutate(
    is_weekend = if_else(Time_Type == "Ridership_Weekend", 1, 0),
    Time_Category = if_else(is_weekend == 1, "Weekend", "Weekday")
  ) %>%
  
  # F. Convert to SF & Clip
  st_as_sf(coords = c("Lon", "Lat"), crs = 4326) %>%
  st_transform(2272) %>%
  st_intersection(philly_boundary) %>%
  dplyr::select(Stop_Code, Stop, Ridership, is_weekend, Time_Category, geometry)

# Check results
cat("Total Aggregated Stops (Unique Locations):", nrow(bus_long) / 2, "\n")
cat("Total Observation Rows (Panel Data):", nrow(bus_long), "\n")
```

### 1.1.3 Visualization

```{r map_stations}
# 1. Prepare Base Map (Philly Boundary)
# 确保之前已经加载了 tigris 包并获取了边界
# philly_boundary <- counties(state = "PA", cb = TRUE, class = "sf") %>%
#   filter(NAME == "Philadelphia") %>%
#   st_transform(2272)

# 2. Plot the Map
ggplot() +
  # A. Base Layer: Philadelphia County Background
  geom_sf(data = philly_boundary, 
          fill = "grey98", 
          color = "grey50", 
          size = 0.5) +
  
  # B. Station Layer: Simple Points
  # [修改点] 只筛选 Weekday 的行来画图，避免每个点画两次导致重叠
  geom_sf(data = bus_long %>% filter(is_weekend == 0), 
          color = "#3182bd",  # SEPTA Blue
          size = 0.1,         # Small dots to avoid clutter
          alpha = 0.6) +      # Slight transparency
  
  # C. Styling
  labs(
    title = "Spatial Coverage of SEPTA Bus Network",
    # [修改点] 修正 nrow 除法的语法
    subtitle = paste0("Total Aggregated Stops: ", nrow(bus_long) / 2),
    caption = "Source: SEPTA Summer 2025 Stop Summary"
  ) +
  theme_void() + # Clean look
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 10, color = "grey40", hjust = 0.5),
    plot.margin = margin(1, 1, 1, 1, "cm")
  )
```

## **1.2 Load and clean secondary dataset:**

### 1.2.1 Crime data:

```{r}
### 1.2.1 Crime data:

# 定义“街头犯罪”列表 (排除室内/家庭纠纷/非蓄意犯罪)
# target_crime_types <- c(
#   "Robbery Firearm",
#   "Robbery No Firearm",
#   "Aggravated Assault Firearm",
#   "Aggravated Assault No Firearm",
#   "Homicide - Criminal",
#   "Rape",
#   "Other Sex Offenses (Not Commercialized)",
#   "Weapon Violations",
#   "Narcotic / Drug Law Violations",
#   "Vandalism/Criminal Mischief",
#   "Prostitution and Commercialized Vice",
#   "Public Drunkenness"
# )

crime_raw <- read_csv("data/crime2024.csv")

# Filter & Transform
crime_raw <- crime_raw %>%
  filter(!is.na(lat) & !is.na(lng))

# 2. 单独保存 1–3 月的数据
crime_q1 <- crime_raw %>%
  filter(lubridate::month(dispatch_date) %in% 1:3) %>%
  mutate(
    crime_date = as.Date(dispatch_date), 
    day_of_week = wday(crime_date), # 1 is Sunday, 7 is Saturday
    is_crime_weekend = if_else(day_of_week %in% c(1, 7), 1, 0)
  ) %>%
    
  st_as_sf(coords = c("lng", "lat"), crs = 4326) %>%
  st_transform(2272)  

 # 1. 筛选 4–6 月
crime_sf <- crime_raw %>%
  filter(lubridate::month(dispatch_date) %in% 4:6)%>%

  # 1. 筛选：只保留列表中的犯罪类型
  #filter(text_general_code %in% target_crime_types) %>%
  
  # 2. 时间特征处理
  mutate(
    crime_date = as.Date(dispatch_date), 
    day_of_week = wday(crime_date), # 1 is Sunday, 7 is Saturday
    is_crime_weekend = if_else(day_of_week %in% c(1, 7), 1, 0)
  ) %>%
  
  st_as_sf(coords = c("lng", "lat"), crs = 4326) %>%
  st_transform(2272)

# 打印检查
cat("Total Selected Crimes (Count):", nrow(crime_sf), "\n")
```

### 1.2.2 Census data (tidycensus):

```{r census_key, eval=FALSE}

# census_api_key("42bf8a20a3df1def380f330cf7edad0dd5842ce6", overwrite = TRUE, install = TRUE)
```

```{r census_key_hidden, include=FALSE}
# Hidden key for rendering
# census_api_key("42bf8a20a3df1def380f330cf7edad0dd5842ce6")
```


```{r}
# Load Census data for Philadelphia tracts
philly_census <- get_acs(
  geography = "tract",
  variables = c(
    total_pop = "B01003_001",
    poverty_pop = "B17001_002",
    med_income = "B19013_001",
    ba_degree = "B15003_022",
    total_edu = "B15003_001",
    labor_force = "B23025_003",
    unemployed = "B23025_005",
    total_housing = "B25002_001",
    vacant_housing = "B25002_003"
  ),
  year = 2023, 
  state = "PA",
  county = "Philadelphia",
  geometry = TRUE,
  output = "wide"
) %>%
  st_transform(2272) %>%
  mutate(
    # 注意：所有变量名后面都要加 "E"
    Poverty_Rate = poverty_popE / total_popE,
    Med_Income = med_incomeE,
    
    # 修正部分：加上 E
    ba_rate = 100 * ba_degreeE / total_eduE,
    unemployment_rate = 100 * unemployedE / labor_forceE,
    vacancy_rate = 100 * vacant_housingE / total_housingE
  ) %>%
  dplyr::select(GEOID, Poverty_Rate, Med_Income, ba_rate, unemployment_rate, vacancy_rate)
```

### 1.2.3 Spatial amenities (OpenDataPhilly)

-   路灯密度
-   酒类销售点
-   警局

```{r}
# A. Alcohol Outlets (Crime Generators)
alcohol_sf <- read_csv("data/PHL_PLCB_geocoded.csv") %>%
  filter(!is.na(lon) & !is.na(lat)) %>%
  st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%
  st_transform(2272)

```

```{r}
# B. Street Lights (Guardianship)
lights_sf <- read_csv("data/Street_Poles.csv") %>%
  filter(!is.na(X) & !is.na(Y)) %>%
  # 修正：原始坐标是 EPSG:3857 (Web Mercator)
  st_as_sf(coords = c("X", "Y"), crs = 3857) %>%
  # 然后再转为费城投影
  st_transform(2272)
```

```{r}
# C. Police Stations (Guardianship) 
# 直接读取包含坐标的 GeoJSON 文件
police_sf <- st_read("data/Police_Stations.geojson", quiet = TRUE) %>%
  st_transform(2272) # 统一转为 PA State Plane

cat("Total Police Stations:", nrow(police_sf))
```

------------------------------------------------------------------------

# Phase 2: Feature Engineering

## **2.1 Buffer creation:**

### 2.1.1 公交站400m buffer

```{r}
# Create Buffer(400m)
bus_buffer <- st_buffer(bus_long, 1312)
```

### 2.1.2 Crime numbers

```{r}
### 2.1.2 Crime numbers (Corrected with Normalization)

# 1. 计算数据集中包含的“总天数” (Exposure)
# 这一步会自动计算你的 crime_sf 数据里涵盖了多少个工作日，多少个周末
day_counts <- crime_sf %>%
  st_drop_geometry() %>%
  group_by(is_crime_weekend) %>%
  summarise(n_days = n_distinct(crime_date)) # 统计有多少个不重复的日期

# 打印天数检查 (你会发现 Weekday 天数大约是 Weekend 的 2.5 倍)
print(day_counts)

# 2. 关联并计算日均犯罪 (Rate)
crime_agg <- st_join(bus_buffer, crime_sf, join = st_intersects) %>%
  filter(is_weekend == is_crime_weekend) %>%
  group_by(Stop_Code, is_weekend) %>%
  summarise(
    Crime_Total_Count = n() # 原始总数 (用于模型)
  ) %>%
  st_drop_geometry() %>%
  # 把天数信息 join 进来
  left_join(day_counts, by = c("is_weekend" = "is_crime_weekend")) %>%
  mutate(
    # 关键修正：计算日均犯罪量
    Crime_Daily_Rate = Crime_Total_Count / n_days,
    
    # 同时也保留 n_days，因为负二项回归模型需要它作为 offset
    Exposure_Days = n_days
  )

# 检查结果
head(crime_agg)

```
### 2.1.3 Spatial Feature

In this part 我们加入了犯罪的时间滞后变量，将上一季度的站点周边犯罪数量纳入考虑
```{r}
### 2.1.2 Lag Crime numbers (Corrected with Normalization)

# 1. 计算 Q1 数据集中包含的“总天数” (Exposure)
day_counts_q1 <- crime_q1 %>%
  st_drop_geometry() %>%
  group_by(is_crime_weekend) %>%
  summarise(n_days_lag = n_distinct(crime_date)) 

# 打印天数检查
print(day_counts_q1)

# 2. 关联并计算日均犯罪 (Rate) —— 完全复制你的 Q2 逻辑
crime_lag_q1 <- st_join(bus_buffer, crime_q1, join = st_intersects) %>%
  filter(is_weekend == is_crime_weekend) %>%
  group_by(Stop_Code, is_weekend) %>%
  summarise(
    Crime_Total_Count_lag = n()   # lag 原始总数
  ) %>%
  st_drop_geometry() %>%
  left_join(day_counts_q1, by = c("is_weekend" = "is_crime_weekend")) %>%
  mutate(
    Crime_Daily_Rate_lag = Crime_Total_Count_lag / n_days_lag,
    Exposure_Days_lag    = n_days_lag
  )

# 检查结果
head(crime_lag_q1)
```


### 2.1.3 POI Numbers（酒类销售点）

```{r}
# 2. Count Alcohol Outlets
alcohol_agg <- st_join(bus_buffer, alcohol_sf, join = st_intersects) %>%
  # 按 Stop 和 时间类型分组，这样每一行都会得到计数
  group_by(Stop_Code, is_weekend) %>%
  summarise(Alcohol_Count = n() - 1) %>% # Subtract 1 because st_join is left join (self-intersection NA check)
  st_drop_geometry()
```

### 2.1.4 Infrastructure Numbers（路灯密度）

```{r}
# 3. Count Street Lights
light_agg <- st_join(bus_buffer, lights_sf, join = st_intersects) %>%
  group_by(Stop_Code, is_weekend) %>%
  summarise(Light_Count = n() - 1) %>%
  st_drop_geometry()
```

### 2.1.5 Census Demographics

```{r}
# 4. Average Census Demographics
census_agg <- st_join(bus_buffer, philly_census, join = st_intersects) %>%
  group_by(Stop_Code, is_weekend) %>%
  summarise(
    Avg_Poverty = mean(Poverty_Rate, na.rm = TRUE),
    Avg_Income = mean(Med_Income, na.rm = TRUE),
    Avg_BA = mean(ba_rate, na.rm = TRUE),
    Avg_Unemployment = mean(unemployment_rate, na.rm = TRUE),
    Avg_Vacancy = mean(vacancy_rate, na.rm = TRUE)
  ) %>%
  st_drop_geometry()
```

## **2.2 k-Nearest Neighbor features:**

### 2.2.1 Police station (KNN-1)

```{r}
# Calculate distance to nearest police station
dist_matrix <- st_distance(bus_long, police_sf)

# 取每一行的最小值，并换算成英里
bus_long$Dist_Police <- apply(dist_matrix, 1, min) / 5280
```

## 2.3 Get Census Tract ID (GEOID) for Fixed Effects

```{r eval=FALSE}
# 我们需要知道每个站点物理上位于哪个普查区 (Point in Polygon)
# 注意：只取 weekday=0 的数据来做空间匹配，避免重复计算
stop_geoid_mapping <- bus_long %>%
  filter(is_weekend == 0) %>%
  st_join(philly_census %>% dplyr::select(GEOID)) %>% # 空间连接：点找面
  st_drop_geometry() %>%
  dplyr::select(Stop_Code, GEOID) %>%
  # 以防万一某个点在边界上匹配到了两个区，去重保留第一个
  distinct(Stop_Code, .keep_all = TRUE)

cat("GEOID mapping created for", nrow(stop_geoid_mapping), "stops.\n")
```

## 2.3* Get Police Service Area (PSA) ID for Fixed Effects
```{r}

# 1. 加载 PSA 边界数据
# 请确保文件名跟你下载的一致
psa_sf <- st_read("data/Boundaries_PSA.geojson", quiet = TRUE) %>%
  st_transform(2272) %>% # 统一坐标系
  # 费城 PSA 数据中，唯一的 ID 通常叫 "PSA_NUM" 或 "PSA_NAME"
  # 我们这里假设它叫 "PSA_NUM" (例如 241, 143 等)
  # 如果你的列名不一样，请运行 names(psa_sf) 检查并修改下面这一行
  dplyr::select(PSA_ID = PSA_NUM) 

# 2. 空间匹配：判断每个公交站属于哪个 PSA
# 注意：只取 weekday=0 的数据来做空间匹配，避免重复计算
stop_psa_mapping <- bus_long %>%
  filter(is_weekend == 0) %>% 
  st_join(psa_sf) %>% # 空间连接：点找面
  st_drop_geometry() %>%
  dplyr::select(Stop_Code, PSA_ID) %>%
  # 以防万一某个点在边界上匹配到了两个区，去重保留第一个
  distinct(Stop_Code, .keep_all = TRUE)

cat("PSA mapping created for", nrow(stop_psa_mapping), "stops.\n")
# 打印一下看看长什么样，确保 ID 是存在的
head(stop_psa_mapping)

```


## 2.4 Merge Features into Master Dataset

```{r eval=FALSE}
## 2.4 Merge Features into Master Dataset (Final Version)

final_data <- bus_long %>%
  # Join all aggregated tables
  left_join(crime_agg,     by = c("Stop_Code", "is_weekend")) %>%      # Q2 crime
  left_join(crime_lag_q1,  by = c("Stop_Code", "is_weekend")) %>%      # ⭐ 新增：Q1 lag crime
  left_join(alcohol_agg,   by = c("Stop_Code", "is_weekend")) %>%
  left_join(light_agg,     by = c("Stop_Code", "is_weekend")) %>%
  left_join(census_agg,    by = c("Stop_Code", "is_weekend")) %>% 
  
  # --- [新增] Join GEOID ---
  left_join(stop_geoid_mapping, by = "Stop_Code") %>%
  # -----------------------
  
  mutate(
    # 1. Handle NAs for Counts
    Crime_Total_Count = replace_na(Crime_Total_Count, 0),
    Crime_Daily_Rate  = replace_na(Crime_Daily_Rate, 0),

    # ⭐ 处理 lag crime 的 NA（如果某站点 Q1 没犯罪）
    Crime_Total_Count_lag = replace_na(Crime_Total_Count_lag, 0),
    Crime_Daily_Rate_lag  = replace_na(Crime_Daily_Rate_lag, 0),

    Alcohol_Count = replace_na(Alcohol_Count, 0),
    Light_Count   = replace_na(Light_Count, 0),
    
    # 2. Log transform Ridership
    Log_Ridership = log(Ridership + 1),
    
    # 3. Create Factor for Interaction Term
    is_weekend_factor = factor(is_weekend, levels = c(0, 1), 
                               labels = c("Weekday", "Weekend")),

  ) %>%
  
  # 5. Clean up
  filter(!is.na(GEOID)) %>%
  na.omit() 

cat("Final Panel Dataset Rows:", nrow(final_data))

```

## 2.4* Merge Features into Master Dataset

```{r}

final_data <- bus_long %>%
  # Join all aggregated tables
  left_join(crime_agg,     by = c("Stop_Code", "is_weekend")) %>%
  left_join(crime_lag_q1,  by = c("Stop_Code", "is_weekend")) %>%
  left_join(alcohol_agg,   by = c("Stop_Code", "is_weekend")) %>%
  left_join(light_agg,     by = c("Stop_Code", "is_weekend")) %>%
  left_join(census_agg,    by = c("Stop_Code", "is_weekend")) %>%
  
  # --- [修改点] Join PSA ID (原为 GEOID) ---
  left_join(stop_psa_mapping, by = "Stop_Code") %>%
  
   mutate(
    # 1. Handle NAs for Counts
    Crime_Total_Count = replace_na(Crime_Total_Count, 0),
    Crime_Daily_Rate  = replace_na(Crime_Daily_Rate, 0),

    # ⭐ 处理 lag crime 的 NA（如果某站点 Q1 没犯罪）
    Crime_Total_Count_lag = replace_na(Crime_Total_Count_lag, 0),
    Crime_Daily_Rate_lag  = replace_na(Crime_Daily_Rate_lag, 0),

    Alcohol_Count = replace_na(Alcohol_Count, 0),
    Light_Count   = replace_na(Light_Count, 0),
    
    # 2. Log transform Ridership
    Log_Ridership = log(Ridership + 1),
    
    # 3. Create Factor for Interaction Term
    is_weekend_factor = factor(is_weekend, levels = c(0, 1), 
                               labels = c("Weekday", "Weekend")),

  ) %>%
  
  # 5. Clean up
  # [修改点] 过滤掉没有匹配到 PSA 的站点 (原为 !is.na(GEOID))
  filter(!is.na(PSA_ID)) %>%
  na.omit() 

cat("Final Panel Dataset Rows:", nrow(final_data))

```

## 2.5 检查PSA数据密度
```{r}

# 1. 统计每个 PSA 有多少个样本 (Panel Data Rows)
psa_counts <- final_data %>%
  group_by(PSA_ID) %>%
  summarise(
    n_observations = n(),              # 总行数 (weekday + weekend)
    n_stops = n_distinct(Stop_Code)    # 物理站点数
  ) %>%
  arrange(n_observations)

# 2. 打印最少样本的 10 个 PSA
print(head(psa_counts, 10))

# 3. 这里的判断标准：
# 如果 n_observations 最小的值都 > 5，恭喜你，CV 问题基本解决！
# 如果还是有 1 或 2，那 CV 依然可能会报错，但概率比 Tract 小多了。

```


------------------------------------------------------------------------

# Phase 3: Exploratory Data Analysis

## 3.1 Distribution of Crime and Bus Stop Ridership (histogram)

Does ridership follow a normal distribution? No. That's why we need Negative Binomial.

```{r}
library(gridExtra)
p1 <- ggplot(final_data, aes(x = Ridership)) +
  geom_histogram(fill = "#3182bd", bins = 50) +
  labs(title = "Distribution of Ridership", x = "Daily Boardings") + plotTheme

p2 <- ggplot(final_data, aes(x = Crime_Total_Count)) +
  geom_histogram(fill = "#de2d26", bins = 50) +
  labs(title = "Distribution of Crime", x = "Crime Count (400m)") + plotTheme

grid.arrange(p1, p2, ncol = 2)
```

**Key Findings:**

```{r}
# 检查均值和方差
mean_crime <- mean(final_data$Crime_Total_Count, na.rm = TRUE)
var_crime <- var(final_data$Crime_Total_Count, na.rm = TRUE)

cat("Mean:", mean_crime, "\n")
cat("Variance:", var_crime, "\n")
cat("Ratio (Var/Mean):", var_crime / mean_crime, "\n")
```

## 3.2 Spatial distribution of crime and Bus Stop Ridership(map)

```{r}
# 需要先加载 patchwork 包，如果前面没加载，这里补充一下
# library(patchwork) 

# --- 1. 准备绘图数据 (Extract Coordinates) ---

# A. 处理公交数据：只取工作日数据以避免重复，并提取坐标
bus_plot_data <- bus_long %>%
  filter(is_weekend == 0) %>%   # 仅使用工作日数据代表典型客流
  mutate(
    X = st_coordinates(geometry)[,1],
    Y = st_coordinates(geometry)[,2]
  ) %>%
  st_drop_geometry() # 移除几何列，方便 stat_density_2d 使用

# B. 处理犯罪数据：提取坐标
crime_plot_data <- crime_sf %>%
  mutate(
    X = st_coordinates(geometry)[,1],
    Y = st_coordinates(geometry)[,2]
  ) %>%
  st_drop_geometry()

# --- 2. 绘制图形 (Create Maps) ---

# 左图：客流热力图 (Ridership Density)
# 注意：aes(weight = Ridership) 是关键，让热力图基于客流量而不是站点数量
p_ridership <- ggplot() +
  # 底图：费城轮廓
  geom_sf(data = philly_boundary, fill = "#f5f5f5", color = "grey80") +
  # 热力层
  stat_density_2d(
    data = bus_plot_data, 
    aes(x = X, y = Y, fill = ..level.., weight = Ridership), 
    geom = "polygon", 
    alpha = 0.75
  ) +
  # 配色：蓝色系 (代表正常活动)
  scale_fill_distiller(palette = "Blues", direction = 1, guide = "none") +
  labs(
    title = "Weekday Ridership Hotspots", 
    subtitle = "High Transit Activity Zones"
  ) +
  mapTheme

# 右图：犯罪热力图 (Crime Density)
p_crime_map <- ggplot() +
  # 底图
  geom_sf(data = philly_boundary, fill = "#f5f5f5", color = "grey80") +
  
  # 热力层 (修改部分)
  stat_density_2d(
    data = crime_plot_data, 
    aes(x = X, y = Y, fill = ..level..), 
    geom = "polygon", 
    alpha = 0.4,       # 1. 调低透明度，减少"硬边"的感觉
    bins = 30,         # 2. 增加层数，让过渡更平滑 (原来默认很少)
    adjust = 0.5       # 3. 调低带宽 (0.5), 让热力点收缩，更聚焦局部热点
  ) +
  
  # 配色
  scale_fill_distiller(palette = "Reds", direction = 1, guide = "none") +
  labs(
    title = "Crime Hotspots", 
    subtitle = "High Incident Zones"
  ) +
  mapTheme

# --- 3. 组合展示 (Combine Side-by-Side) ---

# 使用 patchwork 进行拼接
combined_map <- p_ridership + p_crime_map +
  plot_annotation(
    title = "Spatial Mismatch Analysis: Eyes on the Street vs. Targets?",
    subtitle = "Left: Where people are (Ridership) | Right: Where crimes happen",
    theme = theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      plot.subtitle = element_text(size = 12, color = "grey40", hjust = 0.5)
    )
  )

# 输出图形
combined_map

```

**Key Findings:**

## 3.3 Crime vs. Bus Stop Ridership (scatter plots)

```{r eda_interaction}
# Crime vs. Ridership (Interaction Plot)
# 这是一个非常关键的图，用于验证你的核心假设：周末的客流影响是否不同？

ggplot(final_data, aes(x = Log_Ridership, y = Crime_Daily_Rate, color = is_weekend_factor)) +
  geom_point(alpha = 0.1, size = 1) +
  geom_smooth(method = "glm", method.args = list(family = "quasipoisson"), se = TRUE) +
  scale_color_manual(values = c("Weekday" = "#3182bd", "Weekend" = "#de2d26")) +
  labs(
    title = "Does Ridership impact Crime differently on Weekends?",
    subtitle = "Interaction Effect (Normalized by Number of Days)",
    x = "Log(Daily Ridership)",
    y = "Average Daily Crime Count (per 400m)", # 修改 Y 轴标签
    color = "Time Period"
  ) +
  plotTheme
```

坏了。。。。。。

**Key Findings:**

-   

## 3.4 Crime vs. Spatial & Social features (scatter plots)

```{r 3.4_scatter_plots}
library(patchwork)
library(scales)

# 为了可视化清晰，我们先创建一个带有 log(crime) 的临时绘图数据
plot_data <- final_data %>%
  mutate(
    log_crime = log(Crime_Daily_Rate + 0.01), # +1 避免 log(0)
    # 给 Income 取个 log 看看是否更线性，很多经济学变量 log 后效果更好
    log_income = log(Avg_Income + 1)
  )

# --- 1. POI & Infrastructure (Built Environment) ---

# A. Alcohol Outlets (线性还是指数增加？)
p1 <- ggplot(plot_data, aes(x = Alcohol_Count, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#6A1B9A") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Log(Crime) vs. Alcohol Outlets",
       subtitle = "Check for diminishing returns",
       x = "Count of Alcohol Outlets", y = "Log(Crime Count)") +
  plotTheme

# B. Street Lights (路灯越多越安全？还是路灯只出现在繁华区？)
p2 <- ggplot(plot_data, aes(x = Light_Count, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#6A1B9A") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Log(Crime) vs. Street Lights",
       subtitle = "Is the relationship linear?",
       x = "Count of Street Lights", y = "") +
  plotTheme

# C. Distance to Police (离警局越远越危险？)
p3 <- ggplot(plot_data, aes(x = Dist_Police, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#6A1B9A") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "Log(Crime) vs. Dist to Police",
       subtitle = "Check for U-shape or threshold",
       x = "Distance to Station (Miles)", y = "") +
  plotTheme

# --- 2. Demographics (Census) ---

# D. Poverty Rate (贫困率与犯罪)
p4 <- ggplot(plot_data, aes(x = Avg_Poverty, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#3182bd") +
  geom_smooth(method = "loess", color = "orange", se = FALSE) +
  scale_x_continuous(labels = scales::percent) +
  labs(title = "Log(Crime) vs. Poverty Rate",
       x = "Poverty Rate", y = "Log(Crime Count)") +
  plotTheme

# E. Median Income (收入与犯罪 - 这是一个典型的可能需要 log 的变量)
p5 <- ggplot(plot_data, aes(x = Avg_Income, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#3182bd") +
  geom_smooth(method = "loess", color = "orange", se = FALSE) +
  scale_x_continuous(labels = scales::dollar) +
  labs(title = "Log(Crime) vs. Median Income",
       subtitle = "Does wealth shield against crime?",
       x = "Median Household Income", y = "") +
  plotTheme

# F. Vacancy Rate (空置率/破窗理论)
p6 <- ggplot(plot_data, aes(x = Avg_Vacancy, y = log_crime)) +
  geom_point(alpha = 0.1, color = "#3182bd") +
  geom_smooth(method = "loess", color = "orange", se = FALSE) +
  scale_x_continuous(labels = scales::percent) +
  labs(title = "Log(Crime) vs. Vacancy Rate",
       x = "Housing Vacancy Rate", y = "") +
  plotTheme

# --- Combine Plots ---
(p1 | p2 | p3) / (p4 | p5 | p6) +
  plot_annotation(
    title = "Exploratory Analysis: Variable Functional Forms",
    subtitle = "Red Line = Loess Smoother (Non-linear trend)",
    theme = theme(plot.title = element_text(size = 16, face = "bold"))
  )
```

**Key Findings:**

-   

## 3.5 Correlation Matrix of Features for all Features

```{r}

library(ggcorrplot)

# 1. 选择数值型变量
numeric_vars <- final_data %>%
  st_drop_geometry() %>%
  dplyr::select(
    Crime_Daily_Rate, 
    Ridership, 
    Alcohol_Count, 
    Light_Count, 
    Dist_Police, 
    Avg_Poverty, 
    Avg_Income, 
    Avg_Unemployment,
    Avg_Vacancy
  )

# 2. 计算相关系数矩阵
corr_matrix <- cor(numeric_vars, use = "complete.obs")

# 3. 绘图
ggcorrplot(
  corr_matrix, 
  method = "square", 
  type = "lower", 
  lab = TRUE, 
  lab_size = 3, 
  colors = c("#6D9EC1", "white", "#E46726"), 
  title = "Correlation Matrix of Features",
  ggtheme = theme_minimal()
)

```

如果矩阵显示它们的相关系数超过 0.7，不要把它们都放进模型！所以可以说这里没有超过0.7的，可以都放？ 另外如果后面监测到多重共线性，可以用这个矩阵决定怎么处理数据

## 3.6 Crime distribution in weekdays and weekends

```{r}

### 3.5 Weekend Effect Boxplot

ggplot(final_data, aes(x = is_weekend_factor, y = Crime_Daily_Rate, fill = is_weekend_factor)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) + 
  
  # 加上均值点
  stat_summary(fun = mean, geom = "point", shape = 23, size = 3, fill = "white") +
  
  # 限制 Y 轴范围 (注意：现在的单位是“每天”，数值会很小，比如 0.05)
  # 使用 quantile 自动截断极端值
  coord_cartesian(ylim = c(0, quantile(final_data$Crime_Daily_Rate, 0.95))) +
  
  scale_fill_manual(values = c("Weekday" = "#3182bd", "Weekend" = "#de2d26")) +
  
  labs(
    title = "Daily Crime Risk: Weekday vs. Weekend",
    subtitle = "Comparison of Average Daily Crime Counts per Stop",
    x = "Time Period",
    y = "Average Daily Crime Count (per 400m)", # 修改标签
    caption = "Note: Values represent daily averages to account for fewer weekend days per year."
  ) +
  plotTheme
```

坏了。。。。

**Key Findings:**

-   **Spatial Variation in Property Values:** The average sale price per square foot shows significant geographic clustering across census tracts, with distinct high-value areas concentrated in specific neighborhoods. This indicates strong spatial autocorrelation in housing prices, where adjacent tracts tend to have similar price levels.

-   **Correlation Between Property Condition and Location:** Better average interior conditions are systematically concentrated in particular geographic areas, suggesting that housing maintenance and quality are not randomly distributed but follow spatial patterns that may correlate with neighborhood characteristics and property values.

-   **Heterogeneous Distribution of Housing Size:** The average livable area varies substantially across census tracts, with larger properties clustered in specific regions. This spatial patterning of housing size complements the price distribution, indicating that both property characteristics and location factors contribute to the overall housing market structure in Philadelphia.

------------------------------------------------------------------------

# Phase 4: Model Building(负二项式)

## **4.1 Ridership only:**

```{r Model_1+Summary}

model_1 <- glm.nb(Crime_Total_Count ~ 
                    Log_Ridership + 
                    offset(log(Exposure_Days)), 
                  data = final_data)
summary(model_1)

```

**Coefficient Interpretation:**

## \*\*4.2 The Interaction (核心假设)

```{r Model_2+Summary}
model_2 <- glm.nb(Crime_Total_Count ~ 
                    Log_Ridership * is_weekend_factor + 
                    offset(log(Exposure_Days)), 
                  data = final_data)
summary(model_2)
```

**Coefficient Interpretation:**

-   Coefficient Evolution (vs. Model 1):

-   New Variable Interpretation:

## **4.3 Built Environment and Demographics features:**(加入环境控制)

```{r Model_3+Summary}
model_3 <- glm.nb(Crime_Total_Count ~ 
                    Log_Ridership * is_weekend_factor + 
                    log(Alcohol_Count + 1) +        # 诱发因子
                    poly(Light_Count, 2) +          # 预防因子
                    Avg_Poverty +          # 社会结构
                    Avg_Vacancy + I(Avg_Vacancy^2) +          # 破窗效应
                    Avg_Income + I(Avg_Income^2)  +
                    Avg_Unemployment +
                    Dist_Police +
                    offset(log(Exposure_Days)), 
                  data = final_data)
summary(model_3)
```

**Coefficient Interpretation:**

-   Coefficient Evolution (vs. Model 2):

-   New Variable (Spatial) Interpretation:

## **4.4 空间固定效应:**

```{r Model_4+Summary}
# 加入 'GEOID' (Census Tract) 作为固定效应
# 这相当于为费城每一个普查区都加了一个“基准拦截”，控制了所有观测不到的社区特征
model_4 <- glm.nb(Crime_Total_Count ~ 
                    Log_Ridership * is_weekend_factor + 
                    log(Alcohol_Count + 1) +        # 诱发因子
                    poly(Light_Count, 2) +          # 预防因子
                    Avg_Poverty +          # 社会结构
                    Avg_Vacancy + I(Avg_Vacancy^2) +          # 破窗效应
                    Avg_Income + I(Avg_Income^2)  +
                    Avg_Unemployment +
                    Dist_Police +          # 政策变量：警力可达性
                    factor(PSA_ID) +      # <--- 如果模型跑不动(不收敛)，注释掉这一行
                    offset(log(Exposure_Days)), 
                  data = final_data,
                  control = glm.control(maxit = 100)) # 增加迭代次数以防不收敛
summary(model_4)
```

## Model 5 crime temporal lag
```{r Model_5+Summary}
# 加入警局距离，并加入 'GEOID' (Census Tract) 作为固定效应
# 这相当于为费城每一个普查区都加了一个“基准拦截”，控制了所有观测不到的社区特征
model_5 <- glm.nb(Crime_Total_Count ~ 
                    Log_Ridership * is_weekend_factor + 
                    log(Alcohol_Count + 1) +        # 诱发因子
                    poly(Light_Count, 2) +          # 预防因子
                    Avg_Poverty +          # 社会结构
                    Avg_Vacancy + I(Avg_Vacancy^2) +          # 破窗效应
                    Avg_Income + I(Avg_Income^2)  +
                    Avg_Unemployment +
                    Dist_Police +          # 政策变量：警力可达性
                    factor(PSA_ID) + 
                    log(Crime_Daily_Rate_lag + 0.001) +
                    offset(log(Exposure_Days)), 
                  data = final_data,
                  control = glm.control(maxit = 100)) # 增加迭代次数以防不收敛
summary(model_5)
```

在加入geoid前，aic等于77655，morans i约等于0.5；加入geoid之后，aic等于74009，morans i 约等于0.29，并且只有其他一个其他变量Avg_Poverty从显著变成了不显著。
GEOID 的作用： 当你放入factor(GEOID)，模型就已经知道了“这个区是贫困区”。
如果变量变得不显著： 说明这个变量对犯罪的影响，已经被 GEOID 这个“大标签”完全解释掉了。
如果其他变量依然显著： 这才是最牛的地方！说明像 客流 (Ridership)、路灯、酒铺这些变量，即使在控制了社区背景后，依然对犯罪有独立的、显著的影响。

而且我觉得用census tract可能是最能降morans i的了，其他空间区分方式（比如警区）更是很大一块

总体看来很需要加入geoid作为固定项。AIC 下降 3000 是硬道理，说明模型拟合度极佳。变量显著性稳定，说明模型结构健康。Moran's I 的0.29虽然不完美，但在实际应用类项目中是可以容忍的（你可以说这是 limitations）。

如何解释那个不显著的变量？
在结果表格里，如实汇报它变得不显著。
解释为：“该变量的影响已被社区固定效应所包含（Captured by neighborhood fixed effects）。”


```{r VIF Validation}
vif(model_5)
```

其实在加入geoid做固定效应之后，vif变得很屎（看第二列的平方值），但是log ridership还行（1.53^2=2.3）。
多重共线性（High VIF）的主要后果是让相关变量的系数变得不稳定（方差变大，导致显著性消失）。在你的模型里，Light_Count 和 GEOID 打架，导致路灯的系数废了。但是，Log_Ridership 没有参与这场架（它的 VIF 很低）。

只要核心变量不共线，且模型设定正确（AIC 降低证明了这一点），你对 Log_Ridership 的系数估计依然是无偏且有效的。

以前的解释（不加 GEOID）：
“客流高的站点犯罪率高，但我们不确定是因为客流本身，还是因为这些站点恰好都在贫民窟或治安差的区域。”

现在的解释（加了 GEOID）：
“我们在同一个普查区（Census Tract）内部进行了比较。即便是在同一个社区、面临同样的贫困水平和同样的警力管辖下，那个客流更大的站点，依然比它隔壁客流小的站点有更高的犯罪风险。”

对于不显著的变量，就简单报告：加入了建成环境（路灯、酒铺）作为控制变量，并加入了社区固定效应以消除偏差。虽然这些变量吸收了空间变异，但我们的核心关注点依然是客流带来的稳健影响……


## Model6 比model5减去income
```{r model6+summary}

model_6 <- glm.nb(Crime_Total_Count ~ 
                             Log_Ridership * is_weekend_factor + 
                             log(Alcohol_Count + 1) +
                             Light_Count +
                             
                             # --- 核心修改 ---
                             # 保留贫困率
                             Avg_Poverty +   
                             
                             # 删除了 Avg_Income 和 I(Avg_Income^2）
                             # 检查空置率：如果平方项不显著，也可以删掉平方项只留 Avg_Vacancy
                             Avg_Vacancy + I(Avg_Vacancy^2) + 
                             
                             Avg_Unemployment +
                             Dist_Police +
                             factor(PSA_ID) + 
                             log(Crime_Daily_Rate_lag + 0.001) +
                             offset(log(Exposure_Days)), 
                           data = final_data,
                           control = glm.control(maxit = 100))

summary(model_6)
# 重新看 VIF
vif(model_6)

```


## **4.5 输出对比表格**

```{r}
# 1. 加载必要的包
library(modelsummary)
library(ggplot2)
library(dplyr)

# 2. 定义变量名的“美化映射” (跟 Stargazer 的 label 对应)
coef_map_list <- c(
  "Log_Ridership" = "Ridership (Log)",
  "is_weekend_factorWeekend" = "Weekend Effect",
  "Log_Ridership:is_weekend_factorWeekend" = "Interaction: Ridership × Weekend",
  
  # --- 环境变量 ---
  "log(Alcohol_Count + 1)" = "Alcohol Outlets", 
  
  # --- 路灯 (兼容 model_6 的线性项和旧模型的二次项) ---
  "Light_Count" = "Street Lights (Linear)",       # Model 6 用
  "poly(Light_Count, 2)1" = "Street Lights (Poly 1)", # Model 5 用
  "poly(Light_Count, 2)2" = "Street Lights (Poly 2)", # Model 5 用
  
  # --- 社会经济 (model_6 去掉了 Income) ---
  "Avg_Poverty" = "Poverty Rate",
  "Avg_Vacancy" = "Vacancy Rate",
  "I(Avg_Vacancy^2)" = "Vacancy Rate (Squared)",
  
  # --- 政策变量 ---
  "Dist_Police" = "Dist to Police Station",
  
  # --- 滞后项 ---
  "log(Crime_Daily_Rate_lag + 0.001)" = "Temporal Lag (Prev. Quarter)"
)

# 3. 绘制系数图
p_models <- modelplot(
  list(
    "1. Naive" = model_1, 
    # "3. Context" = model_3, # 可以根据需要保留或注释掉旧模型
    "5. Robust (PSA)" = model_5,   # 含 Income 和 Poly Lights
    "6. Final (Refined)" = model_6 # ⭐ 你的最终模型 (无 Income, 线性路灯)
  ),
  coef_map = coef_map_list, 
  conf_level = 0.95
) +
  # --- 美化代码 ---
  geom_vline(xintercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "What Drives Crime Risk?",
    subtitle = "Model Evolution: Comparing Robust (M5) vs. Final Refined (M6)",
    x = "Effect Size (Coefficient Estimate)",
    y = "",
    caption = "Note: Model 6 removes collinear Income variables and simplifies Street Lights to a linear term."
  ) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 14),
    axis.text.y = element_text(size = 11, face = "bold")
  )

# 展示图形
p_models
```

**Coefficient Interpretation:**
This plot visualizes our journey from a naive baseline to a robust final model. When reading this chart:

Distance from Red Line: The further the dot is to the right, the stronger the positive impact on crime.

Length of the Line: This represents the Confidence Interval. A longer line means higher uncertainty, often caused by multicollinearity.

 Model 1 (Red): The Naive Baseline This model only considers Ridership. While it shows a positive correlation, it ignores the environmental context, providing an incomplete picture.

Model 5 (Blue): The "Over-Fitted" Exploration We introduced extensive environmental variables here. However, notice the extremely long blue line for "Street Lights (Poly 1)." This massive error bar indicates severe multicollinearity. The model became "confused" by too many overlapping variables, making the results unstable.

Model 6 (Green): The Final Refined Model To fix the uncertainty, we refined the variables by simplifying Street Lights to linear.
Result: The error bars (lines) collapsed into tight, precise green dots. This confirms we successfully removed the collinearity noise.

Key Finding: Poverty Rate and Temporal lag crime count are the strongest drivers of crime, significantly outweighing the impact of Ridership.


# Phase 5: Model Validation

## **5.1 Compare all 5 models:**

### 5.1.1 Create predicted vs. actual plot

```{r}
# 1. 重新定义五个模型的公式 (确保与 Phase 4 一致)

f1 <- Crime_Total_Count ~ Log_Ridership + offset(log(Exposure_Days))
f2 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + offset(log(Exposure_Days))
f3 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + log(Alcohol_Count + 1) + poly(Light_Count, 2) + Avg_Poverty + Avg_Vacancy + I(Avg_Vacancy^2) + Avg_Income + I(Avg_Income^2) + Avg_Unemployment + Dist_Police + offset(log(Exposure_Days))

f4 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + log(Alcohol_Count + 1) + poly(Light_Count, 2) + Avg_Poverty + Avg_Vacancy + I(Avg_Vacancy^2) + Avg_Income + I(Avg_Income^2) + Avg_Unemployment + Dist_Police + factor(PSA_ID) + offset(log(Exposure_Days)) 

f5 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + 
      log(Alcohol_Count + 1) + poly(Light_Count, 2) + 
      Avg_Poverty + Avg_Vacancy + I(Avg_Vacancy^2) + 
      Avg_Income + I(Avg_Income^2) + Avg_Unemployment + 
      Dist_Police + factor(PSA_ID) + 
      log(Crime_Daily_Rate_lag + 0.001) + 
      offset(log(Exposure_Days))

f6 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + 
      log(Alcohol_Count + 1) + 
      Light_Count +              # 线性项
      Avg_Poverty +              # 仅保留贫困
      Avg_Vacancy + I(Avg_Vacancy^2) + 
      Avg_Unemployment + 
      Dist_Police + 
      factor(PSA_ID) +           # PSA 固定效应
      log(Crime_Daily_Rate_lag + 0.001) + 
      offset(log(Exposure_Days))

# 2. 训练五个模型
m1 <- glm.nb(f1, data = final_data)
m2 <- glm.nb(f2, data = final_data)
m3 <- glm.nb(f3, data = final_data)
m4 <- glm.nb(f4, data = final_data)
m5 <- glm.nb(f5, data = final_data)
m6 <- glm.nb(f6, data = final_data)

# 3. 生成预测数据 (Gather Predictions)
# type = "response" 会直接返回预测的犯罪数量 (Count)，不需要再 exp
plot_data <- final_data %>%
  dplyr::select(Crime_Total_Count) %>%
  mutate(
    Model_1_Pred = predict(m1, type = "response"),
    Model_2_Pred = predict(m2, type = "response"),
    Model_3_Pred = predict(m3, type = "response"),
    Model_4_Pred = predict(m4, type = "response"),
    Model_5_Pred = predict(m5, type = "response"),
    Model_6_Pred = predict(m6, type = "response"),
  ) %>%
  pivot_longer(
    cols = starts_with("Model"), 
    names_to = "Model", 
    values_to = "Predicted_Count"
  )

# 4. 绘图：Facet Wrap 对比6个模型
# 因为犯罪是计数数据，点会重叠，我们使用 alpha 和 45度线
ggplot(plot_data, aes(x = Predicted_Count, y = Crime_Total_Count)) +
  geom_point(alpha = 0.2, color = "#3182bd", size = 0.8) +
  
  # 添加完美预测线 (y=x)
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  
  facet_wrap(~Model, ncol = 3) +
  
  labs(
    title = "Predicted vs. Actual Crime Counts",
    subtitle = "Comparing Model Fit: Final Model (M6) shows robust predictions",
    x = "Predicted Crime Count",
    y = "Actual Crime Count"
  ) +
  theme_bw() +
  # 限制坐标轴范围以便看清核心区域 (去掉极值干扰)
  coord_cartesian(xlim = c(0, 50), ylim = c(0, 50))
```
**Interpretation:** This plot visualizes our journey from a naive baseline to a robust final model.

Overall, as more contextual and more spatial information are added, the predicted values align more closely with the 45° reference line, indicating improved model fit.

Models 1–2 rely mainly on ridership and weekend interaction, resulting in wide dispersion and systematic under-prediction at higher crime levels.

Models 3–4 incorporate environmental (alcohol outlets, lighting) and demographic features, producing a noticeably tighter prediction band and capturing more variation across stops.

Model 5 adds temporal information (lagged crime rate), which further stabilizes predictions and reduces bias.

Compared with Model 5, Model 6 trims several weak or redundant predictors while keeping the key effects. This parsimonious specification makes the model more stable and interpretable and helps reduce potential multicollinearity among highly correlated socioeconomic variables.


### 5.1.2 Report and Compare RMSE, MAE, R²

```{r}
library(caret)

# 1. 设置 5-Fold Cross Validation
set.seed(999)
folds <- createFolds(final_data$Crime_Total_Count, k = 5, list = TRUE)

# 定义一个辅助函数：跑 CV 并计算指标
run_cv <- function(formula, data, folds) {
  mae_list <- c()
  rmse_list <- c()
  
  for (i in 1:length(folds)) {
    # Split Data
    test_idx <- folds[[i]]
    train_set <- data[-test_idx, ]
    test_set  <- data[test_idx, ]
    
    # Train Model (tryCatch 防止不收敛报错)
    model <- tryCatch({
      glm.nb(formula, data = train_set)
    }, error = function(e) return(NULL))
    
    if(!is.null(model)) {
      # Predict (type = "response" 返回预测的数量)
      preds <- predict(model, newdata = test_set, type = "response")
      
      # Calculate Errors
      actuals <- test_set$Crime_Total_Count
      mae_list <- c(mae_list, mean(abs(actuals - preds)))
      rmse_list <- c(rmse_list, sqrt(mean((actuals - preds)^2)))
    }
  }
  return(c(mean(mae_list), mean(rmse_list)))
}

# 2. 对五个模型分别跑 CV
# 这里的公式需要跟上面定义的一致
results_m1 <- run_cv(f1, final_data, folds)
results_m2 <- run_cv(f2, final_data, folds)
results_m3 <- run_cv(f3, final_data, folds)
results_m4 <- run_cv(f4, final_data, folds)
results_m5 <- run_cv(f5, final_data, folds)
results_m6 <- run_cv(f6, final_data, folds)

# 3. 汇总表格
validation_summary <- data.frame(
  Model = c("1. Ridership Only", "2. +Interaction", "3. +Env & Demo", "4. +Policy & Dist", "5. +Temporal lag", "6. Refined"),
  MAE  = c(results_m1[1], results_m2[1], results_m3[1], results_m4[1], results_m5[1], results_m6[1]),
  RMSE = c(results_m1[2], results_m2[2], results_m3[2], results_m4[2], results_m5[2], results_m6[2])
) %>%
  mutate(
    # 计算相对于 Model 1 的提升百分比
    Improvement_MAE = (MAE[1] - MAE) / MAE[1]
  )

# 4. 展示漂亮的表格
kable(validation_summary, digits = 3, caption = "5-Fold Cross-Validation Metrics") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE) %>%
  column_spec(4, color = "green", bold = TRUE)
```

**Interpretation**:
These findings are validated through 5-fold cross-validation, which evaluates each model on unseen data rather than relying solely on in-sample fit. The consistent reduction in MAE and RMSE across folds confirms that the improvements are not artifacts of overfitting but reflect real gains in out-of-sample predictive performance.

1. Baseline: Mobility Activity (Model 1)

The model using only ridership intensity establishes a basic relationship between human activity levels and crime, but its predictive capacity is limited. Ridership alone captures general exposure but cannot explain spatial heterogeneity across stops.

2. Temporal Structure of Activity (Model 2)

Adding the weekday–weekend interaction improves the model slightly.

3. Built Environment & Demographic features (Model 3)

Alcohol outlet density, Lighting conditions poverty, income, vacancy, and unemployment: these variables substantially reduce prediction error, indicating that crime near transit stops is strongly shaped by both environmental risk factors and neighborhood disadvantage.

4. Spatial Fixed Effect (Model 4)

Introducing police station distance and especially PSA fixed effects further improves performance.
This suggests that:Crime patterns are partly structured by local policing jurisdictions and there are unobserved spatial factors (culture, enforcement style, land use patterns) that are constant within each PSA.PSA fixed effects absorb these stable spatial characteristics, making the model more robust.

5. Temporal Dependence in Crime (Model 5)

Including lagged crime rate (previous quarter) yields the largest single improvement.
Lagged crime captures temporal persistence—areas that experienced more crime in the past tend to remain active in the present.This is a very strong and stable predictor, dramatically improving accuracy.

6. Model Refinement & Parsimony (Model 6)

Model 6 streamlines the specification by removing weaker or collinear variables.Despite using fewer predictors, its accuracy remains nearly identical to Model 5.
This indicates that a refined, parsimonious model with reduced multicollinearity can maintain strong predictive performance while improving interpretability and stability.

# Phase 6: Model Diagnostics

### Check assumptions for best model:

## **6.1 Spatial Autocorrelation of Residuals (Moran's I)**

```{r}

# 1. 明确指定 Model 5 为最佳模型
best_model <- model_6

# 2. 提取残差
# Pearson 残差用于统计检验（因为它是标准化的）
# Deviance 残差用于绘图（因为视觉上更能反映拟合优度）
final_data$resid_pearson  <- residuals(best_model, type = "pearson")
final_data$resid_deviance <- residuals(best_model, type = "deviance")

# 3. 构建空间权重矩阵 (Spatial Weights Matrix)
# 由于公交站点是离散点，使用 k-Nearest Neighbors (KNN) 比距离阈值更稳健
# 这里选取 k=8，意味着每个站点对比它最近的 8 个邻居
coords <- st_coordinates(final_data)
neighbor_nb <- knn2nb(knearneigh(coords, k = 8))
spatial_weights <- nb2listw(neighbor_nb, style = "W")

# 4. 运行 Moran's I 检验
moran_result <- moran.test(final_data$resid_pearson, spatial_weights)

# 打印结果
print(moran_result)

# 解释逻辑：
# 如果 p-value > 0.05: 恭喜！残差是随机分布的，模型非常完美，没有遗漏空间变量。
# 如果 p-value < 0.05 但 Moran's I 很小 (如 < 0.1): 模型还可以，只有轻微的空间依赖。

```
**Interpretation**:
Although we added spatial fixed effect(which is also a way to reduce spatial autocorrelation), the Moran’s I test on the model residuals shows a strong and highly significant level of spatial autocorrelation (Moran’s I = 0.452, p < 2.2 × 10⁻¹⁶).
This indicates that the residuals are not randomly distributed across space and that the model likely fails to capture important spatial dependence structures in the data.

In practical terms, nearby transit stops tend to have systematically similar over- or under-predictions, suggesting the presence of spatially clustered unobserved factors such as local policing practices, neighborhood context, or land-use patterns that are not fully absorbed even after including PSA fixed effects and temporal lag terms.

Given the strength of the residual spatial autocorrelation, the results imply that the model could benefit from incorporating explicit spatial components, such as:

spatially lagged variables (SAR, CAR, or SLX terms)

spatial random effects

geographically weighted or spatially varying coefficient structures

Overall, the Moran’s I result shows that while the model fits well in non-spatial dimensions, unmodeled spatial processes remain significant and should be considered in future extensions.


## **6.2 Spatial Distribution of Residuals**
```{r}

# Spatial Distribution of Residuals
# 1. 将残差添加回空间数据
# type = "deviance" residuals are often better for mapping goodness-of-fit
final_data$spatial_resid <- residuals(best_model, type = "deviance")

# 2. 绘制地图
ggplot() +
  # 底图
  geom_sf(data = philly_boundary, fill = "grey95", color = NA) +
  
  # 站点残差图
  geom_sf(data = final_data, 
          aes(color = spatial_resid), 
          size = 0.8, alpha = 0.7) +
  
  # 颜色比例尺
  scale_color_gradient2(
    low = "blue", mid = "grey90", high = "red",
    midpoint = 0,
    name = "Deviance\nResidual",
    # 限制范围，防止极个别离群值破坏颜色分布
    limits = c(-3, 3), 
    oob = scales::squish 
  ) +
  
  labs(
    title = "Map of Model Residuals",
    subtitle = "Red = Unexpectedly High Crime (Under-predicted)\nBlue = Unexpectedly Low Crime (Over-predicted)"
  ) +
  mapTheme

```

**Interpretation**:

Red spots (positive deviance residuals):These locations experienced more crime than the model expected.

Blue spots (negative deviance residuals):These locations show less crime than predicted.

Although the residuals exhibit noticeable spatial clustering—confirmed by the significant Moran’s I test—the pattern does not form a clear, interpretable spatial structure.
The clusters of over- and under-prediction appear scattered across different parts of the city without aligning with obvious geographic boundaries, transit corridors, or neighborhood divisions.

This suggests that while some localized spatial dependence remains in the model, these patterns do not translate into a single coherent spatial process.
In other words, the remaining spatial signal is weakly structured and heterogeneous, making it difficult to summarize into a simple spatial rule or trend.

## **6.3 Residual plot:（找实际犯罪\>预测）**

```{r}
best_model <- model_6

# 1. 提取拟合值和残差
# 注意：对于 glm.nb，必须指定 type = "pearson" 来标准化残差
model_data <- data.frame(
  Fitted = fitted(best_model),
  Residuals = residuals(best_model, type = "pearson")
)

# 2. 绘制残差 vs. 拟合值图
p_resid_fitted <- ggplot(model_data, aes(x = log(Fitted), y = Residuals)) +
  geom_point(alpha = 0.3, color = "#6A1B9A", size = 1.5) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
  geom_smooth(method = "loess", color = "black", se = FALSE, linewidth = 0.8) +
  labs(
    title = "Residuals vs Fitted Values",
    subtitle = "Checking for systematic bias in the Count Model",
    x = "Log(Fitted Values) - Predicted Crime Count",
    y = "Pearson Residuals"
  ) +
  plotTheme

p_resid_fitted
```
**Interpretation:**

This plot shows the residuals against the predicted crime counts from the final negative binomial model. Overall, the residuals are centered around zero without clear curvature or funnel shapes, suggesting that the model does not suffer from major misspecification or heteroskedasticity.

The “striped” patterns come from the integer nature of crime counts—an effect commonly called the discreteness pattern in count models. This is not a model problem; it simply reflects that many stations share the same small crime counts , leading to stacked fitted values. The smooth LOESS line stays close to zero across the range, indicating no strong systematic bias.

In short, the residual plot shows that the negative binomial model is behaving as expected for count data, and there is no major evidence of nonlinearity or missing predictors.

## **6.4 Q-Q plot:**

```{r}
p_qq <- ggplot(model_data, aes(sample = Residuals)) +
  stat_qq(color = "#6A1B9A", size = 1.5, alpha = 0.5) +
  stat_qq_line(color = "red", linetype = "dashed", linewidth = 1) +
  labs(
    title = "Q-Q Plot of Pearson Residuals",
    subtitle = "Checking for extreme outliers in count data",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  ) +
  plotTheme

p_qq
```

**Interpretation**:

The Q–Q plot shows that the Pearson residuals follow the theoretical quantiles closely in the middle range, meaning the model captures the main distribution of crime counts well. The deviation at the upper tail indicates a few unusually high-crime stops that the model cannot fully explain—this is common in count data, especially when rare but extreme events occur.

Negative Binomial models are designed to handle overdispersion, and the relatively small tail deviation suggests that any remaining overdispersion is limited to a small number of extreme observations rather than a systematic model failure.

Overall, the Q–Q plot supports that the model fits the bulk of the data well, with only mild deviations at the extremes.

## **6.5** Top 50 风险站点地图**:**

```{r}
# 1. 提取预测值 (Predicted Counts)
final_data <- final_data %>%
  mutate(Predicted_Crime = predict(best_model, type = "response"))

# 2. 筛选前 50 个高风险站点
top_50_risky <- final_data %>%
  arrange(desc(Predicted_Crime)) %>%
  slice(1:50)

# 3. 绘制行动地图
ggplot() +
  # 背景底图
  geom_sf(data = philly_boundary, fill = "grey95", color = "white") +
  
  # 所有站点 (灰色背景)
  geom_sf(data = final_data, color = "grey80", size = 0.5, alpha = 0.3) +
  
  # 高风险站点 (红色醒目)
  geom_sf(data = top_50_risky, color = "red", size = 2, alpha = 0.9) +
  
  # (可选) 加上文字标签
  # geom_sf_text(data = head(top_50_risky, 5), aes(label = Stop), dy = 100, size = 3) +

  labs(
    title = "Top 50 High-Risk Bus Stops",
    subtitle = "Priority Zones for Police Patrol Deployment (Based on Model Prediction)",
    caption = "Red dots represent the stops with the highest predicted crime counts."
  ) +
  mapTheme
```

**Interpretation**:

-   **Overall**These 50 stops represent the "Hotspots" identified by our algorithm.

Actionable Insight: "We recommend SEPTA Police allocate dedicated patrol units to these specific locations during peak hours. By focusing resources on these top 1% of stops, we can potentially address a significant portion of the total crime risk."

这很好，加了geoid之后，除了市中心还有另一坨红色

------------------------------------------------------------------------

# Phase 7: Conclusions

## 7.1 Results:

**系数解读:**

-   如果客流系数是 **负**的 -\> **支持“街道眼”理论**（人多反而安全）。

-   如果客流系数是 **正**的 -\> **支持“潜在受害者”理论**（人多扒窃多）。

## 7.2 The Action Plan:

-   **警力部署:** 根据预测的风险值，生成“每日巡逻热点图”。

-   **环境设计 (CPTED):** 对于那些高犯罪站点，如果是路灯不够，装灯；如果是空置房太多，清理社区。

## **7.3 Equity concerns**

-   **Which neighborhoods are hardest to predict?**

-   **Any data bias?**
